{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"FetalSynthGen Introduction FetalSynthGen is a synthetic data generator created to address the challenges of limited data and domain shifts in fetal brain MRI analysis. It is based on the domain randomization approach of SynthSeg [1], which uses anatomical segmentations to create diverse synthetic images. It's application for fetal brain tissue segmentation is described in [2, 3]. Below is a brief overview of the key components of the FetalSynthGen pipeline: Input: The generator begins with a dataset of segmentation maps, which serve as anatomical priors of the simulated fetal brain. Meta-Labels: Instead of directly using the original segmentation labels, the method first merges these labels into four meta-classes : white matter (WM), gray matter (GM), cerebrospinal fluid (CSF), and non-brain tissue (including skull and surrounding maternal tissue). Intensity Clustering: Within each meta-class, the generator uses the Expectation-Maximization (EM) algorithm for intensity clustering, which divides each meta-class into subclasses , capturing the heterogeneity of the simulated brain tissues. The number of subclasses is sampled from a random uniform distribution. Intensity Generation: For each subclass, the generator then samples intensities from a Gaussian Mixture Model (GMM) with randomized parameters. This method creates images with varied contrasts that although often exceed what is realistic, ensure that the model learns features robust to domain shifts related to intensity or contrast. Spatial Transformations: After generating intensities, the synthetic images undergo spatial transformations, including affine and non-rigid diffeomorphic deformations. These simulate variations in image resolution and partial volume effects. Artifact Simulation : The generator corrupts the images by adding a bias field, performing intensity transformations, and simulating various image resolutions. The output of the generator is a paired set of synthetic images and corresponding synthetic segmentations that can be used to train a segmentation model, super-resolution or brain-embedding model. Installation From Source git clone https://github.com/Medical-Image-Analysis-Laboratory/fetalsyngen cd fetalsyngen pip install -e . Acknowledgements This research was funded by the Swiss National Science Foundation (182602 and 215641), ERA-NET Neuron MULTI-FACT project (SNSF 31NE30 203977). We acknowledge the Leenaards and Jeantet Foundations as well as CIBM Center for Biomedical Imaging, a Swiss research center of excellence founded and supported by CHUV, UNIL, EPFL, UNIGE and HUG. References Billot, Benjamin, et al. \u201cSynthSeg: Segmentation of Brain MRI Scans of Any Contrast and Resolution without Retraining.\u201d Medical Image Analysis, vol. 86, 25 Feb. 2023, pp. 102789\u2013102789, www.sciencedirect.com/science/article/pii/S1361841523000506, https://doi.org/10.1016/j.media.2023.102789. Vladyslav Zalevskyi, et al. \u201cImproving Cross-Domain Brain Tissue Segmentation in Fetal MRI with Synthetic Data.\u201d Lecture Notes in Computer Science, 1 Jan. 2024, pp. 437\u2013447, link.springer.com/chapter/10.1007/978-3-031-72378-0_41, https://doi.org/10.1007/978-3-031-72378-0_41. Vladyslav Zalevskyi, et al. \u201cMaximizing Domain Generalization in Fetal Brain Tissue Segmentation: The Role of Synthetic Data Generation, Intensity Clustering and Real Image Fine-Tuning.\u201d ArXiv.org, 2024, arxiv.org/abs/2411.06842. \u200c \u200c \u200c","title":"FetalSynthGen GitHub"},{"location":"#fetalsynthgen","text":"","title":"FetalSynthGen"},{"location":"#introduction","text":"FetalSynthGen is a synthetic data generator created to address the challenges of limited data and domain shifts in fetal brain MRI analysis. It is based on the domain randomization approach of SynthSeg [1], which uses anatomical segmentations to create diverse synthetic images. It's application for fetal brain tissue segmentation is described in [2, 3]. Below is a brief overview of the key components of the FetalSynthGen pipeline: Input: The generator begins with a dataset of segmentation maps, which serve as anatomical priors of the simulated fetal brain. Meta-Labels: Instead of directly using the original segmentation labels, the method first merges these labels into four meta-classes : white matter (WM), gray matter (GM), cerebrospinal fluid (CSF), and non-brain tissue (including skull and surrounding maternal tissue). Intensity Clustering: Within each meta-class, the generator uses the Expectation-Maximization (EM) algorithm for intensity clustering, which divides each meta-class into subclasses , capturing the heterogeneity of the simulated brain tissues. The number of subclasses is sampled from a random uniform distribution. Intensity Generation: For each subclass, the generator then samples intensities from a Gaussian Mixture Model (GMM) with randomized parameters. This method creates images with varied contrasts that although often exceed what is realistic, ensure that the model learns features robust to domain shifts related to intensity or contrast. Spatial Transformations: After generating intensities, the synthetic images undergo spatial transformations, including affine and non-rigid diffeomorphic deformations. These simulate variations in image resolution and partial volume effects. Artifact Simulation : The generator corrupts the images by adding a bias field, performing intensity transformations, and simulating various image resolutions. The output of the generator is a paired set of synthetic images and corresponding synthetic segmentations that can be used to train a segmentation model, super-resolution or brain-embedding model.","title":"Introduction"},{"location":"#installation","text":"","title":"Installation"},{"location":"#from-source","text":"git clone https://github.com/Medical-Image-Analysis-Laboratory/fetalsyngen cd fetalsyngen pip install -e .","title":"From Source"},{"location":"#acknowledgements","text":"This research was funded by the Swiss National Science Foundation (182602 and 215641), ERA-NET Neuron MULTI-FACT project (SNSF 31NE30 203977). We acknowledge the Leenaards and Jeantet Foundations as well as CIBM Center for Biomedical Imaging, a Swiss research center of excellence founded and supported by CHUV, UNIL, EPFL, UNIGE and HUG.","title":"Acknowledgements"},{"location":"#references","text":"Billot, Benjamin, et al. \u201cSynthSeg: Segmentation of Brain MRI Scans of Any Contrast and Resolution without Retraining.\u201d Medical Image Analysis, vol. 86, 25 Feb. 2023, pp. 102789\u2013102789, www.sciencedirect.com/science/article/pii/S1361841523000506, https://doi.org/10.1016/j.media.2023.102789. Vladyslav Zalevskyi, et al. \u201cImproving Cross-Domain Brain Tissue Segmentation in Fetal MRI with Synthetic Data.\u201d Lecture Notes in Computer Science, 1 Jan. 2024, pp. 437\u2013447, link.springer.com/chapter/10.1007/978-3-031-72378-0_41, https://doi.org/10.1007/978-3-031-72378-0_41. Vladyslav Zalevskyi, et al. \u201cMaximizing Domain Generalization in Fetal Brain Tissue Segmentation: The Role of Synthetic Data Generation, Intensity Clustering and Real Image Fine-Tuning.\u201d ArXiv.org, 2024, arxiv.org/abs/2411.06842. \u200c \u200c \u200c","title":"References"},{"location":"configs/","text":"Configs We use Hydra to manage configurations and instantiate classes in the FetalSynthGen pipeline. It allows us to define configurations in YAML files and instantiate classes with these configurations. This makes it easy to modify parameters and experiment with different settings. See the Hydra documentation for more information. Quick hydra overview Configuration files are stored in the configs directory. Each file is a .yaml file that contains the parameters for the generation pipeline and defines an instantiation of a class. Fields in the configuration files can be overridden from the command line or from other configuration files. Some special fields are: _target_ field in the configuration file specifies the class to be instantiated. All other fields are passed as arguments to the class constructor. It can be any callable object, including classes, functions, and lambdas, from any module in the Python path (local or external). defaults field in the configuration file specifies the default configuration file to be used, that is merged with the current configuration file. If a field is present in both files, the one in the current file takes precedence. For example defaults: - generator/default will load the generator/default.yaml file and will make all fields from the generator/default.yaml file available in the current configuration file from the generator.* namespace. null is used in the configuration files to specify a None value in Python. expressions like \"${var}\" can be used to access variable value in the same level of a given config while \"${..var}\" can be used to access variable value from the parent config. Configuration Files We provide a variety of ready-to-use configurations for different tasks. These configuration files are stored in the fetalsyngen/configs/dataset directory. To use them, copy the configuration files to your project root directory into configs/dataset . Feel free to modify these configurations to suit the specific requirements of your project. Validation/Testing Dataset Dataset configuration for loading real images and segmentations. Used for testing and validation on real data. See /datasets/#fetalsyngen.data.datasets.FetalTestDataset for more details. defaults : - transforms/inference _target_ : fetalsyngen.data.datasets.FetalTestDataset bids_path : ./data sub_list : null Real images with synthetic transformations Dataset configuration for applying the same transformations used in the generation of synthetic data to real images and segmentations. See /datasets/#fetalsyngen.data.datasets.FetalSynthDataset for more details. configs/dataset/real_train.yaml > defaults : - generator/default _target_ : fetalsyngen.data.datasets.FetalSynthDataset bids_path : ./data seed_path : null sub_list : null load_image : True image_as_intensity : True Synthetic images and segmentations Dataset configuration for creating synthetic images and segmentations on the fly. See /datasets/#fetalsyngen.data.datasets.FetalSynthDataset for more details. configs/dataset/synth_train.yaml > defaults : - generator/default _target_ : fetalsyngen.data.datasets.FetalSynthDataset bids_path : ./data seed_path : ./data/derivatives/seeds sub_list : null load_image : False image_as_intensity : False Default Generator Configuration configs/dataset/generator/default.yaml > _target_ : fetalsyngen.generator.model.FetalSynthGen shape : [ 256 , 256 , 256 ] resolution : [ 0.5 , 0.5 , 0.5 ] device : cuda # cuda ~6x faster than cpu intensity_generator : _target_ : fetalsyngen.generator.intensity.rand_gmm.ImageFromSeeds min_subclusters : 1 max_subclusters : 3 seed_labels : [ 0 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , 39 , 40 , 41 , 42 , 43 , 44 , 45 , 46 , 47 , 48 , 49 ] generation_classes : [ 0 , 10 , 10 , 10 , 10 , 10 , 10 , 10 , 10 , 10 , 10 , 20 , 20 , 20 , 20 , 20 , 20 , 20 , 20 , 20 , 20 , 30 , 30 , 30 , 30 , 30 , 30 , 30 , 30 , 30 , 30 , 40 , 41 , 42 , 43 , 44 , 45 , 46 , 47 , 48 , 49 ] spatial_deform : _target_ : fetalsyngen.generator.deformation.affine_nonrigid.SpatialDeformation device : \"${..device}\" # uses the device from the generator size : ${..shape} # uses the shape from the generator flip_prb : 0.5 max_rotation : 20 max_shear : 0.02 max_scaling : 0.1 nonlinear_transform : True nonlin_scale_min : 0.03 nonlin_scale_max : 0.06 nonlin_std_max : 4 resampler : _target_ : fetalsyngen.generator.augmentation.synthseg.RandResample min_resolution : 1.9 max_resolution : 2 prob : 1 bias_field : _target_ : fetalsyngen.generator.augmentation.synthseg.RandBiasField prob : 1 scale_min : 0.004 scale_max : 0.02 std_min : 0.01 std_max : 0.3 gamma : _target_ : fetalsyngen.generator.augmentation.synthseg.RandGamma prob : 1 gamma_std : 0.1 noise : _target_ : fetalsyngen.generator.augmentation.synthseg.RandNoise prob : 1 std_min : 5 std_max : 15","title":"Configs"},{"location":"configs/#configs","text":"We use Hydra to manage configurations and instantiate classes in the FetalSynthGen pipeline. It allows us to define configurations in YAML files and instantiate classes with these configurations. This makes it easy to modify parameters and experiment with different settings. See the Hydra documentation for more information.","title":"Configs"},{"location":"configs/#quick-hydra-overview","text":"Configuration files are stored in the configs directory. Each file is a .yaml file that contains the parameters for the generation pipeline and defines an instantiation of a class. Fields in the configuration files can be overridden from the command line or from other configuration files. Some special fields are: _target_ field in the configuration file specifies the class to be instantiated. All other fields are passed as arguments to the class constructor. It can be any callable object, including classes, functions, and lambdas, from any module in the Python path (local or external). defaults field in the configuration file specifies the default configuration file to be used, that is merged with the current configuration file. If a field is present in both files, the one in the current file takes precedence. For example defaults: - generator/default will load the generator/default.yaml file and will make all fields from the generator/default.yaml file available in the current configuration file from the generator.* namespace. null is used in the configuration files to specify a None value in Python. expressions like \"${var}\" can be used to access variable value in the same level of a given config while \"${..var}\" can be used to access variable value from the parent config.","title":"Quick hydra overview"},{"location":"configs/#configuration-files","text":"We provide a variety of ready-to-use configurations for different tasks. These configuration files are stored in the fetalsyngen/configs/dataset directory. To use them, copy the configuration files to your project root directory into configs/dataset . Feel free to modify these configurations to suit the specific requirements of your project.","title":"Configuration Files"},{"location":"configs/#validationtesting-dataset","text":"Dataset configuration for loading real images and segmentations. Used for testing and validation on real data. See /datasets/#fetalsyngen.data.datasets.FetalTestDataset for more details. defaults : - transforms/inference _target_ : fetalsyngen.data.datasets.FetalTestDataset bids_path : ./data sub_list : null","title":"Validation/Testing Dataset"},{"location":"configs/#real-images-with-synthetic-transformations","text":"Dataset configuration for applying the same transformations used in the generation of synthetic data to real images and segmentations. See /datasets/#fetalsyngen.data.datasets.FetalSynthDataset for more details. configs/dataset/real_train.yaml > defaults : - generator/default _target_ : fetalsyngen.data.datasets.FetalSynthDataset bids_path : ./data seed_path : null sub_list : null load_image : True image_as_intensity : True","title":"Real images with synthetic transformations"},{"location":"configs/#synthetic-images-and-segmentations","text":"Dataset configuration for creating synthetic images and segmentations on the fly. See /datasets/#fetalsyngen.data.datasets.FetalSynthDataset for more details. configs/dataset/synth_train.yaml > defaults : - generator/default _target_ : fetalsyngen.data.datasets.FetalSynthDataset bids_path : ./data seed_path : ./data/derivatives/seeds sub_list : null load_image : False image_as_intensity : False","title":"Synthetic images and segmentations"},{"location":"configs/#default-generator-configuration","text":"configs/dataset/generator/default.yaml > _target_ : fetalsyngen.generator.model.FetalSynthGen shape : [ 256 , 256 , 256 ] resolution : [ 0.5 , 0.5 , 0.5 ] device : cuda # cuda ~6x faster than cpu intensity_generator : _target_ : fetalsyngen.generator.intensity.rand_gmm.ImageFromSeeds min_subclusters : 1 max_subclusters : 3 seed_labels : [ 0 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , 39 , 40 , 41 , 42 , 43 , 44 , 45 , 46 , 47 , 48 , 49 ] generation_classes : [ 0 , 10 , 10 , 10 , 10 , 10 , 10 , 10 , 10 , 10 , 10 , 20 , 20 , 20 , 20 , 20 , 20 , 20 , 20 , 20 , 20 , 30 , 30 , 30 , 30 , 30 , 30 , 30 , 30 , 30 , 30 , 40 , 41 , 42 , 43 , 44 , 45 , 46 , 47 , 48 , 49 ] spatial_deform : _target_ : fetalsyngen.generator.deformation.affine_nonrigid.SpatialDeformation device : \"${..device}\" # uses the device from the generator size : ${..shape} # uses the shape from the generator flip_prb : 0.5 max_rotation : 20 max_shear : 0.02 max_scaling : 0.1 nonlinear_transform : True nonlin_scale_min : 0.03 nonlin_scale_max : 0.06 nonlin_std_max : 4 resampler : _target_ : fetalsyngen.generator.augmentation.synthseg.RandResample min_resolution : 1.9 max_resolution : 2 prob : 1 bias_field : _target_ : fetalsyngen.generator.augmentation.synthseg.RandBiasField prob : 1 scale_min : 0.004 scale_max : 0.02 std_min : 0.01 std_max : 0.3 gamma : _target_ : fetalsyngen.generator.augmentation.synthseg.RandGamma prob : 1 gamma_std : 0.1 noise : _target_ : fetalsyngen.generator.augmentation.synthseg.RandNoise prob : 1 std_min : 5 std_max : 15","title":"Default Generator Configuration"},{"location":"datasets/","text":"Datasets Classes for loading and processing datasets. Note \ud83d\udcdd Device : All datasets return samples with tensors on the CPU (even when the synthetic data generation is done on the GPU). This is due to restriction on the GPU usage in the multiprocessing settings, where GPU memory cannot be easily shared between processes. \ud83d\udcdd Dataloader : When using torch.utils.data.DataLoader ensure that you pass multiprocessing_context=\"spawn\" argument to the dataloader object when using FetalSynthDataset to ensure that the spawned processes have access to the GPU. FetalDataset Abstract class defining a dataset for loading fetal data. Source code in fetalsyngen/data/datasets.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 class FetalDataset : \"\"\"Abstract class defining a dataset for loading fetal data.\"\"\" def __init__ ( self , bids_path : str , sub_list : list [ str ] | None , ) -> dict : \"\"\" Args: bids_path: Path to the bids folder with the data. sub_list: List of the subjects to use. If None, all subjects are used. \"\"\" super () . __init__ () self . bids_path = Path ( bids_path ) self . subjects = self . find_subjects ( sub_list ) if self . subjects is None : self . subjects = [ x . name for x in self . bids_path . glob ( \"sub-*\" )] self . sub_ses = [ ( x , y ) for x in self . subjects for y in self . _get_ses ( self . bids_path , x ) ] self . loader = SimpleITKReader () self . scaler = ScaleIntensity ( minv = 0 , maxv = 1 ) self . orientation = Orientation ( axcodes = \"RAS\" ) self . img_paths = self . _load_bids_path ( self . bids_path , \"T2w\" ) self . segm_paths = self . _load_bids_path ( self . bids_path , \"dseg\" ) def find_subjects ( self , sub_list ): subj_found = [ x . name for x in Path ( self . bids_path ) . glob ( \"sub-*\" )] return ( list ( set ( subj_found ) & set ( sub_list )) if sub_list is not None else None ) def _sub_ses_string ( self , sub , ses ): return f \" { sub } _ { ses } \" if ses is not None else sub def _sub_ses_idx ( self , idx ): sub , ses = self . sub_ses [ idx ] return self . _sub_ses_string ( sub , ses ) def _get_ses ( self , bids_path , sub ): \"\"\"Get the session names for the subject.\"\"\" sub_path = bids_path / sub ses_dir = [ x for x in sub_path . iterdir () if x . is_dir ()] ses = [] for s in ses_dir : if \"anat\" in s . name : ses . append ( None ) else : ses . append ( s . name ) return sorted ( ses , key = lambda x : x or \"\" ) def _get_pattern ( self , sub , ses , suffix , extension = \".nii.gz\" ): \"\"\"Get the pattern for the file name.\"\"\" if ses is None : return f \" { sub } /anat/ { sub } *_ { suffix }{ extension } \" else : return f \" { sub } / { ses } /anat/ { sub } _ { ses } *_ { suffix }{ extension } \" def _load_bids_path ( self , path , suffix ): \"\"\" \"Check that for a given path, all subjects have a file with the provided suffix \"\"\" files_paths = [] for sub , ses in self . sub_ses : pattern = self . _get_pattern ( sub , ses , suffix ) files = list ( path . glob ( pattern )) if len ( files ) == 0 : raise FileNotFoundError ( f \"No files found for requested subject { sub } in { path } \" f \"( { pattern } returned nothing)\" ) elif len ( files ) > 1 : raise RuntimeError ( f \"Multiple files found for requested subject { sub } in { path } \" f \"( { pattern } returned { files } )\" ) files_paths . append ( files [ 0 ]) return files_paths def __len__ ( self ): return len ( self . subjects ) def __getitem__ ( self , idx ): raise NotImplementedError ( \"This method should be implemented in the child class.\" ) __init__ ( bids_path , sub_list ) Parameters: bids_path ( str ) \u2013 Path to the bids folder with the data. sub_list ( list [ str ] | None ) \u2013 List of the subjects to use. If None, all subjects are used. Source code in fetalsyngen/data/datasets.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , bids_path : str , sub_list : list [ str ] | None , ) -> dict : \"\"\" Args: bids_path: Path to the bids folder with the data. sub_list: List of the subjects to use. If None, all subjects are used. \"\"\" super () . __init__ () self . bids_path = Path ( bids_path ) self . subjects = self . find_subjects ( sub_list ) if self . subjects is None : self . subjects = [ x . name for x in self . bids_path . glob ( \"sub-*\" )] self . sub_ses = [ ( x , y ) for x in self . subjects for y in self . _get_ses ( self . bids_path , x ) ] self . loader = SimpleITKReader () self . scaler = ScaleIntensity ( minv = 0 , maxv = 1 ) self . orientation = Orientation ( axcodes = \"RAS\" ) self . img_paths = self . _load_bids_path ( self . bids_path , \"T2w\" ) self . segm_paths = self . _load_bids_path ( self . bids_path , \"dseg\" ) FetalTestDataset Bases: FetalDataset Dataset class for loading fetal images offline. Used to load test/validation data. Use the transforms argument to pass additional processing steps (scaling, resampling, cropping, etc.). Source code in fetalsyngen/data/datasets.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 class FetalTestDataset ( FetalDataset ): \"\"\"Dataset class for loading fetal images offline. Used to load test/validation data. Use the `transforms` argument to pass additional processing steps (scaling, resampling, cropping, etc.). \"\"\" def __init__ ( self , bids_path : str , sub_list : list [ str ] | None , transforms : Compose | None = None , ): \"\"\" Args: bids_path: Path to the bids folder with the data. sub_list: List of the subjects to use. If None, all subjects are used. transforms: Compose object with the transformations to apply. Default is None, no transformations are applied. !!! Note We highle recommend using the `transforms` arguments with at least the re-oriented transform to RAS and the intensity scaling to `[0, 1]` to ensure the data consistency. See [inference.yaml](https://github.com/Medical-Image-Analysis-Laboratory/fetalsyngen/blob/dev/configs/dataset/transforms/inference.yaml) for an example of the transforms configuration. \"\"\" super () . __init__ ( bids_path , sub_list ) self . transforms = transforms def _load_data ( self , idx ): # load the image and segmentation image = self . loader ( self . img_paths [ idx ]) segm = self . loader ( self . segm_paths [ idx ]) if len ( image . shape ) == 3 : # add channel dimension image = image . unsqueeze ( 0 ) segm = segm . unsqueeze ( 0 ) elif len ( image . shape ) != 4 : raise ValueError ( f \"Expected 3D or 4D image, got { len ( image . shape ) } D image.\" ) # transform name into a single string otherwise collate fails name = self . sub_ses [ idx ] name = self . _sub_ses_string ( name [ 0 ], ses = name [ 1 ]) return { \"image\" : image , \"label\" : segm . long (), \"name\" : name } def __getitem__ ( self , idx ) -> dict : \"\"\" Returns: Dictionary with the `image` , `label` and the `name` keys. `image` and `label` are `torch.float32` [`monai.data.meta_tensor.MetaTensor`](https://docs.monai.io/en/stable/data.html#metatensor) instances with dimensions `(1, H, W, D)` and `name` is a string of a format `sub_ses` where `sub` is the subject name and `ses` is the session name. \"\"\" data = self . _load_data ( idx ) if self . transforms : data = self . transforms ( data ) return data def reverse_transform ( self , data : dict ) -> dict : \"\"\"Reverse the transformations applied to the data. Args: data: Dictionary with the `image` and `label` keys, like the one returned by the `__getitem__` method. Returns: Dictionary with the `image` and `label` keys where the transformations are reversed. \"\"\" if self . transforms : data = self . transforms . inverse ( data ) return data __init__ ( bids_path , sub_list , transforms = None ) Parameters: bids_path ( str ) \u2013 Path to the bids folder with the data. sub_list ( list [ str ] | None ) \u2013 List of the subjects to use. If None, all subjects are used. transforms ( Compose | None , default: None ) \u2013 Compose object with the transformations to apply. Default is None, no transformations are applied. Note We highle recommend using the transforms arguments with at least the re-oriented transform to RAS and the intensity scaling to [0, 1] to ensure the data consistency. See inference.yaml for an example of the transforms configuration. Source code in fetalsyngen/data/datasets.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def __init__ ( self , bids_path : str , sub_list : list [ str ] | None , transforms : Compose | None = None , ): \"\"\" Args: bids_path: Path to the bids folder with the data. sub_list: List of the subjects to use. If None, all subjects are used. transforms: Compose object with the transformations to apply. Default is None, no transformations are applied. !!! Note We highle recommend using the `transforms` arguments with at least the re-oriented transform to RAS and the intensity scaling to `[0, 1]` to ensure the data consistency. See [inference.yaml](https://github.com/Medical-Image-Analysis-Laboratory/fetalsyngen/blob/dev/configs/dataset/transforms/inference.yaml) for an example of the transforms configuration. \"\"\" super () . __init__ ( bids_path , sub_list ) self . transforms = transforms __getitem__ ( idx ) Returns: dict \u2013 Dictionary with the image , label and the name keys. image and label are torch.float32 monai.data.meta_tensor.MetaTensor instances with dimensions (1, H, W, D) and name is a string of a format sub_ses where sub is the subject name and ses is the session name. Source code in fetalsyngen/data/datasets.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def __getitem__ ( self , idx ) -> dict : \"\"\" Returns: Dictionary with the `image` , `label` and the `name` keys. `image` and `label` are `torch.float32` [`monai.data.meta_tensor.MetaTensor`](https://docs.monai.io/en/stable/data.html#metatensor) instances with dimensions `(1, H, W, D)` and `name` is a string of a format `sub_ses` where `sub` is the subject name and `ses` is the session name. \"\"\" data = self . _load_data ( idx ) if self . transforms : data = self . transforms ( data ) return data reverse_transform ( data ) Reverse the transformations applied to the data. Parameters: data ( dict ) \u2013 Dictionary with the image and label keys, like the one returned by the __getitem__ method. Returns: dict \u2013 Dictionary with the image and label keys where the transformations are reversed. Source code in fetalsyngen/data/datasets.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def reverse_transform ( self , data : dict ) -> dict : \"\"\"Reverse the transformations applied to the data. Args: data: Dictionary with the `image` and `label` keys, like the one returned by the `__getitem__` method. Returns: Dictionary with the `image` and `label` keys where the transformations are reversed. \"\"\" if self . transforms : data = self . transforms . inverse ( data ) return data FetalSynthDataset Bases: FetalDataset Dataset class for generating/augmenting on-the-fly fetal images\" Source code in fetalsyngen/data/datasets.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 class FetalSynthDataset ( FetalDataset ): \"\"\"Dataset class for generating/augmenting on-the-fly fetal images\" \"\"\" def __init__ ( self , bids_path : str , generator : FetalSynthGen , seed_path : str | None , sub_list : list [ str ] | None , load_image : bool = False , image_as_intensity : bool = False , ): \"\"\" Args: bids_path: Path to the bids-formatted folder with the data. seed_path: Path to the folder with the seeds to use for intensity sampling. See `scripts/seed_generation.py` for details on the data formatting. If seed_path is None, the intensity sampling step is skipped and the output image intensities will be based on the input image. generator: a class object defining a generator to use. sub_list: List of the subjects to use. If None, all subjects are used. load_image: If **True**, the image is loaded and passed to the generator, where it can be used as the intensity prior instead of a random intensity sampling or spatially deformed with the same transformation field as segmentation and the syntehtic image. Default is **False**. image_as_intensity: If **True**, the image is used as the intensity prior, instead of sampling the intensities from the seeds. Default is **False**. \"\"\" super () . __init__ ( bids_path , sub_list ) self . seed_path = ( Path ( seed_path ) if isinstance ( seed_path , str ) else None ) self . load_image = load_image self . generator = generator self . image_as_intensity = image_as_intensity # parse seeds paths if not self . image_as_intensity and isinstance ( self . seed_path , Path ): if not self . seed_path . exists (): raise FileNotFoundError ( f \"Provided seed path { self . seed_path } does not exist.\" ) else : self . _load_seed_path () def _load_seed_path ( self ): \"\"\"Load the seeds for the subjects.\"\"\" self . seed_paths = { self . _sub_ses_string ( sub , ses ): defaultdict ( dict ) for ( sub , ses ) in self . sub_ses } avail_seeds = [ int ( x . name . replace ( \"subclasses_\" , \"\" )) for x in self . seed_path . glob ( \"subclasses_*\" ) ] min_seeds_available = min ( avail_seeds ) max_seeds_available = max ( avail_seeds ) for n_sub in range ( min_seeds_available , max_seeds_available + 1 , ): seed_path = self . seed_path / f \"subclasses_ { n_sub } \" if not seed_path . exists (): raise FileNotFoundError ( f \"Provided seed path { seed_path } does not exist.\" ) # load the seeds for the subjects for each meta label 1-4 for i in range ( 1 , 5 ): files = self . _load_bids_path ( seed_path , f \"mlabel_ { i } \" ) for ( sub , ses ), file in zip ( self . sub_ses , files ): sub_ses_str = self . _sub_ses_string ( sub , ses ) self . seed_paths [ sub_ses_str ][ n_sub ][ i ] = file def sample ( self , idx , genparams : dict = {}) -> tuple [ dict , dict ]: \"\"\" Retrieve a single item from the dataset at the specified index. Args: idx (int): The index of the item to retrieve. genparams (dict): Dictionary with generation parameters. Used for fixed generation. Should follow exactly the same structure and be of the same type as the returned generation parameters. Can be used to replicate the augmentations (power) used for the generation of a specific sample. Returns: Dictionaries with the generated data and the generation parameters. First dictionary contains the `image`, `label` and the `name` keys. The second dictionary contains the parameters used for the generation. !!! Note The `image` is scaled to `[0, 1]` and oriented with the `label` to **RAS** and returned on the device specified in the `generator` initialization. \"\"\" # use generation_params to track the parameters used for the generation generation_params = {} image = self . loader ( self . img_paths [ idx ]) if self . load_image else None segm = self . loader ( self . segm_paths [ idx ]) # orient to RAS for consistency image = ( self . orientation ( image . unsqueeze ( 0 )) . squeeze ( 0 ) if self . load_image else None ) segm = self . orientation ( segm . unsqueeze ( 0 )) . squeeze ( 0 ) # transform name into a single string otherwise collate fails name = self . sub_ses [ idx ] name = self . _sub_ses_string ( name [ 0 ], ses = name [ 1 ]) # initialize seeds as dictionary # with paths to the seeds volumes # or None if image is to be used as intensity prior if self . seed_path is not None : seeds = self . seed_paths [ name ] if self . image_as_intensity : seeds = None # log input data generation_params [ \"idx\" ] = idx generation_params [ \"img_paths\" ] = str ( self . img_paths [ idx ]) generation_params [ \"segm_paths\" ] = str ( self . img_paths [ idx ]) generation_params [ \"seeds\" ] = str ( self . seed_path ) generation_time_start = time . time () # generate the synthetic data gen_output , segmentation , image , synth_params = self . generator . sample ( image = image , segmentation = segm , seeds = seeds , genparams = genparams ) # scale the images to [0, 1] gen_output = self . scaler ( gen_output ) image = self . scaler ( image ) if image is not None else None # ensure image and segmentation are on the cpu gen_output = gen_output . cpu () segmentation = segmentation . cpu () image = image . cpu () if image is not None else None generation_params = { ** generation_params , ** synth_params } generation_params [ \"generation_time\" ] = ( time . time () - generation_time_start ) data_out = { \"image\" : gen_output . unsqueeze ( 0 ), \"label\" : segmentation . unsqueeze ( 0 ) . long (), \"name\" : name , } return data_out , generation_params def __getitem__ ( self , idx ) -> dict : \"\"\" Retrieve a single item from the dataset at the specified index. Args: idx (int): The index of the item to retrieve. Returns: Dictionary with the `image`, `label` and the `name` keys. `image` and `label` are `torch.float32` [`monai.data.meta_tensor.MetaTensor`](https://docs.monai.io/en/stable/data.html#metatensor) and `name` is a string of a format `sub_ses` where `sub` is the subject name and `ses` is the session name. !!!Note The `image` is scaled to `[0, 1]` and oriented to **RAS** and returned on the device specified in the `generator` initialization. \"\"\" data_out , generation_params = self . sample ( idx ) self . generation_params = generation_params return data_out def sample_with_meta ( self , idx : int , genparams : dict = {}) -> dict : \"\"\" Retrieve a sample along with its generation parameters and store them in the same dictionary. Args: idx: The index of the sample to retrieve. genparams: Dictionary with generation parameters. Used for fixed generation. Should follow exactly the same structure and be of the same type as the returned generation parameters from the `sample()` method. Can be used to replicate the augmentations (power) used for the generation of a specific sample. Returns: A dictionary with `image`, `label`, `name` and `generation_params` keys. \"\"\" data , generation_params = self . sample ( idx , genparams = genparams ) data [ \"generation_params\" ] = generation_params return data __init__ ( bids_path , generator , seed_path , sub_list , load_image = False , image_as_intensity = False ) Parameters: bids_path ( str ) \u2013 Path to the bids-formatted folder with the data. seed_path ( str | None ) \u2013 Path to the folder with the seeds to use for intensity sampling. See scripts/seed_generation.py for details on the data formatting. If seed_path is None, the intensity sampling step is skipped and the output image intensities will be based on the input image. generator ( FetalSynthGen ) \u2013 a class object defining a generator to use. sub_list ( list [ str ] | None ) \u2013 List of the subjects to use. If None, all subjects are used. load_image ( bool , default: False ) \u2013 If True , the image is loaded and passed to the generator, where it can be used as the intensity prior instead of a random intensity sampling or spatially deformed with the same transformation field as segmentation and the syntehtic image. Default is False . image_as_intensity ( bool , default: False ) \u2013 If True , the image is used as the intensity prior, instead of sampling the intensities from the seeds. Default is False . Source code in fetalsyngen/data/datasets.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def __init__ ( self , bids_path : str , generator : FetalSynthGen , seed_path : str | None , sub_list : list [ str ] | None , load_image : bool = False , image_as_intensity : bool = False , ): \"\"\" Args: bids_path: Path to the bids-formatted folder with the data. seed_path: Path to the folder with the seeds to use for intensity sampling. See `scripts/seed_generation.py` for details on the data formatting. If seed_path is None, the intensity sampling step is skipped and the output image intensities will be based on the input image. generator: a class object defining a generator to use. sub_list: List of the subjects to use. If None, all subjects are used. load_image: If **True**, the image is loaded and passed to the generator, where it can be used as the intensity prior instead of a random intensity sampling or spatially deformed with the same transformation field as segmentation and the syntehtic image. Default is **False**. image_as_intensity: If **True**, the image is used as the intensity prior, instead of sampling the intensities from the seeds. Default is **False**. \"\"\" super () . __init__ ( bids_path , sub_list ) self . seed_path = ( Path ( seed_path ) if isinstance ( seed_path , str ) else None ) self . load_image = load_image self . generator = generator self . image_as_intensity = image_as_intensity # parse seeds paths if not self . image_as_intensity and isinstance ( self . seed_path , Path ): if not self . seed_path . exists (): raise FileNotFoundError ( f \"Provided seed path { self . seed_path } does not exist.\" ) else : self . _load_seed_path () sample ( idx , genparams = {}) Retrieve a single item from the dataset at the specified index. Parameters: idx ( int ) \u2013 The index of the item to retrieve. genparams ( dict , default: {} ) \u2013 Dictionary with generation parameters. Used for fixed generation. Should follow exactly the same structure and be of the same type as the returned generation parameters. Can be used to replicate the augmentations (power) used for the generation of a specific sample. Returns: Dictionaries with the generated data and the generation parameters. First dictionary contains the image , label and the name keys. The second dictionary contains the parameters used for the generation. Note The image is scaled to [0, 1] and oriented with the label to RAS and returned on the device specified in the generator initialization. Source code in fetalsyngen/data/datasets.py 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 def sample ( self , idx , genparams : dict = {}) -> tuple [ dict , dict ]: \"\"\" Retrieve a single item from the dataset at the specified index. Args: idx (int): The index of the item to retrieve. genparams (dict): Dictionary with generation parameters. Used for fixed generation. Should follow exactly the same structure and be of the same type as the returned generation parameters. Can be used to replicate the augmentations (power) used for the generation of a specific sample. Returns: Dictionaries with the generated data and the generation parameters. First dictionary contains the `image`, `label` and the `name` keys. The second dictionary contains the parameters used for the generation. !!! Note The `image` is scaled to `[0, 1]` and oriented with the `label` to **RAS** and returned on the device specified in the `generator` initialization. \"\"\" # use generation_params to track the parameters used for the generation generation_params = {} image = self . loader ( self . img_paths [ idx ]) if self . load_image else None segm = self . loader ( self . segm_paths [ idx ]) # orient to RAS for consistency image = ( self . orientation ( image . unsqueeze ( 0 )) . squeeze ( 0 ) if self . load_image else None ) segm = self . orientation ( segm . unsqueeze ( 0 )) . squeeze ( 0 ) # transform name into a single string otherwise collate fails name = self . sub_ses [ idx ] name = self . _sub_ses_string ( name [ 0 ], ses = name [ 1 ]) # initialize seeds as dictionary # with paths to the seeds volumes # or None if image is to be used as intensity prior if self . seed_path is not None : seeds = self . seed_paths [ name ] if self . image_as_intensity : seeds = None # log input data generation_params [ \"idx\" ] = idx generation_params [ \"img_paths\" ] = str ( self . img_paths [ idx ]) generation_params [ \"segm_paths\" ] = str ( self . img_paths [ idx ]) generation_params [ \"seeds\" ] = str ( self . seed_path ) generation_time_start = time . time () # generate the synthetic data gen_output , segmentation , image , synth_params = self . generator . sample ( image = image , segmentation = segm , seeds = seeds , genparams = genparams ) # scale the images to [0, 1] gen_output = self . scaler ( gen_output ) image = self . scaler ( image ) if image is not None else None # ensure image and segmentation are on the cpu gen_output = gen_output . cpu () segmentation = segmentation . cpu () image = image . cpu () if image is not None else None generation_params = { ** generation_params , ** synth_params } generation_params [ \"generation_time\" ] = ( time . time () - generation_time_start ) data_out = { \"image\" : gen_output . unsqueeze ( 0 ), \"label\" : segmentation . unsqueeze ( 0 ) . long (), \"name\" : name , } return data_out , generation_params __getitem__ ( idx ) Retrieve a single item from the dataset at the specified index. Parameters: idx ( int ) \u2013 The index of the item to retrieve. Returns: dict \u2013 Dictionary with the image , label and the name keys. image and label are torch.float32 monai.data.meta_tensor.MetaTensor and name is a string of a format sub_ses where sub is the subject name and ses is the session name. Note The image is scaled to [0, 1] and oriented to RAS and returned on the device specified in the generator initialization. Source code in fetalsyngen/data/datasets.py 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 def __getitem__ ( self , idx ) -> dict : \"\"\" Retrieve a single item from the dataset at the specified index. Args: idx (int): The index of the item to retrieve. Returns: Dictionary with the `image`, `label` and the `name` keys. `image` and `label` are `torch.float32` [`monai.data.meta_tensor.MetaTensor`](https://docs.monai.io/en/stable/data.html#metatensor) and `name` is a string of a format `sub_ses` where `sub` is the subject name and `ses` is the session name. !!!Note The `image` is scaled to `[0, 1]` and oriented to **RAS** and returned on the device specified in the `generator` initialization. \"\"\" data_out , generation_params = self . sample ( idx ) self . generation_params = generation_params return data_out sample_with_meta ( idx , genparams = {}) Retrieve a sample along with its generation parameters and store them in the same dictionary. Parameters: idx ( int ) \u2013 The index of the sample to retrieve. genparams ( dict , default: {} ) \u2013 Dictionary with generation parameters. Used for fixed generation. Should follow exactly the same structure and be of the same type as the returned generation parameters from the sample() method. Can be used to replicate the augmentations (power) used for the generation of a specific sample. Returns: dict \u2013 A dictionary with image , label , name and generation_params keys. Source code in fetalsyngen/data/datasets.py 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 def sample_with_meta ( self , idx : int , genparams : dict = {}) -> dict : \"\"\" Retrieve a sample along with its generation parameters and store them in the same dictionary. Args: idx: The index of the sample to retrieve. genparams: Dictionary with generation parameters. Used for fixed generation. Should follow exactly the same structure and be of the same type as the returned generation parameters from the `sample()` method. Can be used to replicate the augmentations (power) used for the generation of a specific sample. Returns: A dictionary with `image`, `label`, `name` and `generation_params` keys. \"\"\" data , generation_params = self . sample ( idx , genparams = genparams ) data [ \"generation_params\" ] = generation_params return data Fixed Image Generation It is possible to generate synthetic images of the same 'augmentation' power as any given synthetic image. This is done by passing the genparams dictionary to the sample_with_meta (or sample ) method of the FetalSynthDataset class. The generation_params dictionary is a dictionary of the parameters used to generate the image. The method will then use these parameters to generate a new image with the same augmentation power as the original image. This genparams dictionary can be obtained, for example, from the dictionary returned by the FetalSynthDataset.sample_with_meta method. It then can be directly used to fix (some or all) generation parameters for the new image. See example below: # initialize the dataset class # see the Examples page for more details dataset = FetalSynthDataset ( ... ) # first sample a synthetic image from the dataset sample = dataset . sample_with_meta ( 0 ) # then we sample a synthetic image with the same augmentation power as the first image sample_copy = dataset . sample_with_meta ( 0 , genparams = sample [ \"generation_params\" ]) For example, generation parameters of the first image can be like this: { 'idx' : 0 , 'img_paths' : PosixPath ( '../data/sub-sta38/anat/sub-sta38_rec-irtk_T2w.nii.gz' ), 'segm_paths' : PosixPath ( '../data/sub-sta38/anat/sub-sta38_rec-irtk_T2w.nii.gz' ), 'seeds' : defaultdict ( dict , { 1 : { 1 : PosixPath ( '../data/derivatives/seeds/subclasses_1/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_1.nii.gz' ), 2 : PosixPath ( '../data/derivatives/seeds/subclasses_1/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_2.nii.gz' ), 3 : PosixPath ( '../data/derivatives/seeds/subclasses_1/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_3.nii.gz' ), 4 : PosixPath ( '../data/derivatives/seeds/subclasses_1/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_4.nii.gz' )}, 2 : { 1 : PosixPath ( '../data/derivatives/seeds/subclasses_2/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_1.nii.gz' ), 2 : PosixPath ( '../data/derivatives/seeds/subclasses_2/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_2.nii.gz' ), 3 : PosixPath ( '../data/derivatives/seeds/subclasses_2/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_3.nii.gz' ), 4 : PosixPath ( '../data/derivatives/seeds/subclasses_2/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_4.nii.gz' )}, 3 : { 1 : PosixPath ( '../data/derivatives/seeds/subclasses_3/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_1.nii.gz' ), 2 : PosixPath ( '../data/derivatives/seeds/subclasses_3/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_2.nii.gz' ), 3 : PosixPath ( '../data/derivatives/seeds/subclasses_3/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_3.nii.gz' ), 4 : PosixPath ( '../data/derivatives/seeds/subclasses_3/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_4.nii.gz' )}}), 'selected_seeds' : { 'mlabel2subclusters' : { 1 : 2 , 2 : 1 , 3 : 3 , 4 : 1 }}, 'seed_intensities' : { 'mus' : tensor ([ 109.6722 , 220.9658 , 100.9801 , 38.6364 , 125.5148 , 108.1950 , 216.1060 , 190.5462 , 55.3930 , 59.2667 , 72.0628 , 68.8775 , 76.5113 , 84.6639 , 90.0124 , 94.1701 , 67.0610 , 25.9465 , 31.5438 , 21.0375 , 192.4223 , 173.7434 , 139.9284 , 121.3904 , 145.4289 , 158.1318 , 157.4630 , 150.0894 , 183.9047 , 181.7129 , 114.8939 , 9.5253 , 29.0257 , 97.9543 , 122.0798 , 72.2969 , 26.3086 , 81.8050 , 67.7463 , 72.3737 , 129.8539 , 113.3900 , 141.8177 , 225.0000 , 35.3458 , 173.7635 , 29.5101 , 135.9482 , 188.2391 , 225.0000 ], device = 'cuda:0' ), 'sigmas' : tensor ([ 9.2432 , 23.1060 , 16.4965 , 6.4289 , 24.7862 , 23.7996 , 15.2424 , 20.2845 , 12.6833 , 6.9079 , 6.1214 , 22.1317 , 9.7907 , 5.5302 , 14.3288 , 11.1454 , 16.0453 , 20.9057 , 24.2358 , 13.4785 , 22.7258 , 11.2053 , 12.9420 , 13.4270 , 14.8660 , 22.4874 , 5.6251 , 9.8794 , 8.8749 , 19.0294 , 9.7164 , 6.2293 , 13.6376 , 11.7447 , 14.1414 , 6.4362 , 20.4575 , 14.6729 , 8.4719 , 14.2926 , 6.9458 , 11.5346 , 14.6113 , 6.6516 , 22.1767 , 8.3793 , 20.1699 , 6.3299 , 5.3340 , 21.8027 ], device = 'cuda:0' )}, 'deform_params' : { 'affine' : { 'rotations' : array ([ 0.0008224 , 0.03067143 , - 0.0151502 ]), 'shears' : array ([ - 0.01735838 , 0.00744726 , 0.00012507 ]), 'scalings' : array ([ 1.09345725 , 0.91695532 , 0.98194215 ])}, 'non_rigid' : { 'nonlin_scale' : array ([ 0.05686841 ]), 'nonlin_std' : 1.048839010036788 , 'size_F_small' : [ 15 , 15 , 15 ]}, 'flip' : False }, 'gamma_params' : { 'gamma' : 0.960299468352801 }, 'bf_params' : { 'bf_scale' : None , 'bf_std' : None , 'bf_size' : None }, 'resample_params' : { 'spacing' : array ([ 0.65685245 , 0.65685245 , 0.65685245 ])}, 'noise_params' : { 'noise_std' : None }, 'generation_time' : 0.5615839958190918 } If the key:value pair exists in the passed genparams dictionary, the sample method will use directly the value from the genparams dictionary. If the key:value pair does not exist in the genparams dictionary or it is None , sample method will generate the value randomly, using the corresponding class attributes. See how the keys bf_scale , bf_std , bf_size and noise_std have not been defined in the genparams dictionary above. This means that the sample method will generate these values randomly. The same could have been achieved by not passing them at all. {'idx': 0, 'img_paths': PosixPath('../data/sub-sta38/anat/sub-sta38_rec-irtk_T2w.nii.gz'), 'segm_paths': PosixPath('../data/sub-sta38/anat/sub-sta38_rec-irtk_T2w.nii.gz'), 'seeds': defaultdict(dict, {1: {1: PosixPath('../data/derivatives/seeds/subclasses_1/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_1.nii.gz'), 2: PosixPath('../data/derivatives/seeds/subclasses_1/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_2.nii.gz'), 3: PosixPath('../data/derivatives/seeds/subclasses_1/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_3.nii.gz'), 4: PosixPath('../data/derivatives/seeds/subclasses_1/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_4.nii.gz')}, 2: {1: PosixPath('../data/derivatives/seeds/subclasses_2/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_1.nii.gz'), 2: PosixPath('../data/derivatives/seeds/subclasses_2/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_2.nii.gz'), 3: PosixPath('../data/derivatives/seeds/subclasses_2/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_3.nii.gz'), 4: PosixPath('../data/derivatives/seeds/subclasses_2/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_4.nii.gz')}, 3: {1: PosixPath('../data/derivatives/seeds/subclasses_3/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_1.nii.gz'), 2: PosixPath('../data/derivatives/seeds/subclasses_3/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_2.nii.gz'), 3: PosixPath('../data/derivatives/seeds/subclasses_3/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_3.nii.gz'), 4: PosixPath('../data/derivatives/seeds/subclasses_3/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_4.nii.gz')}}), 'selected_seeds': {'mlabel2subclusters': {1: 2, 2: 1, 3: 3, 4: 1}}, 'seed_intensities': {'mus': tensor([109.6722, 220.9658, 100.9801, 38.6364, 125.5148, 108.1950, 216.1060, 190.5462, 55.3930, 59.2667, 72.0628, 68.8775, 76.5113, 84.6639, 90.0124, 94.1701, 67.0610, 25.9465, 31.5438, 21.0375, 192.4223, 173.7434, 139.9284, 121.3904, 145.4289, 158.1318, 157.4630, 150.0894, 183.9047, 181.7129, 114.8939, 9.5253, 29.0257, 97.9543, 122.0798, 72.2969, 26.3086, 81.8050, 67.7463, 72.3737, 129.8539, 113.3900, 141.8177, 225.0000, 35.3458, 173.7635, 29.5101, 135.9482, 188.2391, 225.0000], device='cuda:0'), 'sigmas': tensor([ 9.2432, 23.1060, 16.4965, 6.4289, 24.7862, 23.7996, 15.2424, 20.2845, 12.6833, 6.9079, 6.1214, 22.1317, 9.7907, 5.5302, 14.3288, 11.1454, 16.0453, 20.9057, 24.2358, 13.4785, 22.7258, 11.2053, 12.9420, 13.4270, 14.8660, 22.4874, 5.6251, 9.8794, 8.8749, 19.0294, 9.7164, 6.2293, 13.6376, 11.7447, 14.1414, 6.4362, 20.4575, 14.6729, 8.4719, 14.2926, 6.9458, 11.5346, 14.6113, 6.6516, 22.1767, 8.3793, 20.1699, 6.3299, 5.3340, 21.8027], device='cuda:0')}, 'deform_params': {'affine': {'rotations': array([ 0.0008224 , 0.03067143, -0.0151502 ]), 'shears': array([-0.01735838, 0.00744726, 0.00012507]), 'scalings': array([1.09345725, 0.91695532, 0.98194215])}, 'non_rigid': {'nonlin_scale': array([0.05686841]), 'nonlin_std': 1.048839010036788, 'size_F_small': [15, 15, 15]}, 'flip': False}, 'gamma_params': {'gamma': 0.960299468352801}, 'bf_params': {'bf_scale': array([0.00797334]), 'bf_std': array([0.21896995]), 'bf_size': [2, 2, 2]}, 'resample_params': {'spacing': array([0.65685245, 0.65685245, 0.65685245])}, 'noise_params': {'noise_std': None}, 'generation_time': 0.6192283630371094} ``` Note If a specific parameter is passed in genparams it means that the probability of its application is 100%. The internal prob is not used as the parameter is fixed. If using custom values for the parameters, ensure that the values are within the range of the parameters defined in the class attributes (especially for the spatial deformation parameters, as the grid is pre-defined at class initialization). Furthermore, ensure that the device location and parameter type is consistent with the one in the returned generation_parameters dictionary.","title":"Datasets"},{"location":"datasets/#datasets","text":"Classes for loading and processing datasets. Note \ud83d\udcdd Device : All datasets return samples with tensors on the CPU (even when the synthetic data generation is done on the GPU). This is due to restriction on the GPU usage in the multiprocessing settings, where GPU memory cannot be easily shared between processes. \ud83d\udcdd Dataloader : When using torch.utils.data.DataLoader ensure that you pass multiprocessing_context=\"spawn\" argument to the dataloader object when using FetalSynthDataset to ensure that the spawned processes have access to the GPU.","title":"Datasets"},{"location":"datasets/#fetalsyngen.data.datasets.FetalDataset","text":"Abstract class defining a dataset for loading fetal data. Source code in fetalsyngen/data/datasets.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 class FetalDataset : \"\"\"Abstract class defining a dataset for loading fetal data.\"\"\" def __init__ ( self , bids_path : str , sub_list : list [ str ] | None , ) -> dict : \"\"\" Args: bids_path: Path to the bids folder with the data. sub_list: List of the subjects to use. If None, all subjects are used. \"\"\" super () . __init__ () self . bids_path = Path ( bids_path ) self . subjects = self . find_subjects ( sub_list ) if self . subjects is None : self . subjects = [ x . name for x in self . bids_path . glob ( \"sub-*\" )] self . sub_ses = [ ( x , y ) for x in self . subjects for y in self . _get_ses ( self . bids_path , x ) ] self . loader = SimpleITKReader () self . scaler = ScaleIntensity ( minv = 0 , maxv = 1 ) self . orientation = Orientation ( axcodes = \"RAS\" ) self . img_paths = self . _load_bids_path ( self . bids_path , \"T2w\" ) self . segm_paths = self . _load_bids_path ( self . bids_path , \"dseg\" ) def find_subjects ( self , sub_list ): subj_found = [ x . name for x in Path ( self . bids_path ) . glob ( \"sub-*\" )] return ( list ( set ( subj_found ) & set ( sub_list )) if sub_list is not None else None ) def _sub_ses_string ( self , sub , ses ): return f \" { sub } _ { ses } \" if ses is not None else sub def _sub_ses_idx ( self , idx ): sub , ses = self . sub_ses [ idx ] return self . _sub_ses_string ( sub , ses ) def _get_ses ( self , bids_path , sub ): \"\"\"Get the session names for the subject.\"\"\" sub_path = bids_path / sub ses_dir = [ x for x in sub_path . iterdir () if x . is_dir ()] ses = [] for s in ses_dir : if \"anat\" in s . name : ses . append ( None ) else : ses . append ( s . name ) return sorted ( ses , key = lambda x : x or \"\" ) def _get_pattern ( self , sub , ses , suffix , extension = \".nii.gz\" ): \"\"\"Get the pattern for the file name.\"\"\" if ses is None : return f \" { sub } /anat/ { sub } *_ { suffix }{ extension } \" else : return f \" { sub } / { ses } /anat/ { sub } _ { ses } *_ { suffix }{ extension } \" def _load_bids_path ( self , path , suffix ): \"\"\" \"Check that for a given path, all subjects have a file with the provided suffix \"\"\" files_paths = [] for sub , ses in self . sub_ses : pattern = self . _get_pattern ( sub , ses , suffix ) files = list ( path . glob ( pattern )) if len ( files ) == 0 : raise FileNotFoundError ( f \"No files found for requested subject { sub } in { path } \" f \"( { pattern } returned nothing)\" ) elif len ( files ) > 1 : raise RuntimeError ( f \"Multiple files found for requested subject { sub } in { path } \" f \"( { pattern } returned { files } )\" ) files_paths . append ( files [ 0 ]) return files_paths def __len__ ( self ): return len ( self . subjects ) def __getitem__ ( self , idx ): raise NotImplementedError ( \"This method should be implemented in the child class.\" )","title":"FetalDataset"},{"location":"datasets/#fetalsyngen.data.datasets.FetalDataset.__init__","text":"Parameters: bids_path ( str ) \u2013 Path to the bids folder with the data. sub_list ( list [ str ] | None ) \u2013 List of the subjects to use. If None, all subjects are used. Source code in fetalsyngen/data/datasets.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , bids_path : str , sub_list : list [ str ] | None , ) -> dict : \"\"\" Args: bids_path: Path to the bids folder with the data. sub_list: List of the subjects to use. If None, all subjects are used. \"\"\" super () . __init__ () self . bids_path = Path ( bids_path ) self . subjects = self . find_subjects ( sub_list ) if self . subjects is None : self . subjects = [ x . name for x in self . bids_path . glob ( \"sub-*\" )] self . sub_ses = [ ( x , y ) for x in self . subjects for y in self . _get_ses ( self . bids_path , x ) ] self . loader = SimpleITKReader () self . scaler = ScaleIntensity ( minv = 0 , maxv = 1 ) self . orientation = Orientation ( axcodes = \"RAS\" ) self . img_paths = self . _load_bids_path ( self . bids_path , \"T2w\" ) self . segm_paths = self . _load_bids_path ( self . bids_path , \"dseg\" )","title":"__init__"},{"location":"datasets/#fetalsyngen.data.datasets.FetalTestDataset","text":"Bases: FetalDataset Dataset class for loading fetal images offline. Used to load test/validation data. Use the transforms argument to pass additional processing steps (scaling, resampling, cropping, etc.). Source code in fetalsyngen/data/datasets.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 class FetalTestDataset ( FetalDataset ): \"\"\"Dataset class for loading fetal images offline. Used to load test/validation data. Use the `transforms` argument to pass additional processing steps (scaling, resampling, cropping, etc.). \"\"\" def __init__ ( self , bids_path : str , sub_list : list [ str ] | None , transforms : Compose | None = None , ): \"\"\" Args: bids_path: Path to the bids folder with the data. sub_list: List of the subjects to use. If None, all subjects are used. transforms: Compose object with the transformations to apply. Default is None, no transformations are applied. !!! Note We highle recommend using the `transforms` arguments with at least the re-oriented transform to RAS and the intensity scaling to `[0, 1]` to ensure the data consistency. See [inference.yaml](https://github.com/Medical-Image-Analysis-Laboratory/fetalsyngen/blob/dev/configs/dataset/transforms/inference.yaml) for an example of the transforms configuration. \"\"\" super () . __init__ ( bids_path , sub_list ) self . transforms = transforms def _load_data ( self , idx ): # load the image and segmentation image = self . loader ( self . img_paths [ idx ]) segm = self . loader ( self . segm_paths [ idx ]) if len ( image . shape ) == 3 : # add channel dimension image = image . unsqueeze ( 0 ) segm = segm . unsqueeze ( 0 ) elif len ( image . shape ) != 4 : raise ValueError ( f \"Expected 3D or 4D image, got { len ( image . shape ) } D image.\" ) # transform name into a single string otherwise collate fails name = self . sub_ses [ idx ] name = self . _sub_ses_string ( name [ 0 ], ses = name [ 1 ]) return { \"image\" : image , \"label\" : segm . long (), \"name\" : name } def __getitem__ ( self , idx ) -> dict : \"\"\" Returns: Dictionary with the `image` , `label` and the `name` keys. `image` and `label` are `torch.float32` [`monai.data.meta_tensor.MetaTensor`](https://docs.monai.io/en/stable/data.html#metatensor) instances with dimensions `(1, H, W, D)` and `name` is a string of a format `sub_ses` where `sub` is the subject name and `ses` is the session name. \"\"\" data = self . _load_data ( idx ) if self . transforms : data = self . transforms ( data ) return data def reverse_transform ( self , data : dict ) -> dict : \"\"\"Reverse the transformations applied to the data. Args: data: Dictionary with the `image` and `label` keys, like the one returned by the `__getitem__` method. Returns: Dictionary with the `image` and `label` keys where the transformations are reversed. \"\"\" if self . transforms : data = self . transforms . inverse ( data ) return data","title":"FetalTestDataset"},{"location":"datasets/#fetalsyngen.data.datasets.FetalTestDataset.__init__","text":"Parameters: bids_path ( str ) \u2013 Path to the bids folder with the data. sub_list ( list [ str ] | None ) \u2013 List of the subjects to use. If None, all subjects are used. transforms ( Compose | None , default: None ) \u2013 Compose object with the transformations to apply. Default is None, no transformations are applied. Note We highle recommend using the transforms arguments with at least the re-oriented transform to RAS and the intensity scaling to [0, 1] to ensure the data consistency. See inference.yaml for an example of the transforms configuration. Source code in fetalsyngen/data/datasets.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def __init__ ( self , bids_path : str , sub_list : list [ str ] | None , transforms : Compose | None = None , ): \"\"\" Args: bids_path: Path to the bids folder with the data. sub_list: List of the subjects to use. If None, all subjects are used. transforms: Compose object with the transformations to apply. Default is None, no transformations are applied. !!! Note We highle recommend using the `transforms` arguments with at least the re-oriented transform to RAS and the intensity scaling to `[0, 1]` to ensure the data consistency. See [inference.yaml](https://github.com/Medical-Image-Analysis-Laboratory/fetalsyngen/blob/dev/configs/dataset/transforms/inference.yaml) for an example of the transforms configuration. \"\"\" super () . __init__ ( bids_path , sub_list ) self . transforms = transforms","title":"__init__"},{"location":"datasets/#fetalsyngen.data.datasets.FetalTestDataset.__getitem__","text":"Returns: dict \u2013 Dictionary with the image , label and the name keys. image and label are torch.float32 monai.data.meta_tensor.MetaTensor instances with dimensions (1, H, W, D) and name is a string of a format sub_ses where sub is the subject name and ses is the session name. Source code in fetalsyngen/data/datasets.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def __getitem__ ( self , idx ) -> dict : \"\"\" Returns: Dictionary with the `image` , `label` and the `name` keys. `image` and `label` are `torch.float32` [`monai.data.meta_tensor.MetaTensor`](https://docs.monai.io/en/stable/data.html#metatensor) instances with dimensions `(1, H, W, D)` and `name` is a string of a format `sub_ses` where `sub` is the subject name and `ses` is the session name. \"\"\" data = self . _load_data ( idx ) if self . transforms : data = self . transforms ( data ) return data","title":"__getitem__"},{"location":"datasets/#fetalsyngen.data.datasets.FetalTestDataset.reverse_transform","text":"Reverse the transformations applied to the data. Parameters: data ( dict ) \u2013 Dictionary with the image and label keys, like the one returned by the __getitem__ method. Returns: dict \u2013 Dictionary with the image and label keys where the transformations are reversed. Source code in fetalsyngen/data/datasets.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def reverse_transform ( self , data : dict ) -> dict : \"\"\"Reverse the transformations applied to the data. Args: data: Dictionary with the `image` and `label` keys, like the one returned by the `__getitem__` method. Returns: Dictionary with the `image` and `label` keys where the transformations are reversed. \"\"\" if self . transforms : data = self . transforms . inverse ( data ) return data","title":"reverse_transform"},{"location":"datasets/#fetalsyngen.data.datasets.FetalSynthDataset","text":"Bases: FetalDataset Dataset class for generating/augmenting on-the-fly fetal images\" Source code in fetalsyngen/data/datasets.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 class FetalSynthDataset ( FetalDataset ): \"\"\"Dataset class for generating/augmenting on-the-fly fetal images\" \"\"\" def __init__ ( self , bids_path : str , generator : FetalSynthGen , seed_path : str | None , sub_list : list [ str ] | None , load_image : bool = False , image_as_intensity : bool = False , ): \"\"\" Args: bids_path: Path to the bids-formatted folder with the data. seed_path: Path to the folder with the seeds to use for intensity sampling. See `scripts/seed_generation.py` for details on the data formatting. If seed_path is None, the intensity sampling step is skipped and the output image intensities will be based on the input image. generator: a class object defining a generator to use. sub_list: List of the subjects to use. If None, all subjects are used. load_image: If **True**, the image is loaded and passed to the generator, where it can be used as the intensity prior instead of a random intensity sampling or spatially deformed with the same transformation field as segmentation and the syntehtic image. Default is **False**. image_as_intensity: If **True**, the image is used as the intensity prior, instead of sampling the intensities from the seeds. Default is **False**. \"\"\" super () . __init__ ( bids_path , sub_list ) self . seed_path = ( Path ( seed_path ) if isinstance ( seed_path , str ) else None ) self . load_image = load_image self . generator = generator self . image_as_intensity = image_as_intensity # parse seeds paths if not self . image_as_intensity and isinstance ( self . seed_path , Path ): if not self . seed_path . exists (): raise FileNotFoundError ( f \"Provided seed path { self . seed_path } does not exist.\" ) else : self . _load_seed_path () def _load_seed_path ( self ): \"\"\"Load the seeds for the subjects.\"\"\" self . seed_paths = { self . _sub_ses_string ( sub , ses ): defaultdict ( dict ) for ( sub , ses ) in self . sub_ses } avail_seeds = [ int ( x . name . replace ( \"subclasses_\" , \"\" )) for x in self . seed_path . glob ( \"subclasses_*\" ) ] min_seeds_available = min ( avail_seeds ) max_seeds_available = max ( avail_seeds ) for n_sub in range ( min_seeds_available , max_seeds_available + 1 , ): seed_path = self . seed_path / f \"subclasses_ { n_sub } \" if not seed_path . exists (): raise FileNotFoundError ( f \"Provided seed path { seed_path } does not exist.\" ) # load the seeds for the subjects for each meta label 1-4 for i in range ( 1 , 5 ): files = self . _load_bids_path ( seed_path , f \"mlabel_ { i } \" ) for ( sub , ses ), file in zip ( self . sub_ses , files ): sub_ses_str = self . _sub_ses_string ( sub , ses ) self . seed_paths [ sub_ses_str ][ n_sub ][ i ] = file def sample ( self , idx , genparams : dict = {}) -> tuple [ dict , dict ]: \"\"\" Retrieve a single item from the dataset at the specified index. Args: idx (int): The index of the item to retrieve. genparams (dict): Dictionary with generation parameters. Used for fixed generation. Should follow exactly the same structure and be of the same type as the returned generation parameters. Can be used to replicate the augmentations (power) used for the generation of a specific sample. Returns: Dictionaries with the generated data and the generation parameters. First dictionary contains the `image`, `label` and the `name` keys. The second dictionary contains the parameters used for the generation. !!! Note The `image` is scaled to `[0, 1]` and oriented with the `label` to **RAS** and returned on the device specified in the `generator` initialization. \"\"\" # use generation_params to track the parameters used for the generation generation_params = {} image = self . loader ( self . img_paths [ idx ]) if self . load_image else None segm = self . loader ( self . segm_paths [ idx ]) # orient to RAS for consistency image = ( self . orientation ( image . unsqueeze ( 0 )) . squeeze ( 0 ) if self . load_image else None ) segm = self . orientation ( segm . unsqueeze ( 0 )) . squeeze ( 0 ) # transform name into a single string otherwise collate fails name = self . sub_ses [ idx ] name = self . _sub_ses_string ( name [ 0 ], ses = name [ 1 ]) # initialize seeds as dictionary # with paths to the seeds volumes # or None if image is to be used as intensity prior if self . seed_path is not None : seeds = self . seed_paths [ name ] if self . image_as_intensity : seeds = None # log input data generation_params [ \"idx\" ] = idx generation_params [ \"img_paths\" ] = str ( self . img_paths [ idx ]) generation_params [ \"segm_paths\" ] = str ( self . img_paths [ idx ]) generation_params [ \"seeds\" ] = str ( self . seed_path ) generation_time_start = time . time () # generate the synthetic data gen_output , segmentation , image , synth_params = self . generator . sample ( image = image , segmentation = segm , seeds = seeds , genparams = genparams ) # scale the images to [0, 1] gen_output = self . scaler ( gen_output ) image = self . scaler ( image ) if image is not None else None # ensure image and segmentation are on the cpu gen_output = gen_output . cpu () segmentation = segmentation . cpu () image = image . cpu () if image is not None else None generation_params = { ** generation_params , ** synth_params } generation_params [ \"generation_time\" ] = ( time . time () - generation_time_start ) data_out = { \"image\" : gen_output . unsqueeze ( 0 ), \"label\" : segmentation . unsqueeze ( 0 ) . long (), \"name\" : name , } return data_out , generation_params def __getitem__ ( self , idx ) -> dict : \"\"\" Retrieve a single item from the dataset at the specified index. Args: idx (int): The index of the item to retrieve. Returns: Dictionary with the `image`, `label` and the `name` keys. `image` and `label` are `torch.float32` [`monai.data.meta_tensor.MetaTensor`](https://docs.monai.io/en/stable/data.html#metatensor) and `name` is a string of a format `sub_ses` where `sub` is the subject name and `ses` is the session name. !!!Note The `image` is scaled to `[0, 1]` and oriented to **RAS** and returned on the device specified in the `generator` initialization. \"\"\" data_out , generation_params = self . sample ( idx ) self . generation_params = generation_params return data_out def sample_with_meta ( self , idx : int , genparams : dict = {}) -> dict : \"\"\" Retrieve a sample along with its generation parameters and store them in the same dictionary. Args: idx: The index of the sample to retrieve. genparams: Dictionary with generation parameters. Used for fixed generation. Should follow exactly the same structure and be of the same type as the returned generation parameters from the `sample()` method. Can be used to replicate the augmentations (power) used for the generation of a specific sample. Returns: A dictionary with `image`, `label`, `name` and `generation_params` keys. \"\"\" data , generation_params = self . sample ( idx , genparams = genparams ) data [ \"generation_params\" ] = generation_params return data","title":"FetalSynthDataset"},{"location":"datasets/#fetalsyngen.data.datasets.FetalSynthDataset.__init__","text":"Parameters: bids_path ( str ) \u2013 Path to the bids-formatted folder with the data. seed_path ( str | None ) \u2013 Path to the folder with the seeds to use for intensity sampling. See scripts/seed_generation.py for details on the data formatting. If seed_path is None, the intensity sampling step is skipped and the output image intensities will be based on the input image. generator ( FetalSynthGen ) \u2013 a class object defining a generator to use. sub_list ( list [ str ] | None ) \u2013 List of the subjects to use. If None, all subjects are used. load_image ( bool , default: False ) \u2013 If True , the image is loaded and passed to the generator, where it can be used as the intensity prior instead of a random intensity sampling or spatially deformed with the same transformation field as segmentation and the syntehtic image. Default is False . image_as_intensity ( bool , default: False ) \u2013 If True , the image is used as the intensity prior, instead of sampling the intensities from the seeds. Default is False . Source code in fetalsyngen/data/datasets.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def __init__ ( self , bids_path : str , generator : FetalSynthGen , seed_path : str | None , sub_list : list [ str ] | None , load_image : bool = False , image_as_intensity : bool = False , ): \"\"\" Args: bids_path: Path to the bids-formatted folder with the data. seed_path: Path to the folder with the seeds to use for intensity sampling. See `scripts/seed_generation.py` for details on the data formatting. If seed_path is None, the intensity sampling step is skipped and the output image intensities will be based on the input image. generator: a class object defining a generator to use. sub_list: List of the subjects to use. If None, all subjects are used. load_image: If **True**, the image is loaded and passed to the generator, where it can be used as the intensity prior instead of a random intensity sampling or spatially deformed with the same transformation field as segmentation and the syntehtic image. Default is **False**. image_as_intensity: If **True**, the image is used as the intensity prior, instead of sampling the intensities from the seeds. Default is **False**. \"\"\" super () . __init__ ( bids_path , sub_list ) self . seed_path = ( Path ( seed_path ) if isinstance ( seed_path , str ) else None ) self . load_image = load_image self . generator = generator self . image_as_intensity = image_as_intensity # parse seeds paths if not self . image_as_intensity and isinstance ( self . seed_path , Path ): if not self . seed_path . exists (): raise FileNotFoundError ( f \"Provided seed path { self . seed_path } does not exist.\" ) else : self . _load_seed_path ()","title":"__init__"},{"location":"datasets/#fetalsyngen.data.datasets.FetalSynthDataset.sample","text":"Retrieve a single item from the dataset at the specified index. Parameters: idx ( int ) \u2013 The index of the item to retrieve. genparams ( dict , default: {} ) \u2013 Dictionary with generation parameters. Used for fixed generation. Should follow exactly the same structure and be of the same type as the returned generation parameters. Can be used to replicate the augmentations (power) used for the generation of a specific sample. Returns: Dictionaries with the generated data and the generation parameters. First dictionary contains the image , label and the name keys. The second dictionary contains the parameters used for the generation. Note The image is scaled to [0, 1] and oriented with the label to RAS and returned on the device specified in the generator initialization. Source code in fetalsyngen/data/datasets.py 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 def sample ( self , idx , genparams : dict = {}) -> tuple [ dict , dict ]: \"\"\" Retrieve a single item from the dataset at the specified index. Args: idx (int): The index of the item to retrieve. genparams (dict): Dictionary with generation parameters. Used for fixed generation. Should follow exactly the same structure and be of the same type as the returned generation parameters. Can be used to replicate the augmentations (power) used for the generation of a specific sample. Returns: Dictionaries with the generated data and the generation parameters. First dictionary contains the `image`, `label` and the `name` keys. The second dictionary contains the parameters used for the generation. !!! Note The `image` is scaled to `[0, 1]` and oriented with the `label` to **RAS** and returned on the device specified in the `generator` initialization. \"\"\" # use generation_params to track the parameters used for the generation generation_params = {} image = self . loader ( self . img_paths [ idx ]) if self . load_image else None segm = self . loader ( self . segm_paths [ idx ]) # orient to RAS for consistency image = ( self . orientation ( image . unsqueeze ( 0 )) . squeeze ( 0 ) if self . load_image else None ) segm = self . orientation ( segm . unsqueeze ( 0 )) . squeeze ( 0 ) # transform name into a single string otherwise collate fails name = self . sub_ses [ idx ] name = self . _sub_ses_string ( name [ 0 ], ses = name [ 1 ]) # initialize seeds as dictionary # with paths to the seeds volumes # or None if image is to be used as intensity prior if self . seed_path is not None : seeds = self . seed_paths [ name ] if self . image_as_intensity : seeds = None # log input data generation_params [ \"idx\" ] = idx generation_params [ \"img_paths\" ] = str ( self . img_paths [ idx ]) generation_params [ \"segm_paths\" ] = str ( self . img_paths [ idx ]) generation_params [ \"seeds\" ] = str ( self . seed_path ) generation_time_start = time . time () # generate the synthetic data gen_output , segmentation , image , synth_params = self . generator . sample ( image = image , segmentation = segm , seeds = seeds , genparams = genparams ) # scale the images to [0, 1] gen_output = self . scaler ( gen_output ) image = self . scaler ( image ) if image is not None else None # ensure image and segmentation are on the cpu gen_output = gen_output . cpu () segmentation = segmentation . cpu () image = image . cpu () if image is not None else None generation_params = { ** generation_params , ** synth_params } generation_params [ \"generation_time\" ] = ( time . time () - generation_time_start ) data_out = { \"image\" : gen_output . unsqueeze ( 0 ), \"label\" : segmentation . unsqueeze ( 0 ) . long (), \"name\" : name , } return data_out , generation_params","title":"sample"},{"location":"datasets/#fetalsyngen.data.datasets.FetalSynthDataset.__getitem__","text":"Retrieve a single item from the dataset at the specified index. Parameters: idx ( int ) \u2013 The index of the item to retrieve. Returns: dict \u2013 Dictionary with the image , label and the name keys. image and label are torch.float32 monai.data.meta_tensor.MetaTensor and name is a string of a format sub_ses where sub is the subject name and ses is the session name. Note The image is scaled to [0, 1] and oriented to RAS and returned on the device specified in the generator initialization. Source code in fetalsyngen/data/datasets.py 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 def __getitem__ ( self , idx ) -> dict : \"\"\" Retrieve a single item from the dataset at the specified index. Args: idx (int): The index of the item to retrieve. Returns: Dictionary with the `image`, `label` and the `name` keys. `image` and `label` are `torch.float32` [`monai.data.meta_tensor.MetaTensor`](https://docs.monai.io/en/stable/data.html#metatensor) and `name` is a string of a format `sub_ses` where `sub` is the subject name and `ses` is the session name. !!!Note The `image` is scaled to `[0, 1]` and oriented to **RAS** and returned on the device specified in the `generator` initialization. \"\"\" data_out , generation_params = self . sample ( idx ) self . generation_params = generation_params return data_out","title":"__getitem__"},{"location":"datasets/#fetalsyngen.data.datasets.FetalSynthDataset.sample_with_meta","text":"Retrieve a sample along with its generation parameters and store them in the same dictionary. Parameters: idx ( int ) \u2013 The index of the sample to retrieve. genparams ( dict , default: {} ) \u2013 Dictionary with generation parameters. Used for fixed generation. Should follow exactly the same structure and be of the same type as the returned generation parameters from the sample() method. Can be used to replicate the augmentations (power) used for the generation of a specific sample. Returns: dict \u2013 A dictionary with image , label , name and generation_params keys. Source code in fetalsyngen/data/datasets.py 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 def sample_with_meta ( self , idx : int , genparams : dict = {}) -> dict : \"\"\" Retrieve a sample along with its generation parameters and store them in the same dictionary. Args: idx: The index of the sample to retrieve. genparams: Dictionary with generation parameters. Used for fixed generation. Should follow exactly the same structure and be of the same type as the returned generation parameters from the `sample()` method. Can be used to replicate the augmentations (power) used for the generation of a specific sample. Returns: A dictionary with `image`, `label`, `name` and `generation_params` keys. \"\"\" data , generation_params = self . sample ( idx , genparams = genparams ) data [ \"generation_params\" ] = generation_params return data","title":"sample_with_meta"},{"location":"datasets/#fixed-image-generation","text":"It is possible to generate synthetic images of the same 'augmentation' power as any given synthetic image. This is done by passing the genparams dictionary to the sample_with_meta (or sample ) method of the FetalSynthDataset class. The generation_params dictionary is a dictionary of the parameters used to generate the image. The method will then use these parameters to generate a new image with the same augmentation power as the original image. This genparams dictionary can be obtained, for example, from the dictionary returned by the FetalSynthDataset.sample_with_meta method. It then can be directly used to fix (some or all) generation parameters for the new image. See example below: # initialize the dataset class # see the Examples page for more details dataset = FetalSynthDataset ( ... ) # first sample a synthetic image from the dataset sample = dataset . sample_with_meta ( 0 ) # then we sample a synthetic image with the same augmentation power as the first image sample_copy = dataset . sample_with_meta ( 0 , genparams = sample [ \"generation_params\" ]) For example, generation parameters of the first image can be like this: { 'idx' : 0 , 'img_paths' : PosixPath ( '../data/sub-sta38/anat/sub-sta38_rec-irtk_T2w.nii.gz' ), 'segm_paths' : PosixPath ( '../data/sub-sta38/anat/sub-sta38_rec-irtk_T2w.nii.gz' ), 'seeds' : defaultdict ( dict , { 1 : { 1 : PosixPath ( '../data/derivatives/seeds/subclasses_1/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_1.nii.gz' ), 2 : PosixPath ( '../data/derivatives/seeds/subclasses_1/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_2.nii.gz' ), 3 : PosixPath ( '../data/derivatives/seeds/subclasses_1/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_3.nii.gz' ), 4 : PosixPath ( '../data/derivatives/seeds/subclasses_1/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_4.nii.gz' )}, 2 : { 1 : PosixPath ( '../data/derivatives/seeds/subclasses_2/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_1.nii.gz' ), 2 : PosixPath ( '../data/derivatives/seeds/subclasses_2/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_2.nii.gz' ), 3 : PosixPath ( '../data/derivatives/seeds/subclasses_2/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_3.nii.gz' ), 4 : PosixPath ( '../data/derivatives/seeds/subclasses_2/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_4.nii.gz' )}, 3 : { 1 : PosixPath ( '../data/derivatives/seeds/subclasses_3/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_1.nii.gz' ), 2 : PosixPath ( '../data/derivatives/seeds/subclasses_3/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_2.nii.gz' ), 3 : PosixPath ( '../data/derivatives/seeds/subclasses_3/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_3.nii.gz' ), 4 : PosixPath ( '../data/derivatives/seeds/subclasses_3/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_4.nii.gz' )}}), 'selected_seeds' : { 'mlabel2subclusters' : { 1 : 2 , 2 : 1 , 3 : 3 , 4 : 1 }}, 'seed_intensities' : { 'mus' : tensor ([ 109.6722 , 220.9658 , 100.9801 , 38.6364 , 125.5148 , 108.1950 , 216.1060 , 190.5462 , 55.3930 , 59.2667 , 72.0628 , 68.8775 , 76.5113 , 84.6639 , 90.0124 , 94.1701 , 67.0610 , 25.9465 , 31.5438 , 21.0375 , 192.4223 , 173.7434 , 139.9284 , 121.3904 , 145.4289 , 158.1318 , 157.4630 , 150.0894 , 183.9047 , 181.7129 , 114.8939 , 9.5253 , 29.0257 , 97.9543 , 122.0798 , 72.2969 , 26.3086 , 81.8050 , 67.7463 , 72.3737 , 129.8539 , 113.3900 , 141.8177 , 225.0000 , 35.3458 , 173.7635 , 29.5101 , 135.9482 , 188.2391 , 225.0000 ], device = 'cuda:0' ), 'sigmas' : tensor ([ 9.2432 , 23.1060 , 16.4965 , 6.4289 , 24.7862 , 23.7996 , 15.2424 , 20.2845 , 12.6833 , 6.9079 , 6.1214 , 22.1317 , 9.7907 , 5.5302 , 14.3288 , 11.1454 , 16.0453 , 20.9057 , 24.2358 , 13.4785 , 22.7258 , 11.2053 , 12.9420 , 13.4270 , 14.8660 , 22.4874 , 5.6251 , 9.8794 , 8.8749 , 19.0294 , 9.7164 , 6.2293 , 13.6376 , 11.7447 , 14.1414 , 6.4362 , 20.4575 , 14.6729 , 8.4719 , 14.2926 , 6.9458 , 11.5346 , 14.6113 , 6.6516 , 22.1767 , 8.3793 , 20.1699 , 6.3299 , 5.3340 , 21.8027 ], device = 'cuda:0' )}, 'deform_params' : { 'affine' : { 'rotations' : array ([ 0.0008224 , 0.03067143 , - 0.0151502 ]), 'shears' : array ([ - 0.01735838 , 0.00744726 , 0.00012507 ]), 'scalings' : array ([ 1.09345725 , 0.91695532 , 0.98194215 ])}, 'non_rigid' : { 'nonlin_scale' : array ([ 0.05686841 ]), 'nonlin_std' : 1.048839010036788 , 'size_F_small' : [ 15 , 15 , 15 ]}, 'flip' : False }, 'gamma_params' : { 'gamma' : 0.960299468352801 }, 'bf_params' : { 'bf_scale' : None , 'bf_std' : None , 'bf_size' : None }, 'resample_params' : { 'spacing' : array ([ 0.65685245 , 0.65685245 , 0.65685245 ])}, 'noise_params' : { 'noise_std' : None }, 'generation_time' : 0.5615839958190918 } If the key:value pair exists in the passed genparams dictionary, the sample method will use directly the value from the genparams dictionary. If the key:value pair does not exist in the genparams dictionary or it is None , sample method will generate the value randomly, using the corresponding class attributes. See how the keys bf_scale , bf_std , bf_size and noise_std have not been defined in the genparams dictionary above. This means that the sample method will generate these values randomly. The same could have been achieved by not passing them at all. {'idx': 0, 'img_paths': PosixPath('../data/sub-sta38/anat/sub-sta38_rec-irtk_T2w.nii.gz'), 'segm_paths': PosixPath('../data/sub-sta38/anat/sub-sta38_rec-irtk_T2w.nii.gz'), 'seeds': defaultdict(dict, {1: {1: PosixPath('../data/derivatives/seeds/subclasses_1/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_1.nii.gz'), 2: PosixPath('../data/derivatives/seeds/subclasses_1/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_2.nii.gz'), 3: PosixPath('../data/derivatives/seeds/subclasses_1/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_3.nii.gz'), 4: PosixPath('../data/derivatives/seeds/subclasses_1/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_4.nii.gz')}, 2: {1: PosixPath('../data/derivatives/seeds/subclasses_2/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_1.nii.gz'), 2: PosixPath('../data/derivatives/seeds/subclasses_2/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_2.nii.gz'), 3: PosixPath('../data/derivatives/seeds/subclasses_2/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_3.nii.gz'), 4: PosixPath('../data/derivatives/seeds/subclasses_2/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_4.nii.gz')}, 3: {1: PosixPath('../data/derivatives/seeds/subclasses_3/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_1.nii.gz'), 2: PosixPath('../data/derivatives/seeds/subclasses_3/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_2.nii.gz'), 3: PosixPath('../data/derivatives/seeds/subclasses_3/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_3.nii.gz'), 4: PosixPath('../data/derivatives/seeds/subclasses_3/sub-sta38/anat/sub-sta38_rec-irtk_T2w_dseg_mlabel_4.nii.gz')}}), 'selected_seeds': {'mlabel2subclusters': {1: 2, 2: 1, 3: 3, 4: 1}}, 'seed_intensities': {'mus': tensor([109.6722, 220.9658, 100.9801, 38.6364, 125.5148, 108.1950, 216.1060, 190.5462, 55.3930, 59.2667, 72.0628, 68.8775, 76.5113, 84.6639, 90.0124, 94.1701, 67.0610, 25.9465, 31.5438, 21.0375, 192.4223, 173.7434, 139.9284, 121.3904, 145.4289, 158.1318, 157.4630, 150.0894, 183.9047, 181.7129, 114.8939, 9.5253, 29.0257, 97.9543, 122.0798, 72.2969, 26.3086, 81.8050, 67.7463, 72.3737, 129.8539, 113.3900, 141.8177, 225.0000, 35.3458, 173.7635, 29.5101, 135.9482, 188.2391, 225.0000], device='cuda:0'), 'sigmas': tensor([ 9.2432, 23.1060, 16.4965, 6.4289, 24.7862, 23.7996, 15.2424, 20.2845, 12.6833, 6.9079, 6.1214, 22.1317, 9.7907, 5.5302, 14.3288, 11.1454, 16.0453, 20.9057, 24.2358, 13.4785, 22.7258, 11.2053, 12.9420, 13.4270, 14.8660, 22.4874, 5.6251, 9.8794, 8.8749, 19.0294, 9.7164, 6.2293, 13.6376, 11.7447, 14.1414, 6.4362, 20.4575, 14.6729, 8.4719, 14.2926, 6.9458, 11.5346, 14.6113, 6.6516, 22.1767, 8.3793, 20.1699, 6.3299, 5.3340, 21.8027], device='cuda:0')}, 'deform_params': {'affine': {'rotations': array([ 0.0008224 , 0.03067143, -0.0151502 ]), 'shears': array([-0.01735838, 0.00744726, 0.00012507]), 'scalings': array([1.09345725, 0.91695532, 0.98194215])}, 'non_rigid': {'nonlin_scale': array([0.05686841]), 'nonlin_std': 1.048839010036788, 'size_F_small': [15, 15, 15]}, 'flip': False}, 'gamma_params': {'gamma': 0.960299468352801}, 'bf_params': {'bf_scale': array([0.00797334]), 'bf_std': array([0.21896995]), 'bf_size': [2, 2, 2]}, 'resample_params': {'spacing': array([0.65685245, 0.65685245, 0.65685245])}, 'noise_params': {'noise_std': None}, 'generation_time': 0.6192283630371094} ``` Note If a specific parameter is passed in genparams it means that the probability of its application is 100%. The internal prob is not used as the parameter is fixed. If using custom values for the parameters, ensure that the values are within the range of the parameters defined in the class attributes (especially for the spatial deformation parameters, as the grid is pre-defined at class initialization). Furthermore, ensure that the device location and parameter type is consistent with the one in the returned generation_parameters dictionary.","title":"Fixed Image Generation"},{"location":"examples/","text":"Examples After installing the package, you can directly use the generator and datasets in your project. See the following examples for guidance on how to instantiate the generator and datasets. Recommended: Using Configuration Files For reproducibility and greater flexibility, we recommend using the configuration files provided in the package. These files define the parameters for the generator and datasets, allowing for quick and easy setup with hydra . Steps to Use Configuration Files Copy the configuration files (entire configs/dataset folder) to your project root directory into configs/dataset . Use the following methods to instantiate the generator and dataset classes: For examples below, set up cfg_path = \"configs/dataset\" as the path to the configuration files and cfg_name as the name of the configuration file you want to use ( cfg_name='synth_train' for example for the synthetic training dataset). See the Configs page for detailed information on configuration files and available generation modes. Using the Imperative API import hydra with hydra . initialize ( config_path = cfg_path , version_base = \"1.2\" ): cfg = hydra . compose ( config_name = cfg_name ) print ( f \"Composed config: { cfg } \" ) dataset = hydra . utils . instantiate ( cfg ) Using the Declarative API import hydra from omegaconf import DictConfig @hydra . main ( config_path = cfg_path , config_name = cfg_name ) def my_app ( cfg : DictConfig ) -> None : print ( cfg ) Note Ensure that the bids and seeds paths in the configuration files are updated to the absolute paths for your data. Using Direct Instantiation You can manually instantiate required classes from the FetalSynthGen in your project as needed. For example, to instantiate the FetalSynthDataset class and generator components follow the example below: # Import necessary classes from fetalsyngen.data.datasets import FetalSynthDataset from fetalsyngen.generator.model import FetalSynthGen from fetalsyngen.generator.augmentation.synthseg import ( RandBiasField , RandGamma , RandNoise , RandResample , ) from fetalsyngen.generator.deformation.affine_nonrigid import SpatialDeformation from fetalsyngen.generator.intensity.rand_gmm import ImageFromSeeds # Instantiate the generator components intensity_generator = ImageFromSeeds ( min_subclusters = 1 , max_subclusters = 3 , seed_labels = [ 1 , 2 , 3 , 4 , 5 ], generation_classes = [ 1 , 2 , 3 , 4 , 5 ], meta_labels = 4 , ) spatial_deform = SpatialDeformation ( max_rotation = 10 , max_shear = 1 , max_scaling = 1 , size = ( 256 , 256 , 256 ), nonlinear_transform = 1 , nonlin_scale_min = 1 , nonlin_scale_max = 1 , nonlin_std_max = 1 , flip_prb = 1 , device = \"cuda\" , ) resampler = RandResample ( prob = 0.5 , max_resolution = 1.5 , min_resolution = 0.5 ) bias_field = RandBiasField ( prob = 0.5 , scale_min = 0.5 , scale_max = 1.5 , std_min = 0.5 , std_max = 1.5 ) noise = RandNoise ( prob = 0.5 , std_min = 0.5 , std_max = 1.5 ) gamma = RandGamma ( prob = 0.5 , gamma_std = 0.5 ) # Instantiate the generator generator = FetalSynthGen ( shape = ( 256 , 256 , 256 ), resolution = ( 0.5 , 0.5 , 0.5 ), device = \"cuda\" , intensity_generator = intensity_generator , spatial_deform = spatial_deform , resampler = resampler , bias_field = bias_field , noise = noise , gamma = gamma , ) # Instantiate the dataset dataset = FetalSynthDataset ( bids_path = \"./../../data\" , generator = generator , seed_path = \"./../../data/derivatives/seeds\" , sub_list = None , ) Additional Resources For more examples of generator instantiation with hydra , see the Generator Instantiation notebook.","title":"Examples"},{"location":"examples/#examples","text":"After installing the package, you can directly use the generator and datasets in your project. See the following examples for guidance on how to instantiate the generator and datasets.","title":"Examples"},{"location":"examples/#recommended-using-configuration-files","text":"For reproducibility and greater flexibility, we recommend using the configuration files provided in the package. These files define the parameters for the generator and datasets, allowing for quick and easy setup with hydra .","title":"Recommended: Using Configuration Files"},{"location":"examples/#steps-to-use-configuration-files","text":"Copy the configuration files (entire configs/dataset folder) to your project root directory into configs/dataset . Use the following methods to instantiate the generator and dataset classes: For examples below, set up cfg_path = \"configs/dataset\" as the path to the configuration files and cfg_name as the name of the configuration file you want to use ( cfg_name='synth_train' for example for the synthetic training dataset). See the Configs page for detailed information on configuration files and available generation modes. Using the Imperative API import hydra with hydra . initialize ( config_path = cfg_path , version_base = \"1.2\" ): cfg = hydra . compose ( config_name = cfg_name ) print ( f \"Composed config: { cfg } \" ) dataset = hydra . utils . instantiate ( cfg ) Using the Declarative API import hydra from omegaconf import DictConfig @hydra . main ( config_path = cfg_path , config_name = cfg_name ) def my_app ( cfg : DictConfig ) -> None : print ( cfg ) Note Ensure that the bids and seeds paths in the configuration files are updated to the absolute paths for your data.","title":"Steps to Use Configuration Files"},{"location":"examples/#using-direct-instantiation","text":"You can manually instantiate required classes from the FetalSynthGen in your project as needed. For example, to instantiate the FetalSynthDataset class and generator components follow the example below: # Import necessary classes from fetalsyngen.data.datasets import FetalSynthDataset from fetalsyngen.generator.model import FetalSynthGen from fetalsyngen.generator.augmentation.synthseg import ( RandBiasField , RandGamma , RandNoise , RandResample , ) from fetalsyngen.generator.deformation.affine_nonrigid import SpatialDeformation from fetalsyngen.generator.intensity.rand_gmm import ImageFromSeeds # Instantiate the generator components intensity_generator = ImageFromSeeds ( min_subclusters = 1 , max_subclusters = 3 , seed_labels = [ 1 , 2 , 3 , 4 , 5 ], generation_classes = [ 1 , 2 , 3 , 4 , 5 ], meta_labels = 4 , ) spatial_deform = SpatialDeformation ( max_rotation = 10 , max_shear = 1 , max_scaling = 1 , size = ( 256 , 256 , 256 ), nonlinear_transform = 1 , nonlin_scale_min = 1 , nonlin_scale_max = 1 , nonlin_std_max = 1 , flip_prb = 1 , device = \"cuda\" , ) resampler = RandResample ( prob = 0.5 , max_resolution = 1.5 , min_resolution = 0.5 ) bias_field = RandBiasField ( prob = 0.5 , scale_min = 0.5 , scale_max = 1.5 , std_min = 0.5 , std_max = 1.5 ) noise = RandNoise ( prob = 0.5 , std_min = 0.5 , std_max = 1.5 ) gamma = RandGamma ( prob = 0.5 , gamma_std = 0.5 ) # Instantiate the generator generator = FetalSynthGen ( shape = ( 256 , 256 , 256 ), resolution = ( 0.5 , 0.5 , 0.5 ), device = \"cuda\" , intensity_generator = intensity_generator , spatial_deform = spatial_deform , resampler = resampler , bias_field = bias_field , noise = noise , gamma = gamma , ) # Instantiate the dataset dataset = FetalSynthDataset ( bids_path = \"./../../data\" , generator = generator , seed_path = \"./../../data/derivatives/seeds\" , sub_list = None , )","title":"Using Direct Instantiation"},{"location":"examples/#additional-resources","text":"For more examples of generator instantiation with hydra , see the Generator Instantiation notebook.","title":"Additional Resources"},{"location":"generation/","text":"Generation API FetalSynthGen Source code in fetalsyngen/generator/model.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 class FetalSynthGen : def __init__ ( self , shape : Iterable [ int ], resolution : Iterable [ float ], device : str , intensity_generator : ImageFromSeeds , spatial_deform : SpatialDeformation , resampler : RandResample , bias_field : RandBiasField , noise : RandNoise , gamma : RandGamma , # optional SR artifacts blur_cortex : BlurCortex | None = None , struct_noise : StructNoise | None = None , simulate_motion : SimulateMotion | None = None , boundaries : SimulatedBoundaries | None = None , ): \"\"\" Initialize the model with the given parameters. !!!Note Augmentations related to SR artifacts are optional and can be set to None if not needed. Args: shape: Shape of the output image. resolution: Resolution of the output image. device: Device to use for computation. intensity_generator: Intensity generator. spatial_deform: Spatial deformation generator. resampler: Resampler. bias_field: Bias field generator. noise: Noise generator. gamma: Gamma correction generator. blur_cortex: Cortex blurring generator. struct_noise: Structural noise generator. simulate_motion: Motion simulation generator. boundaries: Boundaries generator \"\"\" self . shape = shape self . resolution = resolution self . intensity_generator = intensity_generator self . spatial_deform = spatial_deform self . resampled = resampler self . biasfield = bias_field self . gamma = gamma self . noise = noise self . artifacts = { \"blur_cortex\" : blur_cortex , \"struct_noise\" : struct_noise , \"simulate_motion\" : simulate_motion , \"boundaries\" : boundaries , } self . device = device def _validated_genparams ( self , d : dict ) -> dict : \"\"\"Recursively removes all the keys with None values as they are not fixed in the generation.\"\"\" if not isinstance ( d , dict ): return d # Return non-dictionaries as-is return { key : self . _validated_genparams ( value ) for key , value in d . items () if value is not None } def sample ( self , image : torch . Tensor | None , segmentation : torch . Tensor , seeds : torch . Tensor | None , genparams : dict = {}, ) -> tuple [ torch . Tensor , torch . Tensor , torch . Tensor , dict ]: \"\"\" Generate a synthetic image from the input data. Supports both random generation and from a fixed genparams dictionary. Args: image: Image to use as intensity prior if required. segmentation: Segmentation to use as spatial prior. seeds: Seeds to use for intensity generation. genparams: Dictionary with generation parameters. Used for fixed generation. Should follow the structure and be of the same type as the returned generation parameters. Returns: The synthetic image, the segmentation, the original image, and the generation parameters. \"\"\" if genparams : genparams = self . _validated_genparams ( genparams ) # 1. Generate intensity output. if seeds is not None : seeds , selected_seeds = self . intensity_generator . load_seeds ( seeds = seeds , genparams = genparams . get ( \"selected_seeds\" , {}) ) output , seed_intensities = self . intensity_generator . sample_intensities ( seeds = seeds , device = self . device , genparams = genparams . get ( \"seed_intensities\" , {}), ) else : if image is None : raise ValueError ( \"If no seeds are passed, an image must be loaded to be used as intensity prior!\" ) # normalize the image from 0 to 255 to # match the intensity generator output = ( image - image . min ()) / ( image . max () - image . min ()) * 255 selected_seeds = {} seed_intensities = {} # ensure that tensors are on the same device output = output . to ( self . device ) segmentation = segmentation . to ( self . device ) image = image . to ( self . device ) if image is not None else None # 2. Spatially deform the data image , segmentation , output , deform_params = self . spatial_deform . deform ( image = image , segmentation = segmentation , output = output , genparams = genparams . get ( \"deform_params\" , {}), ) # 3. Gamma contrast transformation output , gamma_params = self . gamma ( output , self . device , genparams = genparams . get ( \"gamma_params\" , {}) ) # 4. Bias field corruption output , bf_params = self . biasfield ( output , self . device , genparams = genparams . get ( \"bf_params\" , {}) ) # 5. Downsample to simulate lower reconstruction resolution output , factors , resample_params = self . resampled ( output , np . array ( self . resolution ), self . device , genparams = genparams . get ( \"resample_params\" , {}), ) # 6. Noise corruption output , noise_params = self . noise ( output , self . device , genparams = genparams . get ( \"noise_params\" , {}) ) # 7. Up-sample back to the original resolution/shape output = self . resampled . resize_back ( output , factors ) # 8. Induce SR-artifacts artifacts = {} for name , artifact in self . artifacts . items (): if artifact is not None : output , metadata = artifact ( output , segmentation , self . device , genparams . get ( \"artifact_params\" , {}), resolution = self . resolution , ) artifacts [ name ] = metadata # 9. Aggregete the synth params synth_params = { \"selected_seeds\" : selected_seeds , \"seed_intensities\" : seed_intensities , \"deform_params\" : deform_params , \"gamma_params\" : gamma_params , \"bf_params\" : bf_params , \"resample_params\" : resample_params , \"noise_params\" : noise_params , \"artifacts\" : artifacts , } return output , segmentation , image , synth_params __init__ ( shape , resolution , device , intensity_generator , spatial_deform , resampler , bias_field , noise , gamma , blur_cortex = None , struct_noise = None , simulate_motion = None , boundaries = None ) Initialize the model with the given parameters. Note Augmentations related to SR artifacts are optional and can be set to None if not needed. Parameters: shape ( Iterable [ int ] ) \u2013 Shape of the output image. resolution ( Iterable [ float ] ) \u2013 Resolution of the output image. device ( str ) \u2013 Device to use for computation. intensity_generator ( ImageFromSeeds ) \u2013 Intensity generator. spatial_deform ( SpatialDeformation ) \u2013 Spatial deformation generator. resampler ( RandResample ) \u2013 Resampler. bias_field ( RandBiasField ) \u2013 Bias field generator. noise ( RandNoise ) \u2013 Noise generator. gamma ( RandGamma ) \u2013 Gamma correction generator. blur_cortex ( BlurCortex | None , default: None ) \u2013 Cortex blurring generator. struct_noise ( StructNoise | None , default: None ) \u2013 Structural noise generator. simulate_motion ( SimulateMotion | None , default: None ) \u2013 Motion simulation generator. boundaries ( SimulatedBoundaries | None , default: None ) \u2013 Boundaries generator Source code in fetalsyngen/generator/model.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def __init__ ( self , shape : Iterable [ int ], resolution : Iterable [ float ], device : str , intensity_generator : ImageFromSeeds , spatial_deform : SpatialDeformation , resampler : RandResample , bias_field : RandBiasField , noise : RandNoise , gamma : RandGamma , # optional SR artifacts blur_cortex : BlurCortex | None = None , struct_noise : StructNoise | None = None , simulate_motion : SimulateMotion | None = None , boundaries : SimulatedBoundaries | None = None , ): \"\"\" Initialize the model with the given parameters. !!!Note Augmentations related to SR artifacts are optional and can be set to None if not needed. Args: shape: Shape of the output image. resolution: Resolution of the output image. device: Device to use for computation. intensity_generator: Intensity generator. spatial_deform: Spatial deformation generator. resampler: Resampler. bias_field: Bias field generator. noise: Noise generator. gamma: Gamma correction generator. blur_cortex: Cortex blurring generator. struct_noise: Structural noise generator. simulate_motion: Motion simulation generator. boundaries: Boundaries generator \"\"\" self . shape = shape self . resolution = resolution self . intensity_generator = intensity_generator self . spatial_deform = spatial_deform self . resampled = resampler self . biasfield = bias_field self . gamma = gamma self . noise = noise self . artifacts = { \"blur_cortex\" : blur_cortex , \"struct_noise\" : struct_noise , \"simulate_motion\" : simulate_motion , \"boundaries\" : boundaries , } self . device = device sample ( image , segmentation , seeds , genparams = {}) Generate a synthetic image from the input data. Supports both random generation and from a fixed genparams dictionary. Parameters: image ( Tensor | None ) \u2013 Image to use as intensity prior if required. segmentation ( Tensor ) \u2013 Segmentation to use as spatial prior. seeds ( Tensor | None ) \u2013 Seeds to use for intensity generation. genparams ( dict , default: {} ) \u2013 Dictionary with generation parameters. Used for fixed generation. Should follow the structure and be of the same type as the returned generation parameters. Returns: tuple [ Tensor , Tensor , Tensor , dict ] \u2013 The synthetic image, the segmentation, the original image, and the generation parameters. Source code in fetalsyngen/generator/model.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def sample ( self , image : torch . Tensor | None , segmentation : torch . Tensor , seeds : torch . Tensor | None , genparams : dict = {}, ) -> tuple [ torch . Tensor , torch . Tensor , torch . Tensor , dict ]: \"\"\" Generate a synthetic image from the input data. Supports both random generation and from a fixed genparams dictionary. Args: image: Image to use as intensity prior if required. segmentation: Segmentation to use as spatial prior. seeds: Seeds to use for intensity generation. genparams: Dictionary with generation parameters. Used for fixed generation. Should follow the structure and be of the same type as the returned generation parameters. Returns: The synthetic image, the segmentation, the original image, and the generation parameters. \"\"\" if genparams : genparams = self . _validated_genparams ( genparams ) # 1. Generate intensity output. if seeds is not None : seeds , selected_seeds = self . intensity_generator . load_seeds ( seeds = seeds , genparams = genparams . get ( \"selected_seeds\" , {}) ) output , seed_intensities = self . intensity_generator . sample_intensities ( seeds = seeds , device = self . device , genparams = genparams . get ( \"seed_intensities\" , {}), ) else : if image is None : raise ValueError ( \"If no seeds are passed, an image must be loaded to be used as intensity prior!\" ) # normalize the image from 0 to 255 to # match the intensity generator output = ( image - image . min ()) / ( image . max () - image . min ()) * 255 selected_seeds = {} seed_intensities = {} # ensure that tensors are on the same device output = output . to ( self . device ) segmentation = segmentation . to ( self . device ) image = image . to ( self . device ) if image is not None else None # 2. Spatially deform the data image , segmentation , output , deform_params = self . spatial_deform . deform ( image = image , segmentation = segmentation , output = output , genparams = genparams . get ( \"deform_params\" , {}), ) # 3. Gamma contrast transformation output , gamma_params = self . gamma ( output , self . device , genparams = genparams . get ( \"gamma_params\" , {}) ) # 4. Bias field corruption output , bf_params = self . biasfield ( output , self . device , genparams = genparams . get ( \"bf_params\" , {}) ) # 5. Downsample to simulate lower reconstruction resolution output , factors , resample_params = self . resampled ( output , np . array ( self . resolution ), self . device , genparams = genparams . get ( \"resample_params\" , {}), ) # 6. Noise corruption output , noise_params = self . noise ( output , self . device , genparams = genparams . get ( \"noise_params\" , {}) ) # 7. Up-sample back to the original resolution/shape output = self . resampled . resize_back ( output , factors ) # 8. Induce SR-artifacts artifacts = {} for name , artifact in self . artifacts . items (): if artifact is not None : output , metadata = artifact ( output , segmentation , self . device , genparams . get ( \"artifact_params\" , {}), resolution = self . resolution , ) artifacts [ name ] = metadata # 9. Aggregete the synth params synth_params = { \"selected_seeds\" : selected_seeds , \"seed_intensities\" : seed_intensities , \"deform_params\" : deform_params , \"gamma_params\" : gamma_params , \"bf_params\" : bf_params , \"resample_params\" : resample_params , \"noise_params\" : noise_params , \"artifacts\" : artifacts , } return output , segmentation , image , synth_params ImageFromSeeds Source code in fetalsyngen/generator/intensity/rand_gmm.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 class ImageFromSeeds : def __init__ ( self , min_subclusters : int , max_subclusters : int , seed_labels : Iterable [ int ], generation_classes : Iterable [ int ], meta_labels : int = 4 , ): \"\"\" Args: min_subclusters (int): Minimum number of subclusters to use. max_subclusters (int): Maximum number of subclusters to use. seed_labels (Iterable[int]): Iterable with all possible labels that can occur in the loaded seeds. Should be a unique set of integers starting from [0, ...]. 0 is reserved for the background, that will not have any intensity generated. generation_classes (Iterable[int]): Classes to use for generation. Seeds with the same generation calss will be generated with the same GMM. Should be the same length as seed_labels. meta_labels (int, optional): Number of meta-labels used. Defaults to 4. \"\"\" self . min_subclusters = min_subclusters self . max_subclusters = max_subclusters try : assert len ( set ( seed_labels )) == len ( seed_labels ) except AssertionError : raise ValueError ( \"Parameter seed_labels should have unique values.\" ) try : assert len ( seed_labels ) == len ( generation_classes ) except AssertionError : raise ValueError ( \"Parameters seed_labels and generation_classes should have the same lengths.\" ) self . seed_labels = seed_labels self . generation_classes = generation_classes self . meta_labels = meta_labels self . loader = SimpleITKReader () self . orientation = Orientation ( axcodes = \"RAS\" ) def load_seeds ( self , seeds : dict [ int : dict [ int : Path ]], mlabel2subclusters : dict [ int : int ] | None = None , genparams : dict = {}, ) -> torch . Tensor : \"\"\"Generate an intensity image from seeds. If seed_mapping is provided, it is used to select the number of subclusters to use for each meta label. Otherwise, the number of subclusters is randomly selected from a uniform discrete distribution between `min_subclusters` and `max_subclusters` (both inclusive). Args: seeds: Dictionary with the mapping `subcluster_number: {meta_label: seed_path}`. mlabel2subclusters: Mapping to use when defining how many subclusters to use for each meta-label. Defaults to None. genparams: Dictionary with generation parameters. Defaults to {}. Should contain the key \"mlabel2subclusters\" if the mapping is to be fixed. Returns: torch.Tensor: Intensity image with the same shape as the seeds. Tensor dimensions are **(H, W, D)**. Values inside the tensor correspond to the subclusters, and are grouped by meta-label. `1-19: CSF, 20-29: GM, 30-39: WM, 40-49: Extra-cerebral`. \"\"\" # if no mapping is provided, randomly select the number of subclusters # to use for each meta-label in the format {mlabel: n_subclusters} if mlabel2subclusters is None : mlabel2subclusters = { meta_label : np . random . randint ( self . min_subclusters , self . max_subclusters + 1 ) for meta_label in range ( 1 , self . meta_labels + 1 ) } if \"mlabel2subclusters\" in genparams . keys (): mlabel2subclusters = genparams [ \"mlabel2subclusters\" ] # load the first seed as the one corresponding to mlabel 1 seed = self . loader ( seeds [ mlabel2subclusters [ 1 ]][ 1 ]) seed = self . orientation ( seed . unsqueeze ( 0 )) # re-orient seeds to RAS for mlabel in range ( 2 , self . meta_labels + 1 ): new_seed = self . loader ( seeds [ mlabel2subclusters [ mlabel ]][ mlabel ]) new_seed = self . orientation ( new_seed . unsqueeze ( 0 )) seed += new_seed return seed . long () . squeeze ( 0 ), { \"mlabel2subclusters\" : mlabel2subclusters } def sample_intensities ( self , seeds : torch . Tensor , device : str , genparams : dict = {} ) -> torch . Tensor : \"\"\"Sample the intensities from the seeds. Args: seeds (torch.Tensor): Tensor with the seeds. device (str): Device to use. Should be \"cuda\" or \"cpu\". genparams (dict, optional): Dictionary with generation parameters. Defaults to {}. Should contain the keys \"mus\" and \"sigmas\" if the GMM parameters are to be fixed. Returns: torch.Tensor: Tensor with the intensities. \"\"\" nlabels = max ( self . seed_labels ) + 1 nsamp = len ( self . seed_labels ) # # Sample GMMs means and stds mus = ( 25 + 200 * torch . rand ( nlabels , dtype = torch . float , device = device ) if \"mus\" not in genparams . keys () else genparams [ \"mus\" ] ) sigmas = ( 5 + 20 * torch . rand ( nlabels , dtype = torch . float , device = device , ) if \"sigmas\" not in genparams . keys () else genparams [ \"sigmas\" ] ) # if there are seed labels from the same generation class # set their mean to be the same with some random perturbation if self . generation_classes != self . seed_labels : mus [ self . seed_labels ] = torch . clamp ( mus [ self . generation_classes ] + 25 * torch . randn ( nsamp , dtype = torch . float , device = device ), 0 , 225 , ) intensity_image = mus [ seeds ] + sigmas [ seeds ] * torch . randn ( seeds . shape , dtype = torch . float , device = device ) intensity_image [ intensity_image < 0 ] = 0 return intensity_image , { \"mus\" : mus , \"sigmas\" : sigmas , } __init__ ( min_subclusters , max_subclusters , seed_labels , generation_classes , meta_labels = 4 ) Parameters: min_subclusters ( int ) \u2013 Minimum number of subclusters to use. max_subclusters ( int ) \u2013 Maximum number of subclusters to use. seed_labels ( Iterable [ int ] ) \u2013 Iterable with all possible labels that can occur in the loaded seeds. Should be a unique set of integers starting from [0, ...]. 0 is reserved for the background, that will not have any intensity generated. generation_classes ( Iterable [ int ] ) \u2013 Classes to use for generation. Seeds with the same generation calss will be generated with the same GMM. Should be the same length as seed_labels. meta_labels ( int , default: 4 ) \u2013 Number of meta-labels used. Defaults to 4. Source code in fetalsyngen/generator/intensity/rand_gmm.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , min_subclusters : int , max_subclusters : int , seed_labels : Iterable [ int ], generation_classes : Iterable [ int ], meta_labels : int = 4 , ): \"\"\" Args: min_subclusters (int): Minimum number of subclusters to use. max_subclusters (int): Maximum number of subclusters to use. seed_labels (Iterable[int]): Iterable with all possible labels that can occur in the loaded seeds. Should be a unique set of integers starting from [0, ...]. 0 is reserved for the background, that will not have any intensity generated. generation_classes (Iterable[int]): Classes to use for generation. Seeds with the same generation calss will be generated with the same GMM. Should be the same length as seed_labels. meta_labels (int, optional): Number of meta-labels used. Defaults to 4. \"\"\" self . min_subclusters = min_subclusters self . max_subclusters = max_subclusters try : assert len ( set ( seed_labels )) == len ( seed_labels ) except AssertionError : raise ValueError ( \"Parameter seed_labels should have unique values.\" ) try : assert len ( seed_labels ) == len ( generation_classes ) except AssertionError : raise ValueError ( \"Parameters seed_labels and generation_classes should have the same lengths.\" ) self . seed_labels = seed_labels self . generation_classes = generation_classes self . meta_labels = meta_labels self . loader = SimpleITKReader () self . orientation = Orientation ( axcodes = \"RAS\" ) load_seeds ( seeds , mlabel2subclusters = None , genparams = {}) Generate an intensity image from seeds. If seed_mapping is provided, it is used to select the number of subclusters to use for each meta label. Otherwise, the number of subclusters is randomly selected from a uniform discrete distribution between min_subclusters and max_subclusters (both inclusive). Args: seeds: Dictionary with the mapping `subcluster_number: {meta_label: seed_path}`. mlabel2subclusters: Mapping to use when defining how many subclusters to use for each meta-label. Defaults to None. genparams: Dictionary with generation parameters. Defaults to {}. Should contain the key \"mlabel2subclusters\" if the mapping is to be fixed. Returns: Tensor \u2013 torch.Tensor: Intensity image with the same shape as the seeds. Tensor dimensions are (H, W, D) . Values inside the tensor correspond to the subclusters, and are grouped by meta-label. 1-19: CSF, 20-29: GM, 30-39: WM, 40-49: Extra-cerebral . Source code in fetalsyngen/generator/intensity/rand_gmm.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def load_seeds ( self , seeds : dict [ int : dict [ int : Path ]], mlabel2subclusters : dict [ int : int ] | None = None , genparams : dict = {}, ) -> torch . Tensor : \"\"\"Generate an intensity image from seeds. If seed_mapping is provided, it is used to select the number of subclusters to use for each meta label. Otherwise, the number of subclusters is randomly selected from a uniform discrete distribution between `min_subclusters` and `max_subclusters` (both inclusive). Args: seeds: Dictionary with the mapping `subcluster_number: {meta_label: seed_path}`. mlabel2subclusters: Mapping to use when defining how many subclusters to use for each meta-label. Defaults to None. genparams: Dictionary with generation parameters. Defaults to {}. Should contain the key \"mlabel2subclusters\" if the mapping is to be fixed. Returns: torch.Tensor: Intensity image with the same shape as the seeds. Tensor dimensions are **(H, W, D)**. Values inside the tensor correspond to the subclusters, and are grouped by meta-label. `1-19: CSF, 20-29: GM, 30-39: WM, 40-49: Extra-cerebral`. \"\"\" # if no mapping is provided, randomly select the number of subclusters # to use for each meta-label in the format {mlabel: n_subclusters} if mlabel2subclusters is None : mlabel2subclusters = { meta_label : np . random . randint ( self . min_subclusters , self . max_subclusters + 1 ) for meta_label in range ( 1 , self . meta_labels + 1 ) } if \"mlabel2subclusters\" in genparams . keys (): mlabel2subclusters = genparams [ \"mlabel2subclusters\" ] # load the first seed as the one corresponding to mlabel 1 seed = self . loader ( seeds [ mlabel2subclusters [ 1 ]][ 1 ]) seed = self . orientation ( seed . unsqueeze ( 0 )) # re-orient seeds to RAS for mlabel in range ( 2 , self . meta_labels + 1 ): new_seed = self . loader ( seeds [ mlabel2subclusters [ mlabel ]][ mlabel ]) new_seed = self . orientation ( new_seed . unsqueeze ( 0 )) seed += new_seed return seed . long () . squeeze ( 0 ), { \"mlabel2subclusters\" : mlabel2subclusters } sample_intensities ( seeds , device , genparams = {}) Sample the intensities from the seeds. Parameters: seeds ( Tensor ) \u2013 Tensor with the seeds. device ( str ) \u2013 Device to use. Should be \"cuda\" or \"cpu\". genparams ( dict , default: {} ) \u2013 Dictionary with generation parameters. Defaults to {}. Should contain the keys \"mus\" and \"sigmas\" if the GMM parameters are to be fixed. Returns: Tensor \u2013 torch.Tensor: Tensor with the intensities. Source code in fetalsyngen/generator/intensity/rand_gmm.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def sample_intensities ( self , seeds : torch . Tensor , device : str , genparams : dict = {} ) -> torch . Tensor : \"\"\"Sample the intensities from the seeds. Args: seeds (torch.Tensor): Tensor with the seeds. device (str): Device to use. Should be \"cuda\" or \"cpu\". genparams (dict, optional): Dictionary with generation parameters. Defaults to {}. Should contain the keys \"mus\" and \"sigmas\" if the GMM parameters are to be fixed. Returns: torch.Tensor: Tensor with the intensities. \"\"\" nlabels = max ( self . seed_labels ) + 1 nsamp = len ( self . seed_labels ) # # Sample GMMs means and stds mus = ( 25 + 200 * torch . rand ( nlabels , dtype = torch . float , device = device ) if \"mus\" not in genparams . keys () else genparams [ \"mus\" ] ) sigmas = ( 5 + 20 * torch . rand ( nlabels , dtype = torch . float , device = device , ) if \"sigmas\" not in genparams . keys () else genparams [ \"sigmas\" ] ) # if there are seed labels from the same generation class # set their mean to be the same with some random perturbation if self . generation_classes != self . seed_labels : mus [ self . seed_labels ] = torch . clamp ( mus [ self . generation_classes ] + 25 * torch . randn ( nsamp , dtype = torch . float , device = device ), 0 , 225 , ) intensity_image = mus [ seeds ] + sigmas [ seeds ] * torch . randn ( seeds . shape , dtype = torch . float , device = device ) intensity_image [ intensity_image < 0 ] = 0 return intensity_image , { \"mus\" : mus , \"sigmas\" : sigmas , } SpatialDeformation Class defining the spatial deformation of the image. Combines both random affine and nonlinear transformations to deform the image. Source code in fetalsyngen/generator/deformation/affine_nonrigid.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 class SpatialDeformation : \"\"\" Class defining the spatial deformation of the image. Combines both random affine and nonlinear transformations to deform the image. \"\"\" def __init__ ( self , max_rotation : float , max_shear : float , max_scaling : float , size : Iterable [ int ], prob : float , nonlinear_transform : bool , nonlin_scale_min : float , nonlin_scale_max : float , nonlin_std_max : float , flip_prb : float , device : str , ): \"\"\"Initialize the spatial deformation. Args: max_rotation (float): Maximum rotation in degrees. max_shear (float): Maximum shear. max_scaling (float): Maximum scaling. size (Iterable[int]): Size of the output image. prob (float): Probability of applying the deformation. nonlinear_transform (bool): Whether to apply nonlinear transformation. nonlin_scale_min (float): Minimum scale for the nonlinear transformation. nonlin_scale_max (float): Maximum scale for the nonlinear transformation. nonlin_std_max (float): Maximum standard deviation for the nonlinear transformation. flip_prb (float): Probability of flipping the image. device (str): Device to use for computation. Either \"cuda\" or \"cpu\". \"\"\" self . size = size # 256, 256, 256 self . prob = prob self . flip_prb = flip_prb # randaffine parameters self . max_rotation = max_rotation self . max_shear = max_shear self . max_scaling = max_scaling # nonlinear transform parameters self . nonlinear_transform = nonlinear_transform self . nonlin_scale_min = nonlin_scale_min self . nonlin_scale_max = nonlin_scale_max self . nonlin_std_max = nonlin_std_max self . device = device self . _prepare_grid () def _prepare_grid ( self ): xx , yy , zz = np . meshgrid ( range ( self . size [ 0 ]), range ( self . size [ 1 ]), range ( self . size [ 2 ]), sparse = False , indexing = \"ij\" , ) self . xx = torch . tensor ( xx , dtype = torch . float , device = self . device ) self . yy = torch . tensor ( yy , dtype = torch . float , device = self . device ) self . zz = torch . tensor ( zz , dtype = torch . float , device = self . device ) self . c = torch . tensor ( ( np . array ( self . size ) - 1 ) / 2 , dtype = torch . float , device = self . device , ) self . xc = self . xx - self . c [ 0 ] self . yc = self . yy - self . c [ 1 ] self . zc = self . zz - self . c [ 2 ] def deform ( self , image , segmentation , output , genparams : dict = {} ) -> tuple [ torch . Tensor , torch . Tensor , torch . Tensor , dict ]: \"\"\"Deform the image, segmentation and output. Args: image (torch.Tensor): Image to deform. segmentation (torch.Tensor): Segmentation to deform. output (torch.Tensor): Output to deform. genparams (dict, optional): Dictionary with generation parameters. Defaults to {}. Should contain the keys \"affine\" and \"non_rigid\" if the parameters are fixed. Affine parameters should contain the keys \"rotations\", \"shears\" and \"scalings\". Non-rigid parameters should contain the keys \"nonlin_scale\", \"nonlin_std\" and \"size_F_small\". Returns: Deformed image, segmentation, output and deformation parameters. \"\"\" deform_params = {} if np . random . rand () < self . prob or len ( genparams . keys ()) > 0 : image_shape = output . shape flip = ( np . random . rand () < self . flip_prb if \"flip\" not in genparams . keys () else genparams [ \"flip\" ] ) xx2 , yy2 , zz2 , x1 , y1 , z1 , x2 , y2 , z2 , deform_params = ( self . generate_deformation ( image_shape , random_shift = True , genparams = genparams ) ) # flip the image if nessesary if flip : segmentation = torch . flip ( segmentation , [ 0 ]) output = torch . flip ( output , [ 0 ]) image = torch . flip ( image , [ 0 ]) if image is not None else None output = fast_3D_interp_torch ( output , xx2 , yy2 , zz2 , \"linear\" ) segmentation = fast_3D_interp_torch ( segmentation . to ( self . device ), xx2 , yy2 , zz2 , \"nearest\" ) if image is not None : image = fast_3D_interp_torch ( image . to ( self . device ), xx2 , yy2 , zz2 , \"linear\" ) deform_params [ \"flip\" ] = flip return image , segmentation , output , deform_params def generate_deformation ( self , image_shape , random_shift = True , genparams = {}): # sample affine deformation A , c2 , aff_params = self . random_affine_transform ( shp = image_shape , max_rotation = self . max_rotation , max_shear = self . max_shear , max_scaling = self . max_scaling , random_shift = random_shift , genparams = genparams . get ( \"affine\" , {}), ) # sample nonlinear deformation if self . nonlinear_transform : F , non_rigid_params = self . random_nonlinear_transform ( nonlin_scale_min = self . nonlin_scale_min , nonlin_scale_max = self . nonlin_scale_max , nonlin_std_max = self . nonlin_std_max , genparams = genparams . get ( \"non_rigid\" , {}), ) else : F = None non_rigid_params = {} # deform the images xx2 , yy2 , zz2 , x1 , y1 , z1 , x2 , y2 , z2 = self . deform_image ( image_shape , A , c2 , F ) return ( xx2 , yy2 , zz2 , x1 , y1 , z1 , x2 , y2 , z2 , { \"affine\" : aff_params , \"non_rigid\" : non_rigid_params , }, ) def random_affine_transform ( self , shp , max_rotation , max_shear , max_scaling , random_shift = True , genparams = {}, ): rotations = ( (( 2 * max_rotation * np . random . rand ( 3 ) - max_rotation ) / 180.0 * np . pi ) if \"rotations\" not in genparams . keys () else genparams [ \"rotations\" ] ) shears = ( 2 * max_shear * np . random . rand ( 3 ) - max_shear if \"shears\" not in genparams . keys () else genparams [ \"shears\" ] ) scalings = ( 1 + ( 2 * max_scaling * np . random . rand ( 3 ) - max_scaling ) if \"scalings\" not in genparams . keys () else genparams [ \"scalings\" ] ) # we divide distance maps by this, not perfect, but better than nothing A = torch . tensor ( make_affine_matrix ( rotations , shears , scalings ), dtype = torch . float , device = self . device , ) # sample center if random_shift : max_shift = ( torch . tensor ( np . array ( shp [ 0 : 3 ]) - self . size , dtype = torch . float , device = self . device , ) ) / 2 max_shift [ max_shift < 0 ] = 0 c2 = torch . tensor ( ( np . array ( shp [ 0 : 3 ]) - 1 ) / 2 , dtype = torch . float , device = self . device , ) + ( 2 * ( max_shift * torch . rand ( 3 , dtype = float , device = self . device )) - max_shift ) else : c2 = torch . tensor ( ( np . array ( shp [ 0 : 3 ]) - 1 ) / 2 , dtype = torch . float , device = self . device , ) affine_params = { \"rotations\" : rotations , \"shears\" : shears , \"scalings\" : scalings , } return A , c2 , affine_params def random_nonlinear_transform ( self , nonlin_scale_min , nonlin_scale_max , nonlin_std_max , genparams = {} ): nonlin_scale = ( nonlin_scale_min + np . random . rand ( 1 ) * ( nonlin_scale_max - nonlin_scale_min ) if \"nonlin_scale\" not in genparams . keys () else genparams [ \"nonlin_scale\" ] ) size_F_small = ( np . round ( nonlin_scale * np . array ( self . size )) . astype ( int ) . tolist () if \"size_F_small\" not in genparams . keys () else genparams [ \"size_F_small\" ] ) nonlin_std = ( nonlin_std_max * np . random . rand () if \"nonlin_std\" not in genparams . keys () else genparams [ \"nonlin_std\" ] ) Fsmall = nonlin_std * torch . randn ( [ * size_F_small , 3 ], dtype = torch . float , device = self . device ) F = myzoom_torch ( Fsmall , np . array ( self . size ) / size_F_small ) return F , { \"nonlin_scale\" : nonlin_scale , \"nonlin_std\" : nonlin_std , \"size_F_small\" : size_F_small , } def deform_image ( self , shp , A , c2 , F ): if F is not None : # deform the images (we do nonlinear \"first\" ie after so we can do heavy coronal deformations in photo mode) xx1 = self . xc + F [:, :, :, 0 ] yy1 = self . yc + F [:, :, :, 1 ] zz1 = self . zc + F [:, :, :, 2 ] else : xx1 = self . xc yy1 = self . yc zz1 = self . zc xx2 = A [ 0 , 0 ] * xx1 + A [ 0 , 1 ] * yy1 + A [ 0 , 2 ] * zz1 + c2 [ 0 ] yy2 = A [ 1 , 0 ] * xx1 + A [ 1 , 1 ] * yy1 + A [ 1 , 2 ] * zz1 + c2 [ 1 ] zz2 = A [ 2 , 0 ] * xx1 + A [ 2 , 1 ] * yy1 + A [ 2 , 2 ] * zz1 + c2 [ 2 ] xx2 [ xx2 < 0 ] = 0 yy2 [ yy2 < 0 ] = 0 zz2 [ zz2 < 0 ] = 0 xx2 [ xx2 > ( shp [ 0 ] - 1 )] = shp [ 0 ] - 1 yy2 [ yy2 > ( shp [ 1 ] - 1 )] = shp [ 1 ] - 1 zz2 [ zz2 > ( shp [ 2 ] - 1 )] = shp [ 2 ] - 1 # Get the margins for reading images x1 = torch . floor ( torch . min ( xx2 )) y1 = torch . floor ( torch . min ( yy2 )) z1 = torch . floor ( torch . min ( zz2 )) x2 = 1 + torch . ceil ( torch . max ( xx2 )) y2 = 1 + torch . ceil ( torch . max ( yy2 )) z2 = 1 + torch . ceil ( torch . max ( zz2 )) xx2 -= x1 yy2 -= y1 zz2 -= z1 x1 = x1 . cpu () . numpy () . astype ( int ) y1 = y1 . cpu () . numpy () . astype ( int ) z1 = z1 . cpu () . numpy () . astype ( int ) x2 = x2 . cpu () . numpy () . astype ( int ) y2 = y2 . cpu () . numpy () . astype ( int ) z2 = z2 . cpu () . numpy () . astype ( int ) return xx2 , yy2 , zz2 , x1 , y1 , z1 , x2 , y2 , z2 __init__ ( max_rotation , max_shear , max_scaling , size , prob , nonlinear_transform , nonlin_scale_min , nonlin_scale_max , nonlin_std_max , flip_prb , device ) Initialize the spatial deformation. Parameters: max_rotation ( float ) \u2013 Maximum rotation in degrees. max_shear ( float ) \u2013 Maximum shear. max_scaling ( float ) \u2013 Maximum scaling. size ( Iterable [ int ] ) \u2013 Size of the output image. prob ( float ) \u2013 Probability of applying the deformation. nonlinear_transform ( bool ) \u2013 Whether to apply nonlinear transformation. nonlin_scale_min ( float ) \u2013 Minimum scale for the nonlinear transformation. nonlin_scale_max ( float ) \u2013 Maximum scale for the nonlinear transformation. nonlin_std_max ( float ) \u2013 Maximum standard deviation for the nonlinear transformation. flip_prb ( float ) \u2013 Probability of flipping the image. device ( str ) \u2013 Device to use for computation. Either \"cuda\" or \"cpu\". Source code in fetalsyngen/generator/deformation/affine_nonrigid.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def __init__ ( self , max_rotation : float , max_shear : float , max_scaling : float , size : Iterable [ int ], prob : float , nonlinear_transform : bool , nonlin_scale_min : float , nonlin_scale_max : float , nonlin_std_max : float , flip_prb : float , device : str , ): \"\"\"Initialize the spatial deformation. Args: max_rotation (float): Maximum rotation in degrees. max_shear (float): Maximum shear. max_scaling (float): Maximum scaling. size (Iterable[int]): Size of the output image. prob (float): Probability of applying the deformation. nonlinear_transform (bool): Whether to apply nonlinear transformation. nonlin_scale_min (float): Minimum scale for the nonlinear transformation. nonlin_scale_max (float): Maximum scale for the nonlinear transformation. nonlin_std_max (float): Maximum standard deviation for the nonlinear transformation. flip_prb (float): Probability of flipping the image. device (str): Device to use for computation. Either \"cuda\" or \"cpu\". \"\"\" self . size = size # 256, 256, 256 self . prob = prob self . flip_prb = flip_prb # randaffine parameters self . max_rotation = max_rotation self . max_shear = max_shear self . max_scaling = max_scaling # nonlinear transform parameters self . nonlinear_transform = nonlinear_transform self . nonlin_scale_min = nonlin_scale_min self . nonlin_scale_max = nonlin_scale_max self . nonlin_std_max = nonlin_std_max self . device = device self . _prepare_grid () deform ( image , segmentation , output , genparams = {}) Deform the image, segmentation and output. Parameters: image ( Tensor ) \u2013 Image to deform. segmentation ( Tensor ) \u2013 Segmentation to deform. output ( Tensor ) \u2013 Output to deform. genparams ( dict , default: {} ) \u2013 Dictionary with generation parameters. Defaults to {}. Should contain the keys \"affine\" and \"non_rigid\" if the parameters are fixed. Affine parameters should contain the keys \"rotations\", \"shears\" and \"scalings\". Non-rigid parameters should contain the keys \"nonlin_scale\", \"nonlin_std\" and \"size_F_small\". Returns: tuple [ Tensor , Tensor , Tensor , dict ] \u2013 Deformed image, segmentation, output and deformation parameters. Source code in fetalsyngen/generator/deformation/affine_nonrigid.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def deform ( self , image , segmentation , output , genparams : dict = {} ) -> tuple [ torch . Tensor , torch . Tensor , torch . Tensor , dict ]: \"\"\"Deform the image, segmentation and output. Args: image (torch.Tensor): Image to deform. segmentation (torch.Tensor): Segmentation to deform. output (torch.Tensor): Output to deform. genparams (dict, optional): Dictionary with generation parameters. Defaults to {}. Should contain the keys \"affine\" and \"non_rigid\" if the parameters are fixed. Affine parameters should contain the keys \"rotations\", \"shears\" and \"scalings\". Non-rigid parameters should contain the keys \"nonlin_scale\", \"nonlin_std\" and \"size_F_small\". Returns: Deformed image, segmentation, output and deformation parameters. \"\"\" deform_params = {} if np . random . rand () < self . prob or len ( genparams . keys ()) > 0 : image_shape = output . shape flip = ( np . random . rand () < self . flip_prb if \"flip\" not in genparams . keys () else genparams [ \"flip\" ] ) xx2 , yy2 , zz2 , x1 , y1 , z1 , x2 , y2 , z2 , deform_params = ( self . generate_deformation ( image_shape , random_shift = True , genparams = genparams ) ) # flip the image if nessesary if flip : segmentation = torch . flip ( segmentation , [ 0 ]) output = torch . flip ( output , [ 0 ]) image = torch . flip ( image , [ 0 ]) if image is not None else None output = fast_3D_interp_torch ( output , xx2 , yy2 , zz2 , \"linear\" ) segmentation = fast_3D_interp_torch ( segmentation . to ( self . device ), xx2 , yy2 , zz2 , \"nearest\" ) if image is not None : image = fast_3D_interp_torch ( image . to ( self . device ), xx2 , yy2 , zz2 , \"linear\" ) deform_params [ \"flip\" ] = flip return image , segmentation , output , deform_params RandResample Bases: RandTransform Resample the input image to a random resolution sampled uniformly between min_resolution and max_resolution with a probability of prob . If the resolution is smaller than the input resolution, no resampling is performed. Source code in fetalsyngen/generator/augmentation/synthseg.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 class RandResample ( RandTransform ): \"\"\"Resample the input image to a random resolution sampled uniformly between `min_resolution` and `max_resolution` with a probability of `prob`. If the resolution is smaller than the input resolution, no resampling is performed. \"\"\" def __init__ ( self , prob : float , min_resolution : float , max_resolution : float , ): \"\"\" Initialize the augmentation parameters. Args: prob (float): Probability of applying the augmentation. min_resolution (float): Minimum resolution for the augmentation (in mm). max_resolution (float): Maximum resolution for the augmentation. \"\"\" self . prob = prob self . min_resolution = min_resolution self . max_resolution = max_resolution def __call__ ( self , output , input_resolution , device , genparams : dict = {} ) -> torch . Tensor : \"\"\"Apply the resampling to the input image. Args: output (torch.Tensor): Input image to resample. input_resolution (np.array): Resolution of the input image. device (str): Device to use for computation. genparams (dict): Generation parameters. Default: {}. Should contain the key \"spacing\" if the spacing is fixed. Returns: Resampled image. \"\"\" if np . random . rand () < self . prob or \"spacing\" in genparams . keys (): input_size = np . array ( output . shape ) spacing = ( np . array ([ 1.0 , 1.0 , 1.0 ]) * self . random_uniform ( self . min_resolution , self . max_resolution ) if \"spacing\" not in genparams . keys () else genparams [ \"spacing\" ] ) # Ensure spacing and input_resolution are numpy arrays spacing = np . array ( spacing ) input_resolution = np . array ( input_resolution ) # calculate stds of gaussian kernels # used for blurring to simulate resampling # the data to different resolutions stds = ( ( 0.85 + 0.3 * np . random . rand ()) * np . log ( 5 ) / np . pi * spacing / input_resolution ) # no blur if thickness is equal or smaller to the resolution of the training data stds [ spacing <= input_resolution ] = 0.0 output_blurred = gaussian_blur_3d ( output , stds , device ) # resize the blurred output to the new resolution new_size = ( np . array ( input_size ) * input_resolution / spacing ) . astype ( int ) # calculate the factors for the interpolation factors = np . array ( new_size ) / np . array ( input_size ) # delta is the offset for the interpolation delta = ( 1.0 - factors ) / ( 2.0 * factors ) vx = np . arange ( delta [ 0 ], delta [ 0 ] + new_size [ 0 ] / factors [ 0 ], 1 / factors [ 0 ] )[: new_size [ 0 ]] vy = np . arange ( delta [ 1 ], delta [ 1 ] + new_size [ 1 ] / factors [ 1 ], 1 / factors [ 1 ] )[: new_size [ 1 ]] vz = np . arange ( delta [ 2 ], delta [ 2 ] + new_size [ 2 ] / factors [ 2 ], 1 / factors [ 2 ] )[: new_size [ 2 ]] II , JJ , KK = np . meshgrid ( vx , vy , vz , sparse = False , indexing = \"ij\" ) II = torch . tensor ( II , dtype = torch . float , device = device ) JJ = torch . tensor ( JJ , dtype = torch . float , device = device ) KK = torch . tensor ( KK , dtype = torch . float , device = device ) output_resized = fast_3D_interp_torch ( output_blurred , II , JJ , KK , \"linear\" ) return output_resized , factors , { \"spacing\" : spacing . tolist ()} else : return output , None , { \"spacing\" : None } def resize_back ( self , output_resized , factors ): if factors is not None : output_resized = myzoom_torch ( output_resized , 1 / factors ) return output_resized / torch . max ( output_resized ) else : return output_resized __init__ ( prob , min_resolution , max_resolution ) Initialize the augmentation parameters. Parameters: prob ( float ) \u2013 Probability of applying the augmentation. min_resolution ( float ) \u2013 Minimum resolution for the augmentation (in mm). max_resolution ( float ) \u2013 Maximum resolution for the augmentation. Source code in fetalsyngen/generator/augmentation/synthseg.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , prob : float , min_resolution : float , max_resolution : float , ): \"\"\" Initialize the augmentation parameters. Args: prob (float): Probability of applying the augmentation. min_resolution (float): Minimum resolution for the augmentation (in mm). max_resolution (float): Maximum resolution for the augmentation. \"\"\" self . prob = prob self . min_resolution = min_resolution self . max_resolution = max_resolution __call__ ( output , input_resolution , device , genparams = {}) Apply the resampling to the input image. Parameters: output ( Tensor ) \u2013 Input image to resample. input_resolution ( array ) \u2013 Resolution of the input image. device ( str ) \u2013 Device to use for computation. genparams ( dict , default: {} ) \u2013 Generation parameters. Default: {}. Should contain the key \"spacing\" if the spacing is fixed. Returns: Tensor \u2013 Resampled image. Source code in fetalsyngen/generator/augmentation/synthseg.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def __call__ ( self , output , input_resolution , device , genparams : dict = {} ) -> torch . Tensor : \"\"\"Apply the resampling to the input image. Args: output (torch.Tensor): Input image to resample. input_resolution (np.array): Resolution of the input image. device (str): Device to use for computation. genparams (dict): Generation parameters. Default: {}. Should contain the key \"spacing\" if the spacing is fixed. Returns: Resampled image. \"\"\" if np . random . rand () < self . prob or \"spacing\" in genparams . keys (): input_size = np . array ( output . shape ) spacing = ( np . array ([ 1.0 , 1.0 , 1.0 ]) * self . random_uniform ( self . min_resolution , self . max_resolution ) if \"spacing\" not in genparams . keys () else genparams [ \"spacing\" ] ) # Ensure spacing and input_resolution are numpy arrays spacing = np . array ( spacing ) input_resolution = np . array ( input_resolution ) # calculate stds of gaussian kernels # used for blurring to simulate resampling # the data to different resolutions stds = ( ( 0.85 + 0.3 * np . random . rand ()) * np . log ( 5 ) / np . pi * spacing / input_resolution ) # no blur if thickness is equal or smaller to the resolution of the training data stds [ spacing <= input_resolution ] = 0.0 output_blurred = gaussian_blur_3d ( output , stds , device ) # resize the blurred output to the new resolution new_size = ( np . array ( input_size ) * input_resolution / spacing ) . astype ( int ) # calculate the factors for the interpolation factors = np . array ( new_size ) / np . array ( input_size ) # delta is the offset for the interpolation delta = ( 1.0 - factors ) / ( 2.0 * factors ) vx = np . arange ( delta [ 0 ], delta [ 0 ] + new_size [ 0 ] / factors [ 0 ], 1 / factors [ 0 ] )[: new_size [ 0 ]] vy = np . arange ( delta [ 1 ], delta [ 1 ] + new_size [ 1 ] / factors [ 1 ], 1 / factors [ 1 ] )[: new_size [ 1 ]] vz = np . arange ( delta [ 2 ], delta [ 2 ] + new_size [ 2 ] / factors [ 2 ], 1 / factors [ 2 ] )[: new_size [ 2 ]] II , JJ , KK = np . meshgrid ( vx , vy , vz , sparse = False , indexing = \"ij\" ) II = torch . tensor ( II , dtype = torch . float , device = device ) JJ = torch . tensor ( JJ , dtype = torch . float , device = device ) KK = torch . tensor ( KK , dtype = torch . float , device = device ) output_resized = fast_3D_interp_torch ( output_blurred , II , JJ , KK , \"linear\" ) return output_resized , factors , { \"spacing\" : spacing . tolist ()} else : return output , None , { \"spacing\" : None } RandBiasField Bases: RandTransform Add a random bias field to the input image with a probability of prob . Source code in fetalsyngen/generator/augmentation/synthseg.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 class RandBiasField ( RandTransform ): \"\"\"Add a random bias field to the input image with a probability of `prob`.\"\"\" def __init__ ( self , prob : float , scale_min : float , scale_max : float , std_min : float , std_max : float , ): \"\"\" Args: prob: Probability of applying the augmentation. scale_min: Minimum scale of the bias field. scale_max: Maximum scale of the bias field. std_min: Minimum standard deviation of the bias field. std_max: Maximum standard deviation of the bias. \"\"\" self . prob = prob self . scale_min = scale_min self . scale_max = scale_max self . std_min = std_min self . std_max = std_max def __call__ ( self , output , device , genparams : dict = {}) -> torch . Tensor : \"\"\"Apply the bias field to the input image. Args: output (torch.Tensor): Input image to apply the bias field. device (str): Device to use for computation. genparams (dict): Generation parameters. Default: {}. Should contain the keys \"bf_scale\", \"bf_std\" and \"bf_size\" if the bias field parameters are fixed. Returns: Image with the bias field applied. \"\"\" if np . random . rand () < self . prob or len ( genparams . keys ()) > 0 : image_size = output . shape bf_scale = ( self . scale_min + np . random . rand ( 1 ) * ( self . scale_max - self . scale_min ) if \"bf_scale\" not in genparams . keys () else genparams [ \"bf_scale\" ] ) bf_size = np . round ( bf_scale * np . array ( image_size )) . astype ( int ) . tolist () bf_std = ( self . std_min + ( self . std_max - self . std_min ) * np . random . rand ( 1 ) if \"bf_std\" not in genparams . keys () else genparams [ \"bf_std\" ] ) bf_low_scale = torch . tensor ( bf_std , dtype = torch . float , device = device , ) * torch . randn ( bf_size , dtype = torch . float , device = device ) bf_interp = myzoom_torch ( bf_low_scale , np . array ( image_size ) / bf_size ) bf = torch . exp ( bf_interp ) return output * bf , { \"bf_scale\" : bf_scale , \"bf_std\" : bf_std , \"bf_size\" : bf_size , } else : return output , { \"bf_scale\" : None , \"bf_std\" : None , \"bf_size\" : None } __init__ ( prob , scale_min , scale_max , std_min , std_max ) Parameters: prob ( float ) \u2013 Probability of applying the augmentation. scale_min ( float ) \u2013 Minimum scale of the bias field. scale_max ( float ) \u2013 Maximum scale of the bias field. std_min ( float ) \u2013 Minimum standard deviation of the bias field. std_max ( float ) \u2013 Maximum standard deviation of the bias. Source code in fetalsyngen/generator/augmentation/synthseg.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def __init__ ( self , prob : float , scale_min : float , scale_max : float , std_min : float , std_max : float , ): \"\"\" Args: prob: Probability of applying the augmentation. scale_min: Minimum scale of the bias field. scale_max: Maximum scale of the bias field. std_min: Minimum standard deviation of the bias field. std_max: Maximum standard deviation of the bias. \"\"\" self . prob = prob self . scale_min = scale_min self . scale_max = scale_max self . std_min = std_min self . std_max = std_max __call__ ( output , device , genparams = {}) Apply the bias field to the input image. Parameters: output ( Tensor ) \u2013 Input image to apply the bias field. device ( str ) \u2013 Device to use for computation. genparams ( dict , default: {} ) \u2013 Generation parameters. Default: {}. Should contain the keys \"bf_scale\", \"bf_std\" and \"bf_size\" if the bias field parameters are fixed. Returns: Tensor \u2013 Image with the bias field applied. Source code in fetalsyngen/generator/augmentation/synthseg.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def __call__ ( self , output , device , genparams : dict = {}) -> torch . Tensor : \"\"\"Apply the bias field to the input image. Args: output (torch.Tensor): Input image to apply the bias field. device (str): Device to use for computation. genparams (dict): Generation parameters. Default: {}. Should contain the keys \"bf_scale\", \"bf_std\" and \"bf_size\" if the bias field parameters are fixed. Returns: Image with the bias field applied. \"\"\" if np . random . rand () < self . prob or len ( genparams . keys ()) > 0 : image_size = output . shape bf_scale = ( self . scale_min + np . random . rand ( 1 ) * ( self . scale_max - self . scale_min ) if \"bf_scale\" not in genparams . keys () else genparams [ \"bf_scale\" ] ) bf_size = np . round ( bf_scale * np . array ( image_size )) . astype ( int ) . tolist () bf_std = ( self . std_min + ( self . std_max - self . std_min ) * np . random . rand ( 1 ) if \"bf_std\" not in genparams . keys () else genparams [ \"bf_std\" ] ) bf_low_scale = torch . tensor ( bf_std , dtype = torch . float , device = device , ) * torch . randn ( bf_size , dtype = torch . float , device = device ) bf_interp = myzoom_torch ( bf_low_scale , np . array ( image_size ) / bf_size ) bf = torch . exp ( bf_interp ) return output * bf , { \"bf_scale\" : bf_scale , \"bf_std\" : bf_std , \"bf_size\" : bf_size , } else : return output , { \"bf_scale\" : None , \"bf_std\" : None , \"bf_size\" : None } RandNoise Bases: RandTransform Add random Gaussian noise to the input image with a probability of prob . Source code in fetalsyngen/generator/augmentation/synthseg.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 class RandNoise ( RandTransform ): \"\"\"Add random Gaussian noise to the input image with a probability of `prob`.\"\"\" def __init__ ( self , prob : float , std_min : float , std_max : float ): \"\"\" The image scale is 0-255 so the noise is added in the same scale. Args: prob: Probability of applying the augmentation. std_min: Minimum standard deviation of the noise. std_max: Maximum standard deviation of the noise \"\"\" self . prob = prob self . std_min = std_min self . std_max = std_max def __call__ ( self , output , device , genparams : dict = {}) -> torch . Tensor : \"\"\"Apply the noise to the input image. Args: output (torch.Tensor): Input image to apply the noise. device (str): Device to use for computation. genparams (dict): Generation parameters. Default: {}. Should contain the key \"noise_std\" if the noise standard deviation is fixed. Returns: Image with the noise applied.\"\"\" noise_std = None if np . random . rand () < self . prob or \"noise_std\" in genparams . keys (): noise_std = ( self . std_min + ( self . std_max - self . std_min ) * np . random . rand ( 1 ) if \"noise_std\" not in genparams . keys () else genparams [ \"noise_std\" ] ) noise_std = torch . tensor ( noise_std , dtype = torch . float , device = device , ) output = output + noise_std * torch . randn ( output . shape , dtype = torch . float , device = device ) output [ output < 0 ] = 0 noise_std = noise_std . item () if noise_std is not None else None return output , { \"noise_std\" : noise_std } __init__ ( prob , std_min , std_max ) The image scale is 0-255 so the noise is added in the same scale. Args: prob: Probability of applying the augmentation. std_min: Minimum standard deviation of the noise. std_max: Maximum standard deviation of the noise Source code in fetalsyngen/generator/augmentation/synthseg.py 199 200 201 202 203 204 205 206 207 208 209 def __init__ ( self , prob : float , std_min : float , std_max : float ): \"\"\" The image scale is 0-255 so the noise is added in the same scale. Args: prob: Probability of applying the augmentation. std_min: Minimum standard deviation of the noise. std_max: Maximum standard deviation of the noise \"\"\" self . prob = prob self . std_min = std_min self . std_max = std_max __call__ ( output , device , genparams = {}) Apply the noise to the input image. Parameters: output ( Tensor ) \u2013 Input image to apply the noise. device ( str ) \u2013 Device to use for computation. genparams ( dict , default: {} ) \u2013 Generation parameters. Default: {}. Should contain the key \"noise_std\" if the noise standard deviation is fixed. Returns: Tensor \u2013 Image with the noise applied. Source code in fetalsyngen/generator/augmentation/synthseg.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 def __call__ ( self , output , device , genparams : dict = {}) -> torch . Tensor : \"\"\"Apply the noise to the input image. Args: output (torch.Tensor): Input image to apply the noise. device (str): Device to use for computation. genparams (dict): Generation parameters. Default: {}. Should contain the key \"noise_std\" if the noise standard deviation is fixed. Returns: Image with the noise applied.\"\"\" noise_std = None if np . random . rand () < self . prob or \"noise_std\" in genparams . keys (): noise_std = ( self . std_min + ( self . std_max - self . std_min ) * np . random . rand ( 1 ) if \"noise_std\" not in genparams . keys () else genparams [ \"noise_std\" ] ) noise_std = torch . tensor ( noise_std , dtype = torch . float , device = device , ) output = output + noise_std * torch . randn ( output . shape , dtype = torch . float , device = device ) output [ output < 0 ] = 0 noise_std = noise_std . item () if noise_std is not None else None return output , { \"noise_std\" : noise_std } RandGamma Bases: RandTransform Apply gamma correction to the input image with a probability of prob . Source code in fetalsyngen/generator/augmentation/synthseg.py 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 class RandGamma ( RandTransform ): \"\"\"Apply gamma correction to the input image with a probability of `prob`.\"\"\" def __init__ ( self , prob : float , gamma_std : float ): \"\"\" Args: prob: Probability of applying the augmentation. gamma_std: Standard deviation of the gamma correction. \"\"\" self . prob = prob self . gamma_std = gamma_std def __call__ ( self , output , device , genparams : dict = {}) -> torch . Tensor : \"\"\"Apply the gamma correction to the input image. Args: output (torch.Tensor): Input image to apply the gamma correction. device (str): Device to use for computation. genparams (dict): Generation parameters. Default: {}. Should contain the key \"gamma\" if the gamma correction is fixed. Returns: Image with the gamma correction applied. \"\"\" gamma = None if np . random . rand () < self . prob or \"gamma\" in genparams . keys (): gamma = ( np . exp ( self . gamma_std * np . random . randn ( 1 )[ 0 ]) if \"gamma\" not in genparams . keys () else genparams [ \"gamma\" ] ) gamma_tensor = torch . tensor ( gamma , dtype = float , device = device , ) output = 300.0 * ( output / 300.0 ) ** gamma_tensor return output , { \"gamma\" : gamma } __init__ ( prob , gamma_std ) Parameters: prob ( float ) \u2013 Probability of applying the augmentation. gamma_std ( float ) \u2013 Standard deviation of the gamma correction. Source code in fetalsyngen/generator/augmentation/synthseg.py 246 247 248 249 250 251 252 253 def __init__ ( self , prob : float , gamma_std : float ): \"\"\" Args: prob: Probability of applying the augmentation. gamma_std: Standard deviation of the gamma correction. \"\"\" self . prob = prob self . gamma_std = gamma_std __call__ ( output , device , genparams = {}) Apply the gamma correction to the input image. Parameters: output ( Tensor ) \u2013 Input image to apply the gamma correction. device ( str ) \u2013 Device to use for computation. genparams ( dict , default: {} ) \u2013 Generation parameters. Default: {}. Should contain the key \"gamma\" if the gamma correction is fixed. Returns: Tensor \u2013 Image with the gamma correction applied. Source code in fetalsyngen/generator/augmentation/synthseg.py 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 def __call__ ( self , output , device , genparams : dict = {}) -> torch . Tensor : \"\"\"Apply the gamma correction to the input image. Args: output (torch.Tensor): Input image to apply the gamma correction. device (str): Device to use for computation. genparams (dict): Generation parameters. Default: {}. Should contain the key \"gamma\" if the gamma correction is fixed. Returns: Image with the gamma correction applied. \"\"\" gamma = None if np . random . rand () < self . prob or \"gamma\" in genparams . keys (): gamma = ( np . exp ( self . gamma_std * np . random . randn ( 1 )[ 0 ]) if \"gamma\" not in genparams . keys () else genparams [ \"gamma\" ] ) gamma_tensor = torch . tensor ( gamma , dtype = float , device = device , ) output = 300.0 * ( output / 300.0 ) ** gamma_tensor return output , { \"gamma\" : gamma } Fixed Image Generation","title":"Generation"},{"location":"generation/#generation","text":"","title":"Generation"},{"location":"generation/#api","text":"","title":"API"},{"location":"generation/#fetalsyngen.generator.model.FetalSynthGen","text":"Source code in fetalsyngen/generator/model.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 class FetalSynthGen : def __init__ ( self , shape : Iterable [ int ], resolution : Iterable [ float ], device : str , intensity_generator : ImageFromSeeds , spatial_deform : SpatialDeformation , resampler : RandResample , bias_field : RandBiasField , noise : RandNoise , gamma : RandGamma , # optional SR artifacts blur_cortex : BlurCortex | None = None , struct_noise : StructNoise | None = None , simulate_motion : SimulateMotion | None = None , boundaries : SimulatedBoundaries | None = None , ): \"\"\" Initialize the model with the given parameters. !!!Note Augmentations related to SR artifacts are optional and can be set to None if not needed. Args: shape: Shape of the output image. resolution: Resolution of the output image. device: Device to use for computation. intensity_generator: Intensity generator. spatial_deform: Spatial deformation generator. resampler: Resampler. bias_field: Bias field generator. noise: Noise generator. gamma: Gamma correction generator. blur_cortex: Cortex blurring generator. struct_noise: Structural noise generator. simulate_motion: Motion simulation generator. boundaries: Boundaries generator \"\"\" self . shape = shape self . resolution = resolution self . intensity_generator = intensity_generator self . spatial_deform = spatial_deform self . resampled = resampler self . biasfield = bias_field self . gamma = gamma self . noise = noise self . artifacts = { \"blur_cortex\" : blur_cortex , \"struct_noise\" : struct_noise , \"simulate_motion\" : simulate_motion , \"boundaries\" : boundaries , } self . device = device def _validated_genparams ( self , d : dict ) -> dict : \"\"\"Recursively removes all the keys with None values as they are not fixed in the generation.\"\"\" if not isinstance ( d , dict ): return d # Return non-dictionaries as-is return { key : self . _validated_genparams ( value ) for key , value in d . items () if value is not None } def sample ( self , image : torch . Tensor | None , segmentation : torch . Tensor , seeds : torch . Tensor | None , genparams : dict = {}, ) -> tuple [ torch . Tensor , torch . Tensor , torch . Tensor , dict ]: \"\"\" Generate a synthetic image from the input data. Supports both random generation and from a fixed genparams dictionary. Args: image: Image to use as intensity prior if required. segmentation: Segmentation to use as spatial prior. seeds: Seeds to use for intensity generation. genparams: Dictionary with generation parameters. Used for fixed generation. Should follow the structure and be of the same type as the returned generation parameters. Returns: The synthetic image, the segmentation, the original image, and the generation parameters. \"\"\" if genparams : genparams = self . _validated_genparams ( genparams ) # 1. Generate intensity output. if seeds is not None : seeds , selected_seeds = self . intensity_generator . load_seeds ( seeds = seeds , genparams = genparams . get ( \"selected_seeds\" , {}) ) output , seed_intensities = self . intensity_generator . sample_intensities ( seeds = seeds , device = self . device , genparams = genparams . get ( \"seed_intensities\" , {}), ) else : if image is None : raise ValueError ( \"If no seeds are passed, an image must be loaded to be used as intensity prior!\" ) # normalize the image from 0 to 255 to # match the intensity generator output = ( image - image . min ()) / ( image . max () - image . min ()) * 255 selected_seeds = {} seed_intensities = {} # ensure that tensors are on the same device output = output . to ( self . device ) segmentation = segmentation . to ( self . device ) image = image . to ( self . device ) if image is not None else None # 2. Spatially deform the data image , segmentation , output , deform_params = self . spatial_deform . deform ( image = image , segmentation = segmentation , output = output , genparams = genparams . get ( \"deform_params\" , {}), ) # 3. Gamma contrast transformation output , gamma_params = self . gamma ( output , self . device , genparams = genparams . get ( \"gamma_params\" , {}) ) # 4. Bias field corruption output , bf_params = self . biasfield ( output , self . device , genparams = genparams . get ( \"bf_params\" , {}) ) # 5. Downsample to simulate lower reconstruction resolution output , factors , resample_params = self . resampled ( output , np . array ( self . resolution ), self . device , genparams = genparams . get ( \"resample_params\" , {}), ) # 6. Noise corruption output , noise_params = self . noise ( output , self . device , genparams = genparams . get ( \"noise_params\" , {}) ) # 7. Up-sample back to the original resolution/shape output = self . resampled . resize_back ( output , factors ) # 8. Induce SR-artifacts artifacts = {} for name , artifact in self . artifacts . items (): if artifact is not None : output , metadata = artifact ( output , segmentation , self . device , genparams . get ( \"artifact_params\" , {}), resolution = self . resolution , ) artifacts [ name ] = metadata # 9. Aggregete the synth params synth_params = { \"selected_seeds\" : selected_seeds , \"seed_intensities\" : seed_intensities , \"deform_params\" : deform_params , \"gamma_params\" : gamma_params , \"bf_params\" : bf_params , \"resample_params\" : resample_params , \"noise_params\" : noise_params , \"artifacts\" : artifacts , } return output , segmentation , image , synth_params","title":"FetalSynthGen"},{"location":"generation/#fetalsyngen.generator.model.FetalSynthGen.__init__","text":"Initialize the model with the given parameters. Note Augmentations related to SR artifacts are optional and can be set to None if not needed. Parameters: shape ( Iterable [ int ] ) \u2013 Shape of the output image. resolution ( Iterable [ float ] ) \u2013 Resolution of the output image. device ( str ) \u2013 Device to use for computation. intensity_generator ( ImageFromSeeds ) \u2013 Intensity generator. spatial_deform ( SpatialDeformation ) \u2013 Spatial deformation generator. resampler ( RandResample ) \u2013 Resampler. bias_field ( RandBiasField ) \u2013 Bias field generator. noise ( RandNoise ) \u2013 Noise generator. gamma ( RandGamma ) \u2013 Gamma correction generator. blur_cortex ( BlurCortex | None , default: None ) \u2013 Cortex blurring generator. struct_noise ( StructNoise | None , default: None ) \u2013 Structural noise generator. simulate_motion ( SimulateMotion | None , default: None ) \u2013 Motion simulation generator. boundaries ( SimulatedBoundaries | None , default: None ) \u2013 Boundaries generator Source code in fetalsyngen/generator/model.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def __init__ ( self , shape : Iterable [ int ], resolution : Iterable [ float ], device : str , intensity_generator : ImageFromSeeds , spatial_deform : SpatialDeformation , resampler : RandResample , bias_field : RandBiasField , noise : RandNoise , gamma : RandGamma , # optional SR artifacts blur_cortex : BlurCortex | None = None , struct_noise : StructNoise | None = None , simulate_motion : SimulateMotion | None = None , boundaries : SimulatedBoundaries | None = None , ): \"\"\" Initialize the model with the given parameters. !!!Note Augmentations related to SR artifacts are optional and can be set to None if not needed. Args: shape: Shape of the output image. resolution: Resolution of the output image. device: Device to use for computation. intensity_generator: Intensity generator. spatial_deform: Spatial deformation generator. resampler: Resampler. bias_field: Bias field generator. noise: Noise generator. gamma: Gamma correction generator. blur_cortex: Cortex blurring generator. struct_noise: Structural noise generator. simulate_motion: Motion simulation generator. boundaries: Boundaries generator \"\"\" self . shape = shape self . resolution = resolution self . intensity_generator = intensity_generator self . spatial_deform = spatial_deform self . resampled = resampler self . biasfield = bias_field self . gamma = gamma self . noise = noise self . artifacts = { \"blur_cortex\" : blur_cortex , \"struct_noise\" : struct_noise , \"simulate_motion\" : simulate_motion , \"boundaries\" : boundaries , } self . device = device","title":"__init__"},{"location":"generation/#fetalsyngen.generator.model.FetalSynthGen.sample","text":"Generate a synthetic image from the input data. Supports both random generation and from a fixed genparams dictionary. Parameters: image ( Tensor | None ) \u2013 Image to use as intensity prior if required. segmentation ( Tensor ) \u2013 Segmentation to use as spatial prior. seeds ( Tensor | None ) \u2013 Seeds to use for intensity generation. genparams ( dict , default: {} ) \u2013 Dictionary with generation parameters. Used for fixed generation. Should follow the structure and be of the same type as the returned generation parameters. Returns: tuple [ Tensor , Tensor , Tensor , dict ] \u2013 The synthetic image, the segmentation, the original image, and the generation parameters. Source code in fetalsyngen/generator/model.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def sample ( self , image : torch . Tensor | None , segmentation : torch . Tensor , seeds : torch . Tensor | None , genparams : dict = {}, ) -> tuple [ torch . Tensor , torch . Tensor , torch . Tensor , dict ]: \"\"\" Generate a synthetic image from the input data. Supports both random generation and from a fixed genparams dictionary. Args: image: Image to use as intensity prior if required. segmentation: Segmentation to use as spatial prior. seeds: Seeds to use for intensity generation. genparams: Dictionary with generation parameters. Used for fixed generation. Should follow the structure and be of the same type as the returned generation parameters. Returns: The synthetic image, the segmentation, the original image, and the generation parameters. \"\"\" if genparams : genparams = self . _validated_genparams ( genparams ) # 1. Generate intensity output. if seeds is not None : seeds , selected_seeds = self . intensity_generator . load_seeds ( seeds = seeds , genparams = genparams . get ( \"selected_seeds\" , {}) ) output , seed_intensities = self . intensity_generator . sample_intensities ( seeds = seeds , device = self . device , genparams = genparams . get ( \"seed_intensities\" , {}), ) else : if image is None : raise ValueError ( \"If no seeds are passed, an image must be loaded to be used as intensity prior!\" ) # normalize the image from 0 to 255 to # match the intensity generator output = ( image - image . min ()) / ( image . max () - image . min ()) * 255 selected_seeds = {} seed_intensities = {} # ensure that tensors are on the same device output = output . to ( self . device ) segmentation = segmentation . to ( self . device ) image = image . to ( self . device ) if image is not None else None # 2. Spatially deform the data image , segmentation , output , deform_params = self . spatial_deform . deform ( image = image , segmentation = segmentation , output = output , genparams = genparams . get ( \"deform_params\" , {}), ) # 3. Gamma contrast transformation output , gamma_params = self . gamma ( output , self . device , genparams = genparams . get ( \"gamma_params\" , {}) ) # 4. Bias field corruption output , bf_params = self . biasfield ( output , self . device , genparams = genparams . get ( \"bf_params\" , {}) ) # 5. Downsample to simulate lower reconstruction resolution output , factors , resample_params = self . resampled ( output , np . array ( self . resolution ), self . device , genparams = genparams . get ( \"resample_params\" , {}), ) # 6. Noise corruption output , noise_params = self . noise ( output , self . device , genparams = genparams . get ( \"noise_params\" , {}) ) # 7. Up-sample back to the original resolution/shape output = self . resampled . resize_back ( output , factors ) # 8. Induce SR-artifacts artifacts = {} for name , artifact in self . artifacts . items (): if artifact is not None : output , metadata = artifact ( output , segmentation , self . device , genparams . get ( \"artifact_params\" , {}), resolution = self . resolution , ) artifacts [ name ] = metadata # 9. Aggregete the synth params synth_params = { \"selected_seeds\" : selected_seeds , \"seed_intensities\" : seed_intensities , \"deform_params\" : deform_params , \"gamma_params\" : gamma_params , \"bf_params\" : bf_params , \"resample_params\" : resample_params , \"noise_params\" : noise_params , \"artifacts\" : artifacts , } return output , segmentation , image , synth_params","title":"sample"},{"location":"generation/#fetalsyngen.generator.intensity.rand_gmm.ImageFromSeeds","text":"Source code in fetalsyngen/generator/intensity/rand_gmm.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 class ImageFromSeeds : def __init__ ( self , min_subclusters : int , max_subclusters : int , seed_labels : Iterable [ int ], generation_classes : Iterable [ int ], meta_labels : int = 4 , ): \"\"\" Args: min_subclusters (int): Minimum number of subclusters to use. max_subclusters (int): Maximum number of subclusters to use. seed_labels (Iterable[int]): Iterable with all possible labels that can occur in the loaded seeds. Should be a unique set of integers starting from [0, ...]. 0 is reserved for the background, that will not have any intensity generated. generation_classes (Iterable[int]): Classes to use for generation. Seeds with the same generation calss will be generated with the same GMM. Should be the same length as seed_labels. meta_labels (int, optional): Number of meta-labels used. Defaults to 4. \"\"\" self . min_subclusters = min_subclusters self . max_subclusters = max_subclusters try : assert len ( set ( seed_labels )) == len ( seed_labels ) except AssertionError : raise ValueError ( \"Parameter seed_labels should have unique values.\" ) try : assert len ( seed_labels ) == len ( generation_classes ) except AssertionError : raise ValueError ( \"Parameters seed_labels and generation_classes should have the same lengths.\" ) self . seed_labels = seed_labels self . generation_classes = generation_classes self . meta_labels = meta_labels self . loader = SimpleITKReader () self . orientation = Orientation ( axcodes = \"RAS\" ) def load_seeds ( self , seeds : dict [ int : dict [ int : Path ]], mlabel2subclusters : dict [ int : int ] | None = None , genparams : dict = {}, ) -> torch . Tensor : \"\"\"Generate an intensity image from seeds. If seed_mapping is provided, it is used to select the number of subclusters to use for each meta label. Otherwise, the number of subclusters is randomly selected from a uniform discrete distribution between `min_subclusters` and `max_subclusters` (both inclusive). Args: seeds: Dictionary with the mapping `subcluster_number: {meta_label: seed_path}`. mlabel2subclusters: Mapping to use when defining how many subclusters to use for each meta-label. Defaults to None. genparams: Dictionary with generation parameters. Defaults to {}. Should contain the key \"mlabel2subclusters\" if the mapping is to be fixed. Returns: torch.Tensor: Intensity image with the same shape as the seeds. Tensor dimensions are **(H, W, D)**. Values inside the tensor correspond to the subclusters, and are grouped by meta-label. `1-19: CSF, 20-29: GM, 30-39: WM, 40-49: Extra-cerebral`. \"\"\" # if no mapping is provided, randomly select the number of subclusters # to use for each meta-label in the format {mlabel: n_subclusters} if mlabel2subclusters is None : mlabel2subclusters = { meta_label : np . random . randint ( self . min_subclusters , self . max_subclusters + 1 ) for meta_label in range ( 1 , self . meta_labels + 1 ) } if \"mlabel2subclusters\" in genparams . keys (): mlabel2subclusters = genparams [ \"mlabel2subclusters\" ] # load the first seed as the one corresponding to mlabel 1 seed = self . loader ( seeds [ mlabel2subclusters [ 1 ]][ 1 ]) seed = self . orientation ( seed . unsqueeze ( 0 )) # re-orient seeds to RAS for mlabel in range ( 2 , self . meta_labels + 1 ): new_seed = self . loader ( seeds [ mlabel2subclusters [ mlabel ]][ mlabel ]) new_seed = self . orientation ( new_seed . unsqueeze ( 0 )) seed += new_seed return seed . long () . squeeze ( 0 ), { \"mlabel2subclusters\" : mlabel2subclusters } def sample_intensities ( self , seeds : torch . Tensor , device : str , genparams : dict = {} ) -> torch . Tensor : \"\"\"Sample the intensities from the seeds. Args: seeds (torch.Tensor): Tensor with the seeds. device (str): Device to use. Should be \"cuda\" or \"cpu\". genparams (dict, optional): Dictionary with generation parameters. Defaults to {}. Should contain the keys \"mus\" and \"sigmas\" if the GMM parameters are to be fixed. Returns: torch.Tensor: Tensor with the intensities. \"\"\" nlabels = max ( self . seed_labels ) + 1 nsamp = len ( self . seed_labels ) # # Sample GMMs means and stds mus = ( 25 + 200 * torch . rand ( nlabels , dtype = torch . float , device = device ) if \"mus\" not in genparams . keys () else genparams [ \"mus\" ] ) sigmas = ( 5 + 20 * torch . rand ( nlabels , dtype = torch . float , device = device , ) if \"sigmas\" not in genparams . keys () else genparams [ \"sigmas\" ] ) # if there are seed labels from the same generation class # set their mean to be the same with some random perturbation if self . generation_classes != self . seed_labels : mus [ self . seed_labels ] = torch . clamp ( mus [ self . generation_classes ] + 25 * torch . randn ( nsamp , dtype = torch . float , device = device ), 0 , 225 , ) intensity_image = mus [ seeds ] + sigmas [ seeds ] * torch . randn ( seeds . shape , dtype = torch . float , device = device ) intensity_image [ intensity_image < 0 ] = 0 return intensity_image , { \"mus\" : mus , \"sigmas\" : sigmas , }","title":"ImageFromSeeds"},{"location":"generation/#fetalsyngen.generator.intensity.rand_gmm.ImageFromSeeds.__init__","text":"Parameters: min_subclusters ( int ) \u2013 Minimum number of subclusters to use. max_subclusters ( int ) \u2013 Maximum number of subclusters to use. seed_labels ( Iterable [ int ] ) \u2013 Iterable with all possible labels that can occur in the loaded seeds. Should be a unique set of integers starting from [0, ...]. 0 is reserved for the background, that will not have any intensity generated. generation_classes ( Iterable [ int ] ) \u2013 Classes to use for generation. Seeds with the same generation calss will be generated with the same GMM. Should be the same length as seed_labels. meta_labels ( int , default: 4 ) \u2013 Number of meta-labels used. Defaults to 4. Source code in fetalsyngen/generator/intensity/rand_gmm.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , min_subclusters : int , max_subclusters : int , seed_labels : Iterable [ int ], generation_classes : Iterable [ int ], meta_labels : int = 4 , ): \"\"\" Args: min_subclusters (int): Minimum number of subclusters to use. max_subclusters (int): Maximum number of subclusters to use. seed_labels (Iterable[int]): Iterable with all possible labels that can occur in the loaded seeds. Should be a unique set of integers starting from [0, ...]. 0 is reserved for the background, that will not have any intensity generated. generation_classes (Iterable[int]): Classes to use for generation. Seeds with the same generation calss will be generated with the same GMM. Should be the same length as seed_labels. meta_labels (int, optional): Number of meta-labels used. Defaults to 4. \"\"\" self . min_subclusters = min_subclusters self . max_subclusters = max_subclusters try : assert len ( set ( seed_labels )) == len ( seed_labels ) except AssertionError : raise ValueError ( \"Parameter seed_labels should have unique values.\" ) try : assert len ( seed_labels ) == len ( generation_classes ) except AssertionError : raise ValueError ( \"Parameters seed_labels and generation_classes should have the same lengths.\" ) self . seed_labels = seed_labels self . generation_classes = generation_classes self . meta_labels = meta_labels self . loader = SimpleITKReader () self . orientation = Orientation ( axcodes = \"RAS\" )","title":"__init__"},{"location":"generation/#fetalsyngen.generator.intensity.rand_gmm.ImageFromSeeds.load_seeds","text":"Generate an intensity image from seeds. If seed_mapping is provided, it is used to select the number of subclusters to use for each meta label. Otherwise, the number of subclusters is randomly selected from a uniform discrete distribution between min_subclusters and max_subclusters (both inclusive). Args: seeds: Dictionary with the mapping `subcluster_number: {meta_label: seed_path}`. mlabel2subclusters: Mapping to use when defining how many subclusters to use for each meta-label. Defaults to None. genparams: Dictionary with generation parameters. Defaults to {}. Should contain the key \"mlabel2subclusters\" if the mapping is to be fixed. Returns: Tensor \u2013 torch.Tensor: Intensity image with the same shape as the seeds. Tensor dimensions are (H, W, D) . Values inside the tensor correspond to the subclusters, and are grouped by meta-label. 1-19: CSF, 20-29: GM, 30-39: WM, 40-49: Extra-cerebral . Source code in fetalsyngen/generator/intensity/rand_gmm.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def load_seeds ( self , seeds : dict [ int : dict [ int : Path ]], mlabel2subclusters : dict [ int : int ] | None = None , genparams : dict = {}, ) -> torch . Tensor : \"\"\"Generate an intensity image from seeds. If seed_mapping is provided, it is used to select the number of subclusters to use for each meta label. Otherwise, the number of subclusters is randomly selected from a uniform discrete distribution between `min_subclusters` and `max_subclusters` (both inclusive). Args: seeds: Dictionary with the mapping `subcluster_number: {meta_label: seed_path}`. mlabel2subclusters: Mapping to use when defining how many subclusters to use for each meta-label. Defaults to None. genparams: Dictionary with generation parameters. Defaults to {}. Should contain the key \"mlabel2subclusters\" if the mapping is to be fixed. Returns: torch.Tensor: Intensity image with the same shape as the seeds. Tensor dimensions are **(H, W, D)**. Values inside the tensor correspond to the subclusters, and are grouped by meta-label. `1-19: CSF, 20-29: GM, 30-39: WM, 40-49: Extra-cerebral`. \"\"\" # if no mapping is provided, randomly select the number of subclusters # to use for each meta-label in the format {mlabel: n_subclusters} if mlabel2subclusters is None : mlabel2subclusters = { meta_label : np . random . randint ( self . min_subclusters , self . max_subclusters + 1 ) for meta_label in range ( 1 , self . meta_labels + 1 ) } if \"mlabel2subclusters\" in genparams . keys (): mlabel2subclusters = genparams [ \"mlabel2subclusters\" ] # load the first seed as the one corresponding to mlabel 1 seed = self . loader ( seeds [ mlabel2subclusters [ 1 ]][ 1 ]) seed = self . orientation ( seed . unsqueeze ( 0 )) # re-orient seeds to RAS for mlabel in range ( 2 , self . meta_labels + 1 ): new_seed = self . loader ( seeds [ mlabel2subclusters [ mlabel ]][ mlabel ]) new_seed = self . orientation ( new_seed . unsqueeze ( 0 )) seed += new_seed return seed . long () . squeeze ( 0 ), { \"mlabel2subclusters\" : mlabel2subclusters }","title":"load_seeds"},{"location":"generation/#fetalsyngen.generator.intensity.rand_gmm.ImageFromSeeds.sample_intensities","text":"Sample the intensities from the seeds. Parameters: seeds ( Tensor ) \u2013 Tensor with the seeds. device ( str ) \u2013 Device to use. Should be \"cuda\" or \"cpu\". genparams ( dict , default: {} ) \u2013 Dictionary with generation parameters. Defaults to {}. Should contain the keys \"mus\" and \"sigmas\" if the GMM parameters are to be fixed. Returns: Tensor \u2013 torch.Tensor: Tensor with the intensities. Source code in fetalsyngen/generator/intensity/rand_gmm.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def sample_intensities ( self , seeds : torch . Tensor , device : str , genparams : dict = {} ) -> torch . Tensor : \"\"\"Sample the intensities from the seeds. Args: seeds (torch.Tensor): Tensor with the seeds. device (str): Device to use. Should be \"cuda\" or \"cpu\". genparams (dict, optional): Dictionary with generation parameters. Defaults to {}. Should contain the keys \"mus\" and \"sigmas\" if the GMM parameters are to be fixed. Returns: torch.Tensor: Tensor with the intensities. \"\"\" nlabels = max ( self . seed_labels ) + 1 nsamp = len ( self . seed_labels ) # # Sample GMMs means and stds mus = ( 25 + 200 * torch . rand ( nlabels , dtype = torch . float , device = device ) if \"mus\" not in genparams . keys () else genparams [ \"mus\" ] ) sigmas = ( 5 + 20 * torch . rand ( nlabels , dtype = torch . float , device = device , ) if \"sigmas\" not in genparams . keys () else genparams [ \"sigmas\" ] ) # if there are seed labels from the same generation class # set their mean to be the same with some random perturbation if self . generation_classes != self . seed_labels : mus [ self . seed_labels ] = torch . clamp ( mus [ self . generation_classes ] + 25 * torch . randn ( nsamp , dtype = torch . float , device = device ), 0 , 225 , ) intensity_image = mus [ seeds ] + sigmas [ seeds ] * torch . randn ( seeds . shape , dtype = torch . float , device = device ) intensity_image [ intensity_image < 0 ] = 0 return intensity_image , { \"mus\" : mus , \"sigmas\" : sigmas , }","title":"sample_intensities"},{"location":"generation/#fetalsyngen.generator.deformation.affine_nonrigid.SpatialDeformation","text":"Class defining the spatial deformation of the image. Combines both random affine and nonlinear transformations to deform the image. Source code in fetalsyngen/generator/deformation/affine_nonrigid.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 class SpatialDeformation : \"\"\" Class defining the spatial deformation of the image. Combines both random affine and nonlinear transformations to deform the image. \"\"\" def __init__ ( self , max_rotation : float , max_shear : float , max_scaling : float , size : Iterable [ int ], prob : float , nonlinear_transform : bool , nonlin_scale_min : float , nonlin_scale_max : float , nonlin_std_max : float , flip_prb : float , device : str , ): \"\"\"Initialize the spatial deformation. Args: max_rotation (float): Maximum rotation in degrees. max_shear (float): Maximum shear. max_scaling (float): Maximum scaling. size (Iterable[int]): Size of the output image. prob (float): Probability of applying the deformation. nonlinear_transform (bool): Whether to apply nonlinear transformation. nonlin_scale_min (float): Minimum scale for the nonlinear transformation. nonlin_scale_max (float): Maximum scale for the nonlinear transformation. nonlin_std_max (float): Maximum standard deviation for the nonlinear transformation. flip_prb (float): Probability of flipping the image. device (str): Device to use for computation. Either \"cuda\" or \"cpu\". \"\"\" self . size = size # 256, 256, 256 self . prob = prob self . flip_prb = flip_prb # randaffine parameters self . max_rotation = max_rotation self . max_shear = max_shear self . max_scaling = max_scaling # nonlinear transform parameters self . nonlinear_transform = nonlinear_transform self . nonlin_scale_min = nonlin_scale_min self . nonlin_scale_max = nonlin_scale_max self . nonlin_std_max = nonlin_std_max self . device = device self . _prepare_grid () def _prepare_grid ( self ): xx , yy , zz = np . meshgrid ( range ( self . size [ 0 ]), range ( self . size [ 1 ]), range ( self . size [ 2 ]), sparse = False , indexing = \"ij\" , ) self . xx = torch . tensor ( xx , dtype = torch . float , device = self . device ) self . yy = torch . tensor ( yy , dtype = torch . float , device = self . device ) self . zz = torch . tensor ( zz , dtype = torch . float , device = self . device ) self . c = torch . tensor ( ( np . array ( self . size ) - 1 ) / 2 , dtype = torch . float , device = self . device , ) self . xc = self . xx - self . c [ 0 ] self . yc = self . yy - self . c [ 1 ] self . zc = self . zz - self . c [ 2 ] def deform ( self , image , segmentation , output , genparams : dict = {} ) -> tuple [ torch . Tensor , torch . Tensor , torch . Tensor , dict ]: \"\"\"Deform the image, segmentation and output. Args: image (torch.Tensor): Image to deform. segmentation (torch.Tensor): Segmentation to deform. output (torch.Tensor): Output to deform. genparams (dict, optional): Dictionary with generation parameters. Defaults to {}. Should contain the keys \"affine\" and \"non_rigid\" if the parameters are fixed. Affine parameters should contain the keys \"rotations\", \"shears\" and \"scalings\". Non-rigid parameters should contain the keys \"nonlin_scale\", \"nonlin_std\" and \"size_F_small\". Returns: Deformed image, segmentation, output and deformation parameters. \"\"\" deform_params = {} if np . random . rand () < self . prob or len ( genparams . keys ()) > 0 : image_shape = output . shape flip = ( np . random . rand () < self . flip_prb if \"flip\" not in genparams . keys () else genparams [ \"flip\" ] ) xx2 , yy2 , zz2 , x1 , y1 , z1 , x2 , y2 , z2 , deform_params = ( self . generate_deformation ( image_shape , random_shift = True , genparams = genparams ) ) # flip the image if nessesary if flip : segmentation = torch . flip ( segmentation , [ 0 ]) output = torch . flip ( output , [ 0 ]) image = torch . flip ( image , [ 0 ]) if image is not None else None output = fast_3D_interp_torch ( output , xx2 , yy2 , zz2 , \"linear\" ) segmentation = fast_3D_interp_torch ( segmentation . to ( self . device ), xx2 , yy2 , zz2 , \"nearest\" ) if image is not None : image = fast_3D_interp_torch ( image . to ( self . device ), xx2 , yy2 , zz2 , \"linear\" ) deform_params [ \"flip\" ] = flip return image , segmentation , output , deform_params def generate_deformation ( self , image_shape , random_shift = True , genparams = {}): # sample affine deformation A , c2 , aff_params = self . random_affine_transform ( shp = image_shape , max_rotation = self . max_rotation , max_shear = self . max_shear , max_scaling = self . max_scaling , random_shift = random_shift , genparams = genparams . get ( \"affine\" , {}), ) # sample nonlinear deformation if self . nonlinear_transform : F , non_rigid_params = self . random_nonlinear_transform ( nonlin_scale_min = self . nonlin_scale_min , nonlin_scale_max = self . nonlin_scale_max , nonlin_std_max = self . nonlin_std_max , genparams = genparams . get ( \"non_rigid\" , {}), ) else : F = None non_rigid_params = {} # deform the images xx2 , yy2 , zz2 , x1 , y1 , z1 , x2 , y2 , z2 = self . deform_image ( image_shape , A , c2 , F ) return ( xx2 , yy2 , zz2 , x1 , y1 , z1 , x2 , y2 , z2 , { \"affine\" : aff_params , \"non_rigid\" : non_rigid_params , }, ) def random_affine_transform ( self , shp , max_rotation , max_shear , max_scaling , random_shift = True , genparams = {}, ): rotations = ( (( 2 * max_rotation * np . random . rand ( 3 ) - max_rotation ) / 180.0 * np . pi ) if \"rotations\" not in genparams . keys () else genparams [ \"rotations\" ] ) shears = ( 2 * max_shear * np . random . rand ( 3 ) - max_shear if \"shears\" not in genparams . keys () else genparams [ \"shears\" ] ) scalings = ( 1 + ( 2 * max_scaling * np . random . rand ( 3 ) - max_scaling ) if \"scalings\" not in genparams . keys () else genparams [ \"scalings\" ] ) # we divide distance maps by this, not perfect, but better than nothing A = torch . tensor ( make_affine_matrix ( rotations , shears , scalings ), dtype = torch . float , device = self . device , ) # sample center if random_shift : max_shift = ( torch . tensor ( np . array ( shp [ 0 : 3 ]) - self . size , dtype = torch . float , device = self . device , ) ) / 2 max_shift [ max_shift < 0 ] = 0 c2 = torch . tensor ( ( np . array ( shp [ 0 : 3 ]) - 1 ) / 2 , dtype = torch . float , device = self . device , ) + ( 2 * ( max_shift * torch . rand ( 3 , dtype = float , device = self . device )) - max_shift ) else : c2 = torch . tensor ( ( np . array ( shp [ 0 : 3 ]) - 1 ) / 2 , dtype = torch . float , device = self . device , ) affine_params = { \"rotations\" : rotations , \"shears\" : shears , \"scalings\" : scalings , } return A , c2 , affine_params def random_nonlinear_transform ( self , nonlin_scale_min , nonlin_scale_max , nonlin_std_max , genparams = {} ): nonlin_scale = ( nonlin_scale_min + np . random . rand ( 1 ) * ( nonlin_scale_max - nonlin_scale_min ) if \"nonlin_scale\" not in genparams . keys () else genparams [ \"nonlin_scale\" ] ) size_F_small = ( np . round ( nonlin_scale * np . array ( self . size )) . astype ( int ) . tolist () if \"size_F_small\" not in genparams . keys () else genparams [ \"size_F_small\" ] ) nonlin_std = ( nonlin_std_max * np . random . rand () if \"nonlin_std\" not in genparams . keys () else genparams [ \"nonlin_std\" ] ) Fsmall = nonlin_std * torch . randn ( [ * size_F_small , 3 ], dtype = torch . float , device = self . device ) F = myzoom_torch ( Fsmall , np . array ( self . size ) / size_F_small ) return F , { \"nonlin_scale\" : nonlin_scale , \"nonlin_std\" : nonlin_std , \"size_F_small\" : size_F_small , } def deform_image ( self , shp , A , c2 , F ): if F is not None : # deform the images (we do nonlinear \"first\" ie after so we can do heavy coronal deformations in photo mode) xx1 = self . xc + F [:, :, :, 0 ] yy1 = self . yc + F [:, :, :, 1 ] zz1 = self . zc + F [:, :, :, 2 ] else : xx1 = self . xc yy1 = self . yc zz1 = self . zc xx2 = A [ 0 , 0 ] * xx1 + A [ 0 , 1 ] * yy1 + A [ 0 , 2 ] * zz1 + c2 [ 0 ] yy2 = A [ 1 , 0 ] * xx1 + A [ 1 , 1 ] * yy1 + A [ 1 , 2 ] * zz1 + c2 [ 1 ] zz2 = A [ 2 , 0 ] * xx1 + A [ 2 , 1 ] * yy1 + A [ 2 , 2 ] * zz1 + c2 [ 2 ] xx2 [ xx2 < 0 ] = 0 yy2 [ yy2 < 0 ] = 0 zz2 [ zz2 < 0 ] = 0 xx2 [ xx2 > ( shp [ 0 ] - 1 )] = shp [ 0 ] - 1 yy2 [ yy2 > ( shp [ 1 ] - 1 )] = shp [ 1 ] - 1 zz2 [ zz2 > ( shp [ 2 ] - 1 )] = shp [ 2 ] - 1 # Get the margins for reading images x1 = torch . floor ( torch . min ( xx2 )) y1 = torch . floor ( torch . min ( yy2 )) z1 = torch . floor ( torch . min ( zz2 )) x2 = 1 + torch . ceil ( torch . max ( xx2 )) y2 = 1 + torch . ceil ( torch . max ( yy2 )) z2 = 1 + torch . ceil ( torch . max ( zz2 )) xx2 -= x1 yy2 -= y1 zz2 -= z1 x1 = x1 . cpu () . numpy () . astype ( int ) y1 = y1 . cpu () . numpy () . astype ( int ) z1 = z1 . cpu () . numpy () . astype ( int ) x2 = x2 . cpu () . numpy () . astype ( int ) y2 = y2 . cpu () . numpy () . astype ( int ) z2 = z2 . cpu () . numpy () . astype ( int ) return xx2 , yy2 , zz2 , x1 , y1 , z1 , x2 , y2 , z2","title":"SpatialDeformation"},{"location":"generation/#fetalsyngen.generator.deformation.affine_nonrigid.SpatialDeformation.__init__","text":"Initialize the spatial deformation. Parameters: max_rotation ( float ) \u2013 Maximum rotation in degrees. max_shear ( float ) \u2013 Maximum shear. max_scaling ( float ) \u2013 Maximum scaling. size ( Iterable [ int ] ) \u2013 Size of the output image. prob ( float ) \u2013 Probability of applying the deformation. nonlinear_transform ( bool ) \u2013 Whether to apply nonlinear transformation. nonlin_scale_min ( float ) \u2013 Minimum scale for the nonlinear transformation. nonlin_scale_max ( float ) \u2013 Maximum scale for the nonlinear transformation. nonlin_std_max ( float ) \u2013 Maximum standard deviation for the nonlinear transformation. flip_prb ( float ) \u2013 Probability of flipping the image. device ( str ) \u2013 Device to use for computation. Either \"cuda\" or \"cpu\". Source code in fetalsyngen/generator/deformation/affine_nonrigid.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def __init__ ( self , max_rotation : float , max_shear : float , max_scaling : float , size : Iterable [ int ], prob : float , nonlinear_transform : bool , nonlin_scale_min : float , nonlin_scale_max : float , nonlin_std_max : float , flip_prb : float , device : str , ): \"\"\"Initialize the spatial deformation. Args: max_rotation (float): Maximum rotation in degrees. max_shear (float): Maximum shear. max_scaling (float): Maximum scaling. size (Iterable[int]): Size of the output image. prob (float): Probability of applying the deformation. nonlinear_transform (bool): Whether to apply nonlinear transformation. nonlin_scale_min (float): Minimum scale for the nonlinear transformation. nonlin_scale_max (float): Maximum scale for the nonlinear transformation. nonlin_std_max (float): Maximum standard deviation for the nonlinear transformation. flip_prb (float): Probability of flipping the image. device (str): Device to use for computation. Either \"cuda\" or \"cpu\". \"\"\" self . size = size # 256, 256, 256 self . prob = prob self . flip_prb = flip_prb # randaffine parameters self . max_rotation = max_rotation self . max_shear = max_shear self . max_scaling = max_scaling # nonlinear transform parameters self . nonlinear_transform = nonlinear_transform self . nonlin_scale_min = nonlin_scale_min self . nonlin_scale_max = nonlin_scale_max self . nonlin_std_max = nonlin_std_max self . device = device self . _prepare_grid ()","title":"__init__"},{"location":"generation/#fetalsyngen.generator.deformation.affine_nonrigid.SpatialDeformation.deform","text":"Deform the image, segmentation and output. Parameters: image ( Tensor ) \u2013 Image to deform. segmentation ( Tensor ) \u2013 Segmentation to deform. output ( Tensor ) \u2013 Output to deform. genparams ( dict , default: {} ) \u2013 Dictionary with generation parameters. Defaults to {}. Should contain the keys \"affine\" and \"non_rigid\" if the parameters are fixed. Affine parameters should contain the keys \"rotations\", \"shears\" and \"scalings\". Non-rigid parameters should contain the keys \"nonlin_scale\", \"nonlin_std\" and \"size_F_small\". Returns: tuple [ Tensor , Tensor , Tensor , dict ] \u2013 Deformed image, segmentation, output and deformation parameters. Source code in fetalsyngen/generator/deformation/affine_nonrigid.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def deform ( self , image , segmentation , output , genparams : dict = {} ) -> tuple [ torch . Tensor , torch . Tensor , torch . Tensor , dict ]: \"\"\"Deform the image, segmentation and output. Args: image (torch.Tensor): Image to deform. segmentation (torch.Tensor): Segmentation to deform. output (torch.Tensor): Output to deform. genparams (dict, optional): Dictionary with generation parameters. Defaults to {}. Should contain the keys \"affine\" and \"non_rigid\" if the parameters are fixed. Affine parameters should contain the keys \"rotations\", \"shears\" and \"scalings\". Non-rigid parameters should contain the keys \"nonlin_scale\", \"nonlin_std\" and \"size_F_small\". Returns: Deformed image, segmentation, output and deformation parameters. \"\"\" deform_params = {} if np . random . rand () < self . prob or len ( genparams . keys ()) > 0 : image_shape = output . shape flip = ( np . random . rand () < self . flip_prb if \"flip\" not in genparams . keys () else genparams [ \"flip\" ] ) xx2 , yy2 , zz2 , x1 , y1 , z1 , x2 , y2 , z2 , deform_params = ( self . generate_deformation ( image_shape , random_shift = True , genparams = genparams ) ) # flip the image if nessesary if flip : segmentation = torch . flip ( segmentation , [ 0 ]) output = torch . flip ( output , [ 0 ]) image = torch . flip ( image , [ 0 ]) if image is not None else None output = fast_3D_interp_torch ( output , xx2 , yy2 , zz2 , \"linear\" ) segmentation = fast_3D_interp_torch ( segmentation . to ( self . device ), xx2 , yy2 , zz2 , \"nearest\" ) if image is not None : image = fast_3D_interp_torch ( image . to ( self . device ), xx2 , yy2 , zz2 , \"linear\" ) deform_params [ \"flip\" ] = flip return image , segmentation , output , deform_params","title":"deform"},{"location":"generation/#fetalsyngen.generator.augmentation.synthseg.RandResample","text":"Bases: RandTransform Resample the input image to a random resolution sampled uniformly between min_resolution and max_resolution with a probability of prob . If the resolution is smaller than the input resolution, no resampling is performed. Source code in fetalsyngen/generator/augmentation/synthseg.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 class RandResample ( RandTransform ): \"\"\"Resample the input image to a random resolution sampled uniformly between `min_resolution` and `max_resolution` with a probability of `prob`. If the resolution is smaller than the input resolution, no resampling is performed. \"\"\" def __init__ ( self , prob : float , min_resolution : float , max_resolution : float , ): \"\"\" Initialize the augmentation parameters. Args: prob (float): Probability of applying the augmentation. min_resolution (float): Minimum resolution for the augmentation (in mm). max_resolution (float): Maximum resolution for the augmentation. \"\"\" self . prob = prob self . min_resolution = min_resolution self . max_resolution = max_resolution def __call__ ( self , output , input_resolution , device , genparams : dict = {} ) -> torch . Tensor : \"\"\"Apply the resampling to the input image. Args: output (torch.Tensor): Input image to resample. input_resolution (np.array): Resolution of the input image. device (str): Device to use for computation. genparams (dict): Generation parameters. Default: {}. Should contain the key \"spacing\" if the spacing is fixed. Returns: Resampled image. \"\"\" if np . random . rand () < self . prob or \"spacing\" in genparams . keys (): input_size = np . array ( output . shape ) spacing = ( np . array ([ 1.0 , 1.0 , 1.0 ]) * self . random_uniform ( self . min_resolution , self . max_resolution ) if \"spacing\" not in genparams . keys () else genparams [ \"spacing\" ] ) # Ensure spacing and input_resolution are numpy arrays spacing = np . array ( spacing ) input_resolution = np . array ( input_resolution ) # calculate stds of gaussian kernels # used for blurring to simulate resampling # the data to different resolutions stds = ( ( 0.85 + 0.3 * np . random . rand ()) * np . log ( 5 ) / np . pi * spacing / input_resolution ) # no blur if thickness is equal or smaller to the resolution of the training data stds [ spacing <= input_resolution ] = 0.0 output_blurred = gaussian_blur_3d ( output , stds , device ) # resize the blurred output to the new resolution new_size = ( np . array ( input_size ) * input_resolution / spacing ) . astype ( int ) # calculate the factors for the interpolation factors = np . array ( new_size ) / np . array ( input_size ) # delta is the offset for the interpolation delta = ( 1.0 - factors ) / ( 2.0 * factors ) vx = np . arange ( delta [ 0 ], delta [ 0 ] + new_size [ 0 ] / factors [ 0 ], 1 / factors [ 0 ] )[: new_size [ 0 ]] vy = np . arange ( delta [ 1 ], delta [ 1 ] + new_size [ 1 ] / factors [ 1 ], 1 / factors [ 1 ] )[: new_size [ 1 ]] vz = np . arange ( delta [ 2 ], delta [ 2 ] + new_size [ 2 ] / factors [ 2 ], 1 / factors [ 2 ] )[: new_size [ 2 ]] II , JJ , KK = np . meshgrid ( vx , vy , vz , sparse = False , indexing = \"ij\" ) II = torch . tensor ( II , dtype = torch . float , device = device ) JJ = torch . tensor ( JJ , dtype = torch . float , device = device ) KK = torch . tensor ( KK , dtype = torch . float , device = device ) output_resized = fast_3D_interp_torch ( output_blurred , II , JJ , KK , \"linear\" ) return output_resized , factors , { \"spacing\" : spacing . tolist ()} else : return output , None , { \"spacing\" : None } def resize_back ( self , output_resized , factors ): if factors is not None : output_resized = myzoom_torch ( output_resized , 1 / factors ) return output_resized / torch . max ( output_resized ) else : return output_resized","title":"RandResample"},{"location":"generation/#fetalsyngen.generator.augmentation.synthseg.RandResample.__init__","text":"Initialize the augmentation parameters. Parameters: prob ( float ) \u2013 Probability of applying the augmentation. min_resolution ( float ) \u2013 Minimum resolution for the augmentation (in mm). max_resolution ( float ) \u2013 Maximum resolution for the augmentation. Source code in fetalsyngen/generator/augmentation/synthseg.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , prob : float , min_resolution : float , max_resolution : float , ): \"\"\" Initialize the augmentation parameters. Args: prob (float): Probability of applying the augmentation. min_resolution (float): Minimum resolution for the augmentation (in mm). max_resolution (float): Maximum resolution for the augmentation. \"\"\" self . prob = prob self . min_resolution = min_resolution self . max_resolution = max_resolution","title":"__init__"},{"location":"generation/#fetalsyngen.generator.augmentation.synthseg.RandResample.__call__","text":"Apply the resampling to the input image. Parameters: output ( Tensor ) \u2013 Input image to resample. input_resolution ( array ) \u2013 Resolution of the input image. device ( str ) \u2013 Device to use for computation. genparams ( dict , default: {} ) \u2013 Generation parameters. Default: {}. Should contain the key \"spacing\" if the spacing is fixed. Returns: Tensor \u2013 Resampled image. Source code in fetalsyngen/generator/augmentation/synthseg.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def __call__ ( self , output , input_resolution , device , genparams : dict = {} ) -> torch . Tensor : \"\"\"Apply the resampling to the input image. Args: output (torch.Tensor): Input image to resample. input_resolution (np.array): Resolution of the input image. device (str): Device to use for computation. genparams (dict): Generation parameters. Default: {}. Should contain the key \"spacing\" if the spacing is fixed. Returns: Resampled image. \"\"\" if np . random . rand () < self . prob or \"spacing\" in genparams . keys (): input_size = np . array ( output . shape ) spacing = ( np . array ([ 1.0 , 1.0 , 1.0 ]) * self . random_uniform ( self . min_resolution , self . max_resolution ) if \"spacing\" not in genparams . keys () else genparams [ \"spacing\" ] ) # Ensure spacing and input_resolution are numpy arrays spacing = np . array ( spacing ) input_resolution = np . array ( input_resolution ) # calculate stds of gaussian kernels # used for blurring to simulate resampling # the data to different resolutions stds = ( ( 0.85 + 0.3 * np . random . rand ()) * np . log ( 5 ) / np . pi * spacing / input_resolution ) # no blur if thickness is equal or smaller to the resolution of the training data stds [ spacing <= input_resolution ] = 0.0 output_blurred = gaussian_blur_3d ( output , stds , device ) # resize the blurred output to the new resolution new_size = ( np . array ( input_size ) * input_resolution / spacing ) . astype ( int ) # calculate the factors for the interpolation factors = np . array ( new_size ) / np . array ( input_size ) # delta is the offset for the interpolation delta = ( 1.0 - factors ) / ( 2.0 * factors ) vx = np . arange ( delta [ 0 ], delta [ 0 ] + new_size [ 0 ] / factors [ 0 ], 1 / factors [ 0 ] )[: new_size [ 0 ]] vy = np . arange ( delta [ 1 ], delta [ 1 ] + new_size [ 1 ] / factors [ 1 ], 1 / factors [ 1 ] )[: new_size [ 1 ]] vz = np . arange ( delta [ 2 ], delta [ 2 ] + new_size [ 2 ] / factors [ 2 ], 1 / factors [ 2 ] )[: new_size [ 2 ]] II , JJ , KK = np . meshgrid ( vx , vy , vz , sparse = False , indexing = \"ij\" ) II = torch . tensor ( II , dtype = torch . float , device = device ) JJ = torch . tensor ( JJ , dtype = torch . float , device = device ) KK = torch . tensor ( KK , dtype = torch . float , device = device ) output_resized = fast_3D_interp_torch ( output_blurred , II , JJ , KK , \"linear\" ) return output_resized , factors , { \"spacing\" : spacing . tolist ()} else : return output , None , { \"spacing\" : None }","title":"__call__"},{"location":"generation/#fetalsyngen.generator.augmentation.synthseg.RandBiasField","text":"Bases: RandTransform Add a random bias field to the input image with a probability of prob . Source code in fetalsyngen/generator/augmentation/synthseg.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 class RandBiasField ( RandTransform ): \"\"\"Add a random bias field to the input image with a probability of `prob`.\"\"\" def __init__ ( self , prob : float , scale_min : float , scale_max : float , std_min : float , std_max : float , ): \"\"\" Args: prob: Probability of applying the augmentation. scale_min: Minimum scale of the bias field. scale_max: Maximum scale of the bias field. std_min: Minimum standard deviation of the bias field. std_max: Maximum standard deviation of the bias. \"\"\" self . prob = prob self . scale_min = scale_min self . scale_max = scale_max self . std_min = std_min self . std_max = std_max def __call__ ( self , output , device , genparams : dict = {}) -> torch . Tensor : \"\"\"Apply the bias field to the input image. Args: output (torch.Tensor): Input image to apply the bias field. device (str): Device to use for computation. genparams (dict): Generation parameters. Default: {}. Should contain the keys \"bf_scale\", \"bf_std\" and \"bf_size\" if the bias field parameters are fixed. Returns: Image with the bias field applied. \"\"\" if np . random . rand () < self . prob or len ( genparams . keys ()) > 0 : image_size = output . shape bf_scale = ( self . scale_min + np . random . rand ( 1 ) * ( self . scale_max - self . scale_min ) if \"bf_scale\" not in genparams . keys () else genparams [ \"bf_scale\" ] ) bf_size = np . round ( bf_scale * np . array ( image_size )) . astype ( int ) . tolist () bf_std = ( self . std_min + ( self . std_max - self . std_min ) * np . random . rand ( 1 ) if \"bf_std\" not in genparams . keys () else genparams [ \"bf_std\" ] ) bf_low_scale = torch . tensor ( bf_std , dtype = torch . float , device = device , ) * torch . randn ( bf_size , dtype = torch . float , device = device ) bf_interp = myzoom_torch ( bf_low_scale , np . array ( image_size ) / bf_size ) bf = torch . exp ( bf_interp ) return output * bf , { \"bf_scale\" : bf_scale , \"bf_std\" : bf_std , \"bf_size\" : bf_size , } else : return output , { \"bf_scale\" : None , \"bf_std\" : None , \"bf_size\" : None }","title":"RandBiasField"},{"location":"generation/#fetalsyngen.generator.augmentation.synthseg.RandBiasField.__init__","text":"Parameters: prob ( float ) \u2013 Probability of applying the augmentation. scale_min ( float ) \u2013 Minimum scale of the bias field. scale_max ( float ) \u2013 Maximum scale of the bias field. std_min ( float ) \u2013 Minimum standard deviation of the bias field. std_max ( float ) \u2013 Maximum standard deviation of the bias. Source code in fetalsyngen/generator/augmentation/synthseg.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def __init__ ( self , prob : float , scale_min : float , scale_max : float , std_min : float , std_max : float , ): \"\"\" Args: prob: Probability of applying the augmentation. scale_min: Minimum scale of the bias field. scale_max: Maximum scale of the bias field. std_min: Minimum standard deviation of the bias field. std_max: Maximum standard deviation of the bias. \"\"\" self . prob = prob self . scale_min = scale_min self . scale_max = scale_max self . std_min = std_min self . std_max = std_max","title":"__init__"},{"location":"generation/#fetalsyngen.generator.augmentation.synthseg.RandBiasField.__call__","text":"Apply the bias field to the input image. Parameters: output ( Tensor ) \u2013 Input image to apply the bias field. device ( str ) \u2013 Device to use for computation. genparams ( dict , default: {} ) \u2013 Generation parameters. Default: {}. Should contain the keys \"bf_scale\", \"bf_std\" and \"bf_size\" if the bias field parameters are fixed. Returns: Tensor \u2013 Image with the bias field applied. Source code in fetalsyngen/generator/augmentation/synthseg.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def __call__ ( self , output , device , genparams : dict = {}) -> torch . Tensor : \"\"\"Apply the bias field to the input image. Args: output (torch.Tensor): Input image to apply the bias field. device (str): Device to use for computation. genparams (dict): Generation parameters. Default: {}. Should contain the keys \"bf_scale\", \"bf_std\" and \"bf_size\" if the bias field parameters are fixed. Returns: Image with the bias field applied. \"\"\" if np . random . rand () < self . prob or len ( genparams . keys ()) > 0 : image_size = output . shape bf_scale = ( self . scale_min + np . random . rand ( 1 ) * ( self . scale_max - self . scale_min ) if \"bf_scale\" not in genparams . keys () else genparams [ \"bf_scale\" ] ) bf_size = np . round ( bf_scale * np . array ( image_size )) . astype ( int ) . tolist () bf_std = ( self . std_min + ( self . std_max - self . std_min ) * np . random . rand ( 1 ) if \"bf_std\" not in genparams . keys () else genparams [ \"bf_std\" ] ) bf_low_scale = torch . tensor ( bf_std , dtype = torch . float , device = device , ) * torch . randn ( bf_size , dtype = torch . float , device = device ) bf_interp = myzoom_torch ( bf_low_scale , np . array ( image_size ) / bf_size ) bf = torch . exp ( bf_interp ) return output * bf , { \"bf_scale\" : bf_scale , \"bf_std\" : bf_std , \"bf_size\" : bf_size , } else : return output , { \"bf_scale\" : None , \"bf_std\" : None , \"bf_size\" : None }","title":"__call__"},{"location":"generation/#fetalsyngen.generator.augmentation.synthseg.RandNoise","text":"Bases: RandTransform Add random Gaussian noise to the input image with a probability of prob . Source code in fetalsyngen/generator/augmentation/synthseg.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 class RandNoise ( RandTransform ): \"\"\"Add random Gaussian noise to the input image with a probability of `prob`.\"\"\" def __init__ ( self , prob : float , std_min : float , std_max : float ): \"\"\" The image scale is 0-255 so the noise is added in the same scale. Args: prob: Probability of applying the augmentation. std_min: Minimum standard deviation of the noise. std_max: Maximum standard deviation of the noise \"\"\" self . prob = prob self . std_min = std_min self . std_max = std_max def __call__ ( self , output , device , genparams : dict = {}) -> torch . Tensor : \"\"\"Apply the noise to the input image. Args: output (torch.Tensor): Input image to apply the noise. device (str): Device to use for computation. genparams (dict): Generation parameters. Default: {}. Should contain the key \"noise_std\" if the noise standard deviation is fixed. Returns: Image with the noise applied.\"\"\" noise_std = None if np . random . rand () < self . prob or \"noise_std\" in genparams . keys (): noise_std = ( self . std_min + ( self . std_max - self . std_min ) * np . random . rand ( 1 ) if \"noise_std\" not in genparams . keys () else genparams [ \"noise_std\" ] ) noise_std = torch . tensor ( noise_std , dtype = torch . float , device = device , ) output = output + noise_std * torch . randn ( output . shape , dtype = torch . float , device = device ) output [ output < 0 ] = 0 noise_std = noise_std . item () if noise_std is not None else None return output , { \"noise_std\" : noise_std }","title":"RandNoise"},{"location":"generation/#fetalsyngen.generator.augmentation.synthseg.RandNoise.__init__","text":"The image scale is 0-255 so the noise is added in the same scale. Args: prob: Probability of applying the augmentation. std_min: Minimum standard deviation of the noise. std_max: Maximum standard deviation of the noise Source code in fetalsyngen/generator/augmentation/synthseg.py 199 200 201 202 203 204 205 206 207 208 209 def __init__ ( self , prob : float , std_min : float , std_max : float ): \"\"\" The image scale is 0-255 so the noise is added in the same scale. Args: prob: Probability of applying the augmentation. std_min: Minimum standard deviation of the noise. std_max: Maximum standard deviation of the noise \"\"\" self . prob = prob self . std_min = std_min self . std_max = std_max","title":"__init__"},{"location":"generation/#fetalsyngen.generator.augmentation.synthseg.RandNoise.__call__","text":"Apply the noise to the input image. Parameters: output ( Tensor ) \u2013 Input image to apply the noise. device ( str ) \u2013 Device to use for computation. genparams ( dict , default: {} ) \u2013 Generation parameters. Default: {}. Should contain the key \"noise_std\" if the noise standard deviation is fixed. Returns: Tensor \u2013 Image with the noise applied. Source code in fetalsyngen/generator/augmentation/synthseg.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 def __call__ ( self , output , device , genparams : dict = {}) -> torch . Tensor : \"\"\"Apply the noise to the input image. Args: output (torch.Tensor): Input image to apply the noise. device (str): Device to use for computation. genparams (dict): Generation parameters. Default: {}. Should contain the key \"noise_std\" if the noise standard deviation is fixed. Returns: Image with the noise applied.\"\"\" noise_std = None if np . random . rand () < self . prob or \"noise_std\" in genparams . keys (): noise_std = ( self . std_min + ( self . std_max - self . std_min ) * np . random . rand ( 1 ) if \"noise_std\" not in genparams . keys () else genparams [ \"noise_std\" ] ) noise_std = torch . tensor ( noise_std , dtype = torch . float , device = device , ) output = output + noise_std * torch . randn ( output . shape , dtype = torch . float , device = device ) output [ output < 0 ] = 0 noise_std = noise_std . item () if noise_std is not None else None return output , { \"noise_std\" : noise_std }","title":"__call__"},{"location":"generation/#fetalsyngen.generator.augmentation.synthseg.RandGamma","text":"Bases: RandTransform Apply gamma correction to the input image with a probability of prob . Source code in fetalsyngen/generator/augmentation/synthseg.py 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 class RandGamma ( RandTransform ): \"\"\"Apply gamma correction to the input image with a probability of `prob`.\"\"\" def __init__ ( self , prob : float , gamma_std : float ): \"\"\" Args: prob: Probability of applying the augmentation. gamma_std: Standard deviation of the gamma correction. \"\"\" self . prob = prob self . gamma_std = gamma_std def __call__ ( self , output , device , genparams : dict = {}) -> torch . Tensor : \"\"\"Apply the gamma correction to the input image. Args: output (torch.Tensor): Input image to apply the gamma correction. device (str): Device to use for computation. genparams (dict): Generation parameters. Default: {}. Should contain the key \"gamma\" if the gamma correction is fixed. Returns: Image with the gamma correction applied. \"\"\" gamma = None if np . random . rand () < self . prob or \"gamma\" in genparams . keys (): gamma = ( np . exp ( self . gamma_std * np . random . randn ( 1 )[ 0 ]) if \"gamma\" not in genparams . keys () else genparams [ \"gamma\" ] ) gamma_tensor = torch . tensor ( gamma , dtype = float , device = device , ) output = 300.0 * ( output / 300.0 ) ** gamma_tensor return output , { \"gamma\" : gamma }","title":"RandGamma"},{"location":"generation/#fetalsyngen.generator.augmentation.synthseg.RandGamma.__init__","text":"Parameters: prob ( float ) \u2013 Probability of applying the augmentation. gamma_std ( float ) \u2013 Standard deviation of the gamma correction. Source code in fetalsyngen/generator/augmentation/synthseg.py 246 247 248 249 250 251 252 253 def __init__ ( self , prob : float , gamma_std : float ): \"\"\" Args: prob: Probability of applying the augmentation. gamma_std: Standard deviation of the gamma correction. \"\"\" self . prob = prob self . gamma_std = gamma_std","title":"__init__"},{"location":"generation/#fetalsyngen.generator.augmentation.synthseg.RandGamma.__call__","text":"Apply the gamma correction to the input image. Parameters: output ( Tensor ) \u2013 Input image to apply the gamma correction. device ( str ) \u2013 Device to use for computation. genparams ( dict , default: {} ) \u2013 Generation parameters. Default: {}. Should contain the key \"gamma\" if the gamma correction is fixed. Returns: Tensor \u2013 Image with the gamma correction applied. Source code in fetalsyngen/generator/augmentation/synthseg.py 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 def __call__ ( self , output , device , genparams : dict = {}) -> torch . Tensor : \"\"\"Apply the gamma correction to the input image. Args: output (torch.Tensor): Input image to apply the gamma correction. device (str): Device to use for computation. genparams (dict): Generation parameters. Default: {}. Should contain the key \"gamma\" if the gamma correction is fixed. Returns: Image with the gamma correction applied. \"\"\" gamma = None if np . random . rand () < self . prob or \"gamma\" in genparams . keys (): gamma = ( np . exp ( self . gamma_std * np . random . randn ( 1 )[ 0 ]) if \"gamma\" not in genparams . keys () else genparams [ \"gamma\" ] ) gamma_tensor = torch . tensor ( gamma , dtype = float , device = device , ) output = 300.0 * ( output / 300.0 ) ** gamma_tensor return output , { \"gamma\" : gamma }","title":"__call__"},{"location":"generation/#fixed-image-generation","text":"","title":"Fixed Image Generation"},{"location":"seed_generation/","text":"Seed Generation Introduction Seed generation is the first step in creating synthetic fetal brain MRI images with FetalSynthGen. It addresses the limited number of segmentation classes in fetal brain MRI. It allows the simulation of finer variations in tissue data. It reduces reliance on artificial contrast between segmentation labels and instead focuses on simulating anatomical variability within tissue types, helping to prevent models from learning label-specific contrast boundaries that may not exist in real images, which can be a problem with super-resolution MRI. How is it performed? First, original segmentation labels are merged into four meta-labels : cerebrospinal fluid (CSF), white matter (WM), gray matter (GM), and non-brain tissue (skull and surrounding tissue). Then, the Expectation-Maximisation (EM) algorithm is used for intensity clustering within each meta-label. This process divides each meta-label into a random number of subclasses , from one to four. These subclasses become the basis for generating synthetic images. Later in the generation pipeline, a Gaussian Mixture Model (GMM) is sampled for each subclass, and is used to sample intensities for voxels inside of it. This process ensures the synthetic images better reflect the heterogeneous nature of fetal brain tissue and the variability seen in real MRI data. See figure below for an example of seed generation. Usage The script fetalsyngen/scripts/generate_seeds.py is used to generate seeds for the FetalSynthGen pipeline. generate_seeds.py [-h] --bids_path BIDS_PATH --out_path OUT_PATH [--max_subclasses MAX_SUBCLASSES] --annotation {feta,dhcp} options: -h, --help show this help message and exit --bids_path BIDS_PATH Path to BIDS folder with the segmentations and images for seeds generation --out_path OUT_PATH Path to save the seeds --max_subclasses MAX_SUBCLASSES How many subclasses to simulate for each tissue type (meta-label) --annotation {feta,dhcp} Annotation type. Should be either 'feta' or 'dhcp' Example: python generate_seeds.py --bids_path /path/to/bids --out_path /path/to/out --max_subclasses 10 --annotation feta Note The --annotation flag specifies the type of segmentation labels to use. The feta option uses the FeTA Challenge labels, while the dhcp option uses the Developing Human Connectome Project (dHCP) labels as they have different segmentation classes. Output Format The output seeds are saved as int8 NIfTI files, with the same dimensions as the input images to speed up the seed loading process during generation. The output folder structure is as follows: \u251c\u2500\u2500 derivatives \u2502 \u251c\u2500\u2500 seeds \u2502 \u2502 \u2514\u2500\u2500 subclasses_ { 1 - max_subclasses } \u2502 \u2502 \u2514\u2500\u2500 sub - sta30 \u2502 \u2502 \u2514\u2500\u2500 anat \u2502 \u2502 \u2514\u2500\u2500 sub - sta30_rec - irtk_T2w_dseg_mlabel_ { 1 - 4 } . nii . gz \u2514\u2500\u2500 sub - sta30 \u2514\u2500\u2500 anat \u251c\u2500\u2500 sub - sta30_rec - irtk_T2w_dseg . nii . gz \u2514\u2500\u2500 sub - sta30_rec - irtk_T2w . nii . gz","title":"Seed Generation"},{"location":"seed_generation/#seed-generation","text":"","title":"Seed Generation"},{"location":"seed_generation/#introduction","text":"Seed generation is the first step in creating synthetic fetal brain MRI images with FetalSynthGen. It addresses the limited number of segmentation classes in fetal brain MRI. It allows the simulation of finer variations in tissue data. It reduces reliance on artificial contrast between segmentation labels and instead focuses on simulating anatomical variability within tissue types, helping to prevent models from learning label-specific contrast boundaries that may not exist in real images, which can be a problem with super-resolution MRI. How is it performed? First, original segmentation labels are merged into four meta-labels : cerebrospinal fluid (CSF), white matter (WM), gray matter (GM), and non-brain tissue (skull and surrounding tissue). Then, the Expectation-Maximisation (EM) algorithm is used for intensity clustering within each meta-label. This process divides each meta-label into a random number of subclasses , from one to four. These subclasses become the basis for generating synthetic images. Later in the generation pipeline, a Gaussian Mixture Model (GMM) is sampled for each subclass, and is used to sample intensities for voxels inside of it. This process ensures the synthetic images better reflect the heterogeneous nature of fetal brain tissue and the variability seen in real MRI data. See figure below for an example of seed generation.","title":"Introduction"},{"location":"seed_generation/#usage","text":"The script fetalsyngen/scripts/generate_seeds.py is used to generate seeds for the FetalSynthGen pipeline. generate_seeds.py [-h] --bids_path BIDS_PATH --out_path OUT_PATH [--max_subclasses MAX_SUBCLASSES] --annotation {feta,dhcp} options: -h, --help show this help message and exit --bids_path BIDS_PATH Path to BIDS folder with the segmentations and images for seeds generation --out_path OUT_PATH Path to save the seeds --max_subclasses MAX_SUBCLASSES How many subclasses to simulate for each tissue type (meta-label) --annotation {feta,dhcp} Annotation type. Should be either 'feta' or 'dhcp' Example: python generate_seeds.py --bids_path /path/to/bids --out_path /path/to/out --max_subclasses 10 --annotation feta Note The --annotation flag specifies the type of segmentation labels to use. The feta option uses the FeTA Challenge labels, while the dhcp option uses the Developing Human Connectome Project (dHCP) labels as they have different segmentation classes.","title":"Usage"},{"location":"seed_generation/#output-format","text":"The output seeds are saved as int8 NIfTI files, with the same dimensions as the input images to speed up the seed loading process during generation. The output folder structure is as follows: \u251c\u2500\u2500 derivatives \u2502 \u251c\u2500\u2500 seeds \u2502 \u2502 \u2514\u2500\u2500 subclasses_ { 1 - max_subclasses } \u2502 \u2502 \u2514\u2500\u2500 sub - sta30 \u2502 \u2502 \u2514\u2500\u2500 anat \u2502 \u2502 \u2514\u2500\u2500 sub - sta30_rec - irtk_T2w_dseg_mlabel_ { 1 - 4 } . nii . gz \u2514\u2500\u2500 sub - sta30 \u2514\u2500\u2500 anat \u251c\u2500\u2500 sub - sta30_rec - irtk_T2w_dseg . nii . gz \u2514\u2500\u2500 sub - sta30_rec - irtk_T2w . nii . gz","title":"Output Format"},{"location":"sr_artif_api/","text":"Super-resolution artifacts Super-resolution reconstruction algorithms are used commonly in fetal MRI imaging to improve the resolution of the images, due to specifics of fetal (brain) MRI acquisitions (see figure below [1]). Fig. 1. (A) Illustration of the data acquisition and reconstruction in fetal brain MRI. Stacks of 2D images are acquired at multiple orientations and combined into a single 3D volume using super-resolution reconstruction techniques. Quality control checks are implemented on the stacks of 2D images (Step 1) and on the SRR volume (Step 2). (B) SRR volumes with different quality scores. (C) Example of the different SR artifacts We implemented a SR artifact simulation framework to generate synthetic fetal brain MRI images with different types of artifacts. To enable it, simply pass corresponding classes described below to the generator class . You can see examples of its application in the following notebook . It consists of following classes that each implement a specific type(s) of artifacts: Cortex blur Default configuration: blur_cortex : _target_ : fetalsyngen.generator.augmentation.artifacts.BlurCortex prob : 0.4 cortex_label : 2 nblur_min : 50 nblur_max : 200 sigma_gamma_loc : 3 sigma_gamma_scale : 1 std_blur_shape : 2 std_blur_scale : 1 fetalsyngen.generator.augmentation.artifacts.BlurCortex Bases: RandTransform Blurs the cortex in the image (like in cases with imprecise reconstructions). Given a cortex_label , blurs the cortex with a Gaussian blur (shape and scale defined by std_blur_shape and std_blur_scale ). Then, generates 3D Gaussian blobs (between nblur_min and nblur_max ) with a given width (parametrized by a gamma distribution with parameters sigma_gamma_loc and sigma_gamma_scale ) defining where the blurring will be applied. Source code in fetalsyngen/generator/augmentation/artifacts.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 class BlurCortex ( RandTransform ): \"\"\"Blurs the cortex in the image (like in cases with imprecise reconstructions). Given a `cortex_label`, blurs the cortex with a Gaussian blur (shape and scale defined by `std_blur_shape` and `std_blur_scale`). Then, generates 3D Gaussian blobs (between `nblur_min` and `nblur_max`) with a given width (parametrized by a gamma distribution with parameters `sigma_gamma_loc` and `sigma_gamma_scale`) defining where the blurring will be applied. \"\"\" def __init__ ( self , prob : float , cortex_label : int , nblur_min : int , nblur_max : int , sigma_gamma_loc : int = 3 , sigma_gamma_scale : int = 1 , std_blur_shape : int = 2 , std_blur_scale : int = 1 , ): \"\"\" Initialize the augmentation parameters. Args: prob (float): Probability of applying the augmentation. cortex_label (int): Label of the cortex in the segmentation. nblur_min (int): Minimum number of blurs to apply. nblur_max (int): Maximum number of blurs to apply. sigma_gamma_loc (int): Location parameter of the gamma distribution for the blurring width. sigma_gamma_scale (int): Scale parameter of the gamma distribution for the blurring width. std_blur_shape (int): Shape parameter of the gamma distribution defining the Gaussian blur standard deviation. std_blur_scale (int): Scale parameter of the gamma distribution defining the Gaussian blur blur standard deviation. \"\"\" self . prob = prob self . cortex_label = cortex_label self . nblur_min = nblur_min self . nblur_max = nblur_max self . sigma_gamma_loc = sigma_gamma_loc self . sigma_gamma_scale = sigma_gamma_scale self . std_blur_shape = std_blur_shape self . std_blur_scale = std_blur_scale def blur_proba ( self , shape , cortex , device ): \"\"\" Generate the probability map for the blurring based on the cortex segmentation. This functions puts more probability of a blurring occuring in the frontal region of the brain, as observed empirically. \"\"\" x , y , z = shape # Blurring is more likely to happen in the frontal lobe cortex_prob = mog_3d_tensor ( shape , [( 0 , y , z // 2 ), ( x , y , z // 2 )], [ x // 5 , y // 5 ], device , ) idx_cortex = torch . where ( cortex > 0 ) cortex_prob = cortex_prob [ idx_cortex ] cortex_prob = cortex_prob / cortex_prob . sum () return cortex_prob def __call__ ( self , output , seg , device , genparams : dict = {}, ** kwargs ) -> tuple [ torch . Tensor , dict ]: \"\"\"Apply the blurring to the input image. Args: output (torch.Tensor): Input image to resample. seg (torch.Tensor): Input segmentation corresponding to the image. device (str): Device to use for computation. genparams (dict): Generation parameters. Default: {}. Should contain the key \"spacing\" if the spacing is fixed. Returns: Resampled image and Metadata containing the blurring parameters. \"\"\" if np . random . rand () < self . prob or len ( genparams . keys ()) > 0 : nblur = ( np . random . randint ( self . nblur_min , self . nblur_max ) if \"nblur\" not in genparams . keys () else genparams [ \"nblur\" ] ) std_blurs = np . random . gamma ( self . std_blur_shape , self . std_blur_scale , 3 ) cortex = seg == self . cortex_label cortex_prob = self . blur_proba ( output . shape , cortex , device ) # Reshape cortex prob onto to the cortex idx = torch . multinomial ( cortex_prob , nblur ) idx_cortex = torch . where ( cortex > 0 ) centers = [ [ idx_cortex [ i ][ id . item ()] . item () for i in range ( 3 )] for id in idx ] # Spatial merging parameters. sigmas = np . random . gamma ( self . sigma_gamma_loc , self . sigma_gamma_scale , ( nblur , 3 ) ) gaussian = mog_3d_tensor ( output . shape , centers = centers , sigmas = sigmas , device = output . device , ) # Generate the blurred image output_blur = gaussian_blur_3d ( output . float (), stds = std_blurs , device = output . device ) output = output * ( 1 - gaussian ) + output_blur * gaussian return output , { \"nblur\" : nblur , } else : return output , { \"nblur\" : None , } __init__ ( prob , cortex_label , nblur_min , nblur_max , sigma_gamma_loc = 3 , sigma_gamma_scale = 1 , std_blur_shape = 2 , std_blur_scale = 1 ) Initialize the augmentation parameters. Parameters: prob ( float ) \u2013 Probability of applying the augmentation. cortex_label ( int ) \u2013 Label of the cortex in the segmentation. nblur_min ( int ) \u2013 Minimum number of blurs to apply. nblur_max ( int ) \u2013 Maximum number of blurs to apply. sigma_gamma_loc ( int , default: 3 ) \u2013 Location parameter of the gamma distribution for the blurring width. sigma_gamma_scale ( int , default: 1 ) \u2013 Scale parameter of the gamma distribution for the blurring width. std_blur_shape ( int , default: 2 ) \u2013 Shape parameter of the gamma distribution defining the Gaussian blur standard deviation. std_blur_scale ( int , default: 1 ) \u2013 Scale parameter of the gamma distribution defining the Gaussian blur blur standard deviation. Source code in fetalsyngen/generator/augmentation/artifacts.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def __init__ ( self , prob : float , cortex_label : int , nblur_min : int , nblur_max : int , sigma_gamma_loc : int = 3 , sigma_gamma_scale : int = 1 , std_blur_shape : int = 2 , std_blur_scale : int = 1 , ): \"\"\" Initialize the augmentation parameters. Args: prob (float): Probability of applying the augmentation. cortex_label (int): Label of the cortex in the segmentation. nblur_min (int): Minimum number of blurs to apply. nblur_max (int): Maximum number of blurs to apply. sigma_gamma_loc (int): Location parameter of the gamma distribution for the blurring width. sigma_gamma_scale (int): Scale parameter of the gamma distribution for the blurring width. std_blur_shape (int): Shape parameter of the gamma distribution defining the Gaussian blur standard deviation. std_blur_scale (int): Scale parameter of the gamma distribution defining the Gaussian blur blur standard deviation. \"\"\" self . prob = prob self . cortex_label = cortex_label self . nblur_min = nblur_min self . nblur_max = nblur_max self . sigma_gamma_loc = sigma_gamma_loc self . sigma_gamma_scale = sigma_gamma_scale self . std_blur_shape = std_blur_shape self . std_blur_scale = std_blur_scale blur_proba ( shape , cortex , device ) Generate the probability map for the blurring based on the cortex segmentation. This functions puts more probability of a blurring occuring in the frontal region of the brain, as observed empirically. Source code in fetalsyngen/generator/augmentation/artifacts.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def blur_proba ( self , shape , cortex , device ): \"\"\" Generate the probability map for the blurring based on the cortex segmentation. This functions puts more probability of a blurring occuring in the frontal region of the brain, as observed empirically. \"\"\" x , y , z = shape # Blurring is more likely to happen in the frontal lobe cortex_prob = mog_3d_tensor ( shape , [( 0 , y , z // 2 ), ( x , y , z // 2 )], [ x // 5 , y // 5 ], device , ) idx_cortex = torch . where ( cortex > 0 ) cortex_prob = cortex_prob [ idx_cortex ] cortex_prob = cortex_prob / cortex_prob . sum () return cortex_prob __call__ ( output , seg , device , genparams = {}, ** kwargs ) Apply the blurring to the input image. Parameters: output ( Tensor ) \u2013 Input image to resample. seg ( Tensor ) \u2013 Input segmentation corresponding to the image. device ( str ) \u2013 Device to use for computation. genparams ( dict , default: {} ) \u2013 Generation parameters. Default: {}. Should contain the key \"spacing\" if the spacing is fixed. Returns: tuple [ Tensor , dict ] \u2013 Resampled image and Metadata containing the blurring parameters. Source code in fetalsyngen/generator/augmentation/artifacts.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def __call__ ( self , output , seg , device , genparams : dict = {}, ** kwargs ) -> tuple [ torch . Tensor , dict ]: \"\"\"Apply the blurring to the input image. Args: output (torch.Tensor): Input image to resample. seg (torch.Tensor): Input segmentation corresponding to the image. device (str): Device to use for computation. genparams (dict): Generation parameters. Default: {}. Should contain the key \"spacing\" if the spacing is fixed. Returns: Resampled image and Metadata containing the blurring parameters. \"\"\" if np . random . rand () < self . prob or len ( genparams . keys ()) > 0 : nblur = ( np . random . randint ( self . nblur_min , self . nblur_max ) if \"nblur\" not in genparams . keys () else genparams [ \"nblur\" ] ) std_blurs = np . random . gamma ( self . std_blur_shape , self . std_blur_scale , 3 ) cortex = seg == self . cortex_label cortex_prob = self . blur_proba ( output . shape , cortex , device ) # Reshape cortex prob onto to the cortex idx = torch . multinomial ( cortex_prob , nblur ) idx_cortex = torch . where ( cortex > 0 ) centers = [ [ idx_cortex [ i ][ id . item ()] . item () for i in range ( 3 )] for id in idx ] # Spatial merging parameters. sigmas = np . random . gamma ( self . sigma_gamma_loc , self . sigma_gamma_scale , ( nblur , 3 ) ) gaussian = mog_3d_tensor ( output . shape , centers = centers , sigmas = sigmas , device = output . device , ) # Generate the blurred image output_blur = gaussian_blur_3d ( output . float (), stds = std_blurs , device = output . device ) output = output * ( 1 - gaussian ) + output_blur * gaussian return output , { \"nblur\" : nblur , } else : return output , { \"nblur\" : None , } Skull stripping artifacts Default configuration: _target_ : fetalsyngen.generator.augmentation.artifacts.SimulatedBoundaries prob_no_mask : 0.5 prob_if_mask_halo : 0.5 prob_if_mask_fuzzy : 0.5 fetalsyngen.generator.augmentation.artifacts.SimulatedBoundaries Bases: RandTransform Simulates various types of boundaries in the image, either doing no masking (with probability prob_no_mask ), adding a halo around the mask (with probability prob_if_mask_halo ), or adding fuzzy boundaries to the mask (with probability prob_if_mask_fuzzy ). Source code in fetalsyngen/generator/augmentation/artifacts.py 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 class SimulatedBoundaries ( RandTransform ): \"\"\" Simulates various types of boundaries in the image, either doing no masking (with probability `prob_no_mask`), adding a halo around the mask (with probability `prob_if_mask_halo`), or adding fuzzy boundaries to the mask (with probability `prob_if_mask_fuzzy`). \"\"\" def __init__ ( self , prob_no_mask : float , prob_if_mask_halo : float , prob_if_mask_fuzzy : float , ): \"\"\" Initialize the augmentation parameters. Args: prob_no_mask (float): Probability of not applying any mask. prob_if_mask_halo (float): Probability of applying a halo around the mask (in case masking is enabled). prob_if_mask_fuzzy (float): Probability of applying fuzzy boundaries to the mask (in case masking is enabled). \"\"\" self . prob_no_mask = prob_no_mask self . prob_halo = prob_if_mask_halo self . prob_fuzzy = prob_if_mask_fuzzy self . reset_seeds () def reset_seeds ( self ): \"\"\" Reset the seeds for the augmentation. \"\"\" self . no_mask_on = None self . halo_on = None self . halo_radius = None self . fuzzy_on = None self . n_generate_fuzzy = None self . n_centers = None self . base_sigma = None def sample_seeds ( self ): \"\"\" Sample the seeds for the augmentation. \"\"\" self . reset_seeds () self . no_mask_on = np . random . rand () < self . prob_no_mask if not self . no_mask_on : self . halo_on = np . random . rand () < self . prob_halo if self . halo_on : self . halo_radius = np . random . randint ( 5 , 15 ) self . fuzzy_on = np . random . rand () < self . prob_fuzzy if self . fuzzy_on : self . n_generate_fuzzy = np . random . randint ( 2 , 5 ) self . n_centers = np . random . poisson ( 100 ) self . base_sigma = np . random . poisson ( 8 ) def build_halo ( self , mask , radius ) -> torch . Tensor : \"\"\" Build a halo around the mask with a given radius. Args: mask (torch.Tensor): Input mask. radius (int): Radius of the halo. Returns: Mask with the halo. \"\"\" device = mask . device kernel = torch . tensor ( ball ( radius )) . float () . to ( device ) . unsqueeze ( 0 ) . unsqueeze ( 0 ) mask = mask . float () . view ( 1 , 1 , * mask . shape [ - 3 :]) mask = torch . nn . functional . conv3d ( mask , kernel , padding = \"same\" ) return ( mask > 0 ) . int () . view ( * mask . shape [ - 3 :]) def generate_fuzzy_boundaries ( self , mask , kernel_size = 7 , threshold_filter = 3 ) -> torch . Tensor : \"\"\" Generate fuzzy boundaries around the mask. Args: mask (torch.Tensor): Input mask. kernel_size (int): Size of the kernel for the dilation. threshold_filter (int): Threshold for the count of neighboring voxels. Returns: Mask with fuzzy boundaries. \"\"\" shape = mask . shape diff = ( dilate ( mask , kernel_size ) - mask ) . view ( shape [ - 3 :]) non_zero = diff . nonzero ( as_tuple = True ) idx = torch . randperm ( len ( non_zero [ 0 ]))[: int ( len ( non_zero [ 0 ]) * 0.9 )] idx = ( non_zero [ 0 ][ idx ], non_zero [ 1 ][ idx ], non_zero [ 2 ][ idx ]) diff [ idx ] = 0 dsamp = ( apply_kernel ( diff ) . squeeze () > threshold_filter ) . bool () closing = erode ( dilate ( torch . clamp ( mask + dsamp , 0 , 1 ), 5 ), 5 ) return closing . view ( shape ) def __call__ ( self , output , seg , device , genparams : dict = {}, ** kwargs ) -> tuple [ torch . Tensor , dict ]: \"\"\" Apply the simulated boundaries to the input image. Args: output (torch.Tensor): Input image to resample. seg (torch.Tensor): Input segmentation corresponding to the image. device (str): Device to use for computation. genparams (dict): Generation parameters. Returns: Image with structured noise and metadata containing the structured noise parameters. \"\"\" device = seg . device mask = ( seg > 0 ) . int () mask = mask . clone () self . sample_seeds () metadata = { \"no_mask_on\" : self . no_mask_on , \"halo_on\" : self . halo_on , \"fuzzy_on\" : self . fuzzy_on , } if self . no_mask_on : return output , metadata if self . halo_on : mask = self . build_halo ( mask , self . halo_radius ) if self . fuzzy_on : # Generate fuzzy boundaries for the mask mask_modif = mask . clone () for _ in range ( self . n_generate_fuzzy ): mask_modif = self . generate_fuzzy_boundaries ( mask_modif ) # Sample centers in the voxels that have been added # with a MoG surf = torch . where (( mask_modif - mask ) . squeeze () > 0 ) idx = torch . randperm ( surf [ 0 ] . shape [ 0 ])[: self . n_centers ] centers = [( surf [ 0 ][ i ], surf [ 1 ][ i ], surf [ 2 ][ i ]) for i in idx ] sigmas = [ self . base_sigma + 10 * np . random . beta ( 2 , 5 ) for _ in range ( len ( centers )) ] mog = mog_3d_tensor ( mask_modif . shape [ - 3 :], centers = centers , sigmas = sigmas , device = device , ) . view ( 1 , 1 , * mask_modif . shape [ - 3 :]) # Generate the probability map for the surface surf_proba = torch . zeros_like ( mog [ 0 , 0 ]) . float () surf_proba [ surf ] = mog [ 0 , 0 ][ surf ] # Generate kernel_size-1 x n_generate_fuzzy -1 dilations # Roughly matches the width of the generated halo n_dilate = 6 * ( self . n_generate_fuzzy - 1 ) # Then, generate more realistic boundaries by making the # boundary of the bask more or less large according to the # probability map. dilate_stack = [ mask ] * 2 for i in range ( n_dilate - 2 ): dilate_stack . append ( self . build_halo ( dilate_stack [ - 1 ], 1 )) # Generate a stack of dilations intersected with the mask dilate_stack = torch . stack ( dilate_stack , 0 ) * mask_modif . view ( 1 , * mask_modif . shape [ - 3 :] ) surf_proba = torch . clamp ( ( surf_proba * len ( dilate_stack ) - 1 ) . round () . int (), 0 , None ) # Generate the final mask with the fuzzily generated boundaries # and also randomized halos. one_hot = torch . nn . functional . one_hot ( surf_proba . to ( torch . int64 ), num_classes = len ( dilate_stack ) ) . int () dilate_stack = dilate_stack . permute ( 1 , 2 , 3 , 0 ) . int () mask = ( one_hot * dilate_stack ) . sum ( - 1 ) return output * mask , metadata __init__ ( prob_no_mask , prob_if_mask_halo , prob_if_mask_fuzzy ) Initialize the augmentation parameters. Parameters: prob_no_mask ( float ) \u2013 Probability of not applying any mask. prob_if_mask_halo ( float ) \u2013 Probability of applying a halo around the mask (in case masking is enabled). prob_if_mask_fuzzy ( float ) \u2013 Probability of applying fuzzy boundaries to the mask (in case masking is enabled). Source code in fetalsyngen/generator/augmentation/artifacts.py 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 def __init__ ( self , prob_no_mask : float , prob_if_mask_halo : float , prob_if_mask_fuzzy : float , ): \"\"\" Initialize the augmentation parameters. Args: prob_no_mask (float): Probability of not applying any mask. prob_if_mask_halo (float): Probability of applying a halo around the mask (in case masking is enabled). prob_if_mask_fuzzy (float): Probability of applying fuzzy boundaries to the mask (in case masking is enabled). \"\"\" self . prob_no_mask = prob_no_mask self . prob_halo = prob_if_mask_halo self . prob_fuzzy = prob_if_mask_fuzzy self . reset_seeds () reset_seeds () Reset the seeds for the augmentation. Source code in fetalsyngen/generator/augmentation/artifacts.py 399 400 401 402 403 404 405 406 407 408 409 def reset_seeds ( self ): \"\"\" Reset the seeds for the augmentation. \"\"\" self . no_mask_on = None self . halo_on = None self . halo_radius = None self . fuzzy_on = None self . n_generate_fuzzy = None self . n_centers = None self . base_sigma = None sample_seeds () Sample the seeds for the augmentation. Source code in fetalsyngen/generator/augmentation/artifacts.py 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 def sample_seeds ( self ): \"\"\" Sample the seeds for the augmentation. \"\"\" self . reset_seeds () self . no_mask_on = np . random . rand () < self . prob_no_mask if not self . no_mask_on : self . halo_on = np . random . rand () < self . prob_halo if self . halo_on : self . halo_radius = np . random . randint ( 5 , 15 ) self . fuzzy_on = np . random . rand () < self . prob_fuzzy if self . fuzzy_on : self . n_generate_fuzzy = np . random . randint ( 2 , 5 ) self . n_centers = np . random . poisson ( 100 ) self . base_sigma = np . random . poisson ( 8 ) build_halo ( mask , radius ) Build a halo around the mask with a given radius. Parameters: mask ( Tensor ) \u2013 Input mask. radius ( int ) \u2013 Radius of the halo. Returns: Tensor \u2013 Mask with the halo. Source code in fetalsyngen/generator/augmentation/artifacts.py 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 def build_halo ( self , mask , radius ) -> torch . Tensor : \"\"\" Build a halo around the mask with a given radius. Args: mask (torch.Tensor): Input mask. radius (int): Radius of the halo. Returns: Mask with the halo. \"\"\" device = mask . device kernel = torch . tensor ( ball ( radius )) . float () . to ( device ) . unsqueeze ( 0 ) . unsqueeze ( 0 ) mask = mask . float () . view ( 1 , 1 , * mask . shape [ - 3 :]) mask = torch . nn . functional . conv3d ( mask , kernel , padding = \"same\" ) return ( mask > 0 ) . int () . view ( * mask . shape [ - 3 :]) generate_fuzzy_boundaries ( mask , kernel_size = 7 , threshold_filter = 3 ) Generate fuzzy boundaries around the mask. Parameters: mask ( Tensor ) \u2013 Input mask. kernel_size ( int , default: 7 ) \u2013 Size of the kernel for the dilation. threshold_filter ( int , default: 3 ) \u2013 Threshold for the count of neighboring voxels. Returns: Tensor \u2013 Mask with fuzzy boundaries. Source code in fetalsyngen/generator/augmentation/artifacts.py 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 def generate_fuzzy_boundaries ( self , mask , kernel_size = 7 , threshold_filter = 3 ) -> torch . Tensor : \"\"\" Generate fuzzy boundaries around the mask. Args: mask (torch.Tensor): Input mask. kernel_size (int): Size of the kernel for the dilation. threshold_filter (int): Threshold for the count of neighboring voxels. Returns: Mask with fuzzy boundaries. \"\"\" shape = mask . shape diff = ( dilate ( mask , kernel_size ) - mask ) . view ( shape [ - 3 :]) non_zero = diff . nonzero ( as_tuple = True ) idx = torch . randperm ( len ( non_zero [ 0 ]))[: int ( len ( non_zero [ 0 ]) * 0.9 )] idx = ( non_zero [ 0 ][ idx ], non_zero [ 1 ][ idx ], non_zero [ 2 ][ idx ]) diff [ idx ] = 0 dsamp = ( apply_kernel ( diff ) . squeeze () > threshold_filter ) . bool () closing = erode ( dilate ( torch . clamp ( mask + dsamp , 0 , 1 ), 5 ), 5 ) return closing . view ( shape ) __call__ ( output , seg , device , genparams = {}, ** kwargs ) Apply the simulated boundaries to the input image. Parameters: output ( Tensor ) \u2013 Input image to resample. seg ( Tensor ) \u2013 Input segmentation corresponding to the image. device ( str ) \u2013 Device to use for computation. genparams ( dict , default: {} ) \u2013 Generation parameters. Returns: tuple [ Tensor , dict ] \u2013 Image with structured noise and metadata containing the structured noise parameters. Source code in fetalsyngen/generator/augmentation/artifacts.py 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 def __call__ ( self , output , seg , device , genparams : dict = {}, ** kwargs ) -> tuple [ torch . Tensor , dict ]: \"\"\" Apply the simulated boundaries to the input image. Args: output (torch.Tensor): Input image to resample. seg (torch.Tensor): Input segmentation corresponding to the image. device (str): Device to use for computation. genparams (dict): Generation parameters. Returns: Image with structured noise and metadata containing the structured noise parameters. \"\"\" device = seg . device mask = ( seg > 0 ) . int () mask = mask . clone () self . sample_seeds () metadata = { \"no_mask_on\" : self . no_mask_on , \"halo_on\" : self . halo_on , \"fuzzy_on\" : self . fuzzy_on , } if self . no_mask_on : return output , metadata if self . halo_on : mask = self . build_halo ( mask , self . halo_radius ) if self . fuzzy_on : # Generate fuzzy boundaries for the mask mask_modif = mask . clone () for _ in range ( self . n_generate_fuzzy ): mask_modif = self . generate_fuzzy_boundaries ( mask_modif ) # Sample centers in the voxels that have been added # with a MoG surf = torch . where (( mask_modif - mask ) . squeeze () > 0 ) idx = torch . randperm ( surf [ 0 ] . shape [ 0 ])[: self . n_centers ] centers = [( surf [ 0 ][ i ], surf [ 1 ][ i ], surf [ 2 ][ i ]) for i in idx ] sigmas = [ self . base_sigma + 10 * np . random . beta ( 2 , 5 ) for _ in range ( len ( centers )) ] mog = mog_3d_tensor ( mask_modif . shape [ - 3 :], centers = centers , sigmas = sigmas , device = device , ) . view ( 1 , 1 , * mask_modif . shape [ - 3 :]) # Generate the probability map for the surface surf_proba = torch . zeros_like ( mog [ 0 , 0 ]) . float () surf_proba [ surf ] = mog [ 0 , 0 ][ surf ] # Generate kernel_size-1 x n_generate_fuzzy -1 dilations # Roughly matches the width of the generated halo n_dilate = 6 * ( self . n_generate_fuzzy - 1 ) # Then, generate more realistic boundaries by making the # boundary of the bask more or less large according to the # probability map. dilate_stack = [ mask ] * 2 for i in range ( n_dilate - 2 ): dilate_stack . append ( self . build_halo ( dilate_stack [ - 1 ], 1 )) # Generate a stack of dilations intersected with the mask dilate_stack = torch . stack ( dilate_stack , 0 ) * mask_modif . view ( 1 , * mask_modif . shape [ - 3 :] ) surf_proba = torch . clamp ( ( surf_proba * len ( dilate_stack ) - 1 ) . round () . int (), 0 , None ) # Generate the final mask with the fuzzily generated boundaries # and also randomized halos. one_hot = torch . nn . functional . one_hot ( surf_proba . to ( torch . int64 ), num_classes = len ( dilate_stack ) ) . int () dilate_stack = dilate_stack . permute ( 1 , 2 , 3 , 0 ) . int () mask = ( one_hot * dilate_stack ) . sum ( - 1 ) return output * mask , metadata Structural noise Default configuration: struct_noise : _target_ : fetalsyngen.generator.augmentation.artifacts.StructNoise prob : 0.4 wm_label : 3 std_min : 0.2 std_max : 0.4 nloc_min : 5 nloc_max : 15 fetalsyngen.generator.augmentation.artifacts.StructNoise Bases: RandTransform Adds a structured noise to the white matter in the image, similar to what can be seen with NeSVoR reconstructions without prior denoising. Given a wm_label , generates a multi-scale noise (between nstages_min and nstages_max stages) with a standard deviation between std_min and std_max . The noise is then added in a spatially varying manner at nloc locations ( between n_loc_min and n_loc_max locations) in the white matter. The merging is done as a weighted sum of the original image and the noisy image, with the weights defined by a MoG with centers at the nloc locations and sigmas defined by sigma_mu and sigma_std . Source code in fetalsyngen/generator/augmentation/artifacts.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 class StructNoise ( RandTransform ): \"\"\"Adds a structured noise to the white matter in the image, similar to what can be seen with NeSVoR reconstructions without prior denoising. Given a `wm_label`, generates a multi-scale noise (between `nstages_min` and `nstages_max` stages) with a standard deviation between `std_min` and `std_max`. The noise is then added in a spatially varying manner at `nloc` locations ( between `n_loc_min` and `n_loc_max` locations) in the white matter. The merging is done as a weighted sum of the original image and the noisy image, with the weights defined by a MoG with centers at the `nloc` locations and sigmas defined by `sigma_mu` and `sigma_std`. \"\"\" ### TO REFACTOR: THIS IS PERLIN NOISE def __init__ ( self , prob : float , wm_label : int , std_min : float , std_max : float , nloc_min : int , nloc_max : int , nstages_min : int = 1 , nstages_max : int = 5 , sigma_mu : int = 25 , sigma_std : int = 5 , ): \"\"\" Initialize the augmentation parameters. Args: prob (float): Probability of applying the augmentation. wm_label (int): Label of the white matter in the segmentation. std_min (float): Minimum standard deviation of the noise. std_max (float): Maximum standard deviation of the noise. nloc_min (int): Minimum number of locations to add noise. nloc_max (int): Maximum number of locations to add noise. nstages_min (int): Minimum number of stages for the noise. nstages_max (int): Maximum number of stages for the noise. sigma_mu (int): Mean of the sigmas for the MoG. sigma_std (int): Standard deviation of the sigmas for the MoG. \"\"\" self . prob = prob self . wm_label = wm_label self . nstages_min = nstages_min self . nstages_max = nstages_max self . std_min = std_min self . std_max = std_max self . nloc_min = nloc_min self . nloc_max = nloc_max self . sigma_mu = sigma_mu self . sigma_std = sigma_std def __call__ ( self , output , seg , device , genparams : dict = {}, ** kwargs ) -> tuple [ torch . Tensor , dict ]: \"\"\" Apply the structured noise to the input image. Args: output (torch.Tensor): Input image to resample. seg (torch.Tensor): Input segmentation corresponding to the image. device (str): Device to use for computation. genparams (dict): Generation parameters. Returns: Image with structured noise and metadata containing the structured noise parameters. \"\"\" if np . random . rand () < self . prob or \"nloc\" in genparams . keys (): ## Parameters nstages = ( np . random . randint ( self . nstages_min , self . nstages_max ) if \"nstages\" not in genparams else genparams [ \"nstages\" ] ) noise_std = self . std_min + ( self . std_max - self . std_min ) * np . random . rand () nloc = ( np . random . randint ( self . nloc_min , self . nloc_max , ) if \"nloc\" not in genparams else genparams [ \"nloc\" ] ) ## wm = seg == self . wm_label idx_wm = torch . nonzero ( wm , as_tuple = True ) idx = torch . randint ( 0 , len ( idx_wm [ 0 ]), ( nloc ,)) mask = ( seg > 0 ) . int () # Add multiscale noise. Start with a small tensor and add the noise to it. lr_gaussian_noise = torch . zeros ( [ i // 2 ** nstages for i in output . shape ] ) . to ( device ) for k in range ( nstages ): shape = [ i // 2 ** ( nstages - k ) for i in output . shape ] next_shape = [ i // 2 ** ( nstages - 1 - k ) for i in output . shape ] lr_gaussian_noise += torch . randn ( shape ) . to ( device ) lr_gaussian_noise = torch . nn . functional . interpolate ( lr_gaussian_noise . unsqueeze ( 0 ) . unsqueeze ( 0 ), size = next_shape , mode = \"trilinear\" , align_corners = False , ) . squeeze () lr_gaussian_noise = lr_gaussian_noise / torch . max ( abs ( lr_gaussian_noise )) output_noisy = torch . clamp ( output + noise_std * lr_gaussian_noise , 0 , output . max () * 2 ) sigmas = ( ( torch . clamp ( self . sigma_mu + self . sigma_std * torch . randn ( len ( idx )), 1 , 40 , ) ) . cpu () . numpy () ) centers = [ ( idx_wm [ 0 ][ id ] . item (), idx_wm [ 1 ][ id ] . item (), idx_wm [ 2 ][ id ] . item (), ) for id in idx ] gaussian = mog_3d_tensor ( output . shape , centers = centers , sigmas = sigmas , device = device ) output = output * ( 1 - mask ) + mask * ( gaussian * output_noisy + ( 1 - gaussian ) * output ) args = { \"nstages\" : nstages , \"noise_std\" : noise_std , \"nloc\" : nloc , } return output , args else : return output , {} __init__ ( prob , wm_label , std_min , std_max , nloc_min , nloc_max , nstages_min = 1 , nstages_max = 5 , sigma_mu = 25 , sigma_std = 5 ) Initialize the augmentation parameters. Parameters: prob ( float ) \u2013 Probability of applying the augmentation. wm_label ( int ) \u2013 Label of the white matter in the segmentation. std_min ( float ) \u2013 Minimum standard deviation of the noise. std_max ( float ) \u2013 Maximum standard deviation of the noise. nloc_min ( int ) \u2013 Minimum number of locations to add noise. nloc_max ( int ) \u2013 Maximum number of locations to add noise. nstages_min ( int , default: 1 ) \u2013 Minimum number of stages for the noise. nstages_max ( int , default: 5 ) \u2013 Maximum number of stages for the noise. sigma_mu ( int , default: 25 ) \u2013 Mean of the sigmas for the MoG. sigma_std ( int , default: 5 ) \u2013 Standard deviation of the sigmas for the MoG. Source code in fetalsyngen/generator/augmentation/artifacts.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def __init__ ( self , prob : float , wm_label : int , std_min : float , std_max : float , nloc_min : int , nloc_max : int , nstages_min : int = 1 , nstages_max : int = 5 , sigma_mu : int = 25 , sigma_std : int = 5 , ): \"\"\" Initialize the augmentation parameters. Args: prob (float): Probability of applying the augmentation. wm_label (int): Label of the white matter in the segmentation. std_min (float): Minimum standard deviation of the noise. std_max (float): Maximum standard deviation of the noise. nloc_min (int): Minimum number of locations to add noise. nloc_max (int): Maximum number of locations to add noise. nstages_min (int): Minimum number of stages for the noise. nstages_max (int): Maximum number of stages for the noise. sigma_mu (int): Mean of the sigmas for the MoG. sigma_std (int): Standard deviation of the sigmas for the MoG. \"\"\" self . prob = prob self . wm_label = wm_label self . nstages_min = nstages_min self . nstages_max = nstages_max self . std_min = std_min self . std_max = std_max self . nloc_min = nloc_min self . nloc_max = nloc_max self . sigma_mu = sigma_mu self . sigma_std = sigma_std __call__ ( output , seg , device , genparams = {}, ** kwargs ) Apply the structured noise to the input image. Parameters: output ( Tensor ) \u2013 Input image to resample. seg ( Tensor ) \u2013 Input segmentation corresponding to the image. device ( str ) \u2013 Device to use for computation. genparams ( dict , default: {} ) \u2013 Generation parameters. Returns: tuple [ Tensor , dict ] \u2013 Image with structured noise and metadata containing the structured noise parameters. Source code in fetalsyngen/generator/augmentation/artifacts.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def __call__ ( self , output , seg , device , genparams : dict = {}, ** kwargs ) -> tuple [ torch . Tensor , dict ]: \"\"\" Apply the structured noise to the input image. Args: output (torch.Tensor): Input image to resample. seg (torch.Tensor): Input segmentation corresponding to the image. device (str): Device to use for computation. genparams (dict): Generation parameters. Returns: Image with structured noise and metadata containing the structured noise parameters. \"\"\" if np . random . rand () < self . prob or \"nloc\" in genparams . keys (): ## Parameters nstages = ( np . random . randint ( self . nstages_min , self . nstages_max ) if \"nstages\" not in genparams else genparams [ \"nstages\" ] ) noise_std = self . std_min + ( self . std_max - self . std_min ) * np . random . rand () nloc = ( np . random . randint ( self . nloc_min , self . nloc_max , ) if \"nloc\" not in genparams else genparams [ \"nloc\" ] ) ## wm = seg == self . wm_label idx_wm = torch . nonzero ( wm , as_tuple = True ) idx = torch . randint ( 0 , len ( idx_wm [ 0 ]), ( nloc ,)) mask = ( seg > 0 ) . int () # Add multiscale noise. Start with a small tensor and add the noise to it. lr_gaussian_noise = torch . zeros ( [ i // 2 ** nstages for i in output . shape ] ) . to ( device ) for k in range ( nstages ): shape = [ i // 2 ** ( nstages - k ) for i in output . shape ] next_shape = [ i // 2 ** ( nstages - 1 - k ) for i in output . shape ] lr_gaussian_noise += torch . randn ( shape ) . to ( device ) lr_gaussian_noise = torch . nn . functional . interpolate ( lr_gaussian_noise . unsqueeze ( 0 ) . unsqueeze ( 0 ), size = next_shape , mode = \"trilinear\" , align_corners = False , ) . squeeze () lr_gaussian_noise = lr_gaussian_noise / torch . max ( abs ( lr_gaussian_noise )) output_noisy = torch . clamp ( output + noise_std * lr_gaussian_noise , 0 , output . max () * 2 ) sigmas = ( ( torch . clamp ( self . sigma_mu + self . sigma_std * torch . randn ( len ( idx )), 1 , 40 , ) ) . cpu () . numpy () ) centers = [ ( idx_wm [ 0 ][ id ] . item (), idx_wm [ 1 ][ id ] . item (), idx_wm [ 2 ][ id ] . item (), ) for id in idx ] gaussian = mog_3d_tensor ( output . shape , centers = centers , sigmas = sigmas , device = device ) output = output * ( 1 - mask ) + mask * ( gaussian * output_noisy + ( 1 - gaussian ) * output ) args = { \"nstages\" : nstages , \"noise_std\" : noise_std , \"nloc\" : nloc , } return output , args else : return output , {} Artifacts related to the wrong fetal motion estimation during SR reconstruction Default configuration: simulate_motion : _target_ : fetalsyngen.generator.augmentation.artifacts.SimulateMotion prob : 0.4 scanner_params : _target_ : fetalsyngen.generator.artifacts.utils.ScannerParams resolution_slice_fac_min : 0.5 resolution_slice_fac_max : 2 resolution_slice_max : 1.5 slice_thickness_min : 1.5 slice_thickness_max : 3.5 gap_min : 1.5 gap_max : 5.5 min_num_stack : 2 max_num_stack : 6 max_num_slices : 250 noise_sigma_min : 0 noise_sigma_max : 0.1 TR_min : 1 TR_max : 2 prob_gamma : 0.1 gamma_std : 0.05 prob_void : 0.2 slice_size : null restrict_transform : False txy : 3.0 recon_params : _target_ : fetalsyngen.generator.artifacts.utils.ReconParams prob_misreg_slice : 0.08 slices_misreg_ratio : 0.1 prob_misreg_stack : 0.08 txy : 3.0 prob_merge : 0.8 merge_ngaussians_min : 2 merge_ngaussians_max : 4 prob_smooth : 0.2 prob_rm_slices : 0.3 rm_slices_min : 0.1 rm_slices_max : 0.4 fetalsyngen.generator.augmentation.artifacts.SimulateMotion Bases: RandTransform Simulates motion in the image by simulating low-resolution slices (based on the scanner_params and then doing a simple point-spread function based on the low-resolution slices (using recon_params ). Source code in fetalsyngen/generator/augmentation/artifacts.py 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 class SimulateMotion ( RandTransform ): \"\"\" Simulates motion in the image by simulating low-resolution slices (based on the `scanner_params` and then doing a simple point-spread function based on the low-resolution slices (using `recon_params`). \"\"\" def __init__ ( self , prob : float , scanner_params : ScannerParams , recon_params : ReconParams , ): \"\"\" Initialize the augmentation parameters. Args: prob (float): Probability of applying the augmentation. scanner_params (ScannerParams): Dataclass of parameters for the scanner. recon_params (ReconParams): Dataclass of parameters for the reconstructor. \"\"\" self . scanner_args = scanner_params self . recon_args = recon_params self . prob = prob def __call__ ( self , output , seg , device , genparams : dict = {}, ** kwargs ) -> tuple [ torch . Tensor , dict ]: \"\"\" Apply the motion simulation to the input image. Args: output (torch.Tensor): Input image to resample. seg (torch.Tensor): Input segmentation corresponding to the image. device (str): Device to use for computation. genparams (dict): Generation parameters. Returns: Image with simulated motion and metadata containing the motion simulation parameters. \"\"\" # def _artifact_simulate_motion(self, im, seg, generator_params, res): if np . random . rand () < self . prob : device = output . device dshape = ( 1 , 1 , * output . shape [ - 3 :]) res = kwargs [ \"resolution\" ] res_ = np . float64 ( res [ 0 ]) metadata = {} d = { \"resolution\" : res_ , \"volume\" : output . view ( dshape ) . float () . to ( device ), \"mask\" : ( seg > 0 ) . view ( dshape ) . float () . to ( device ), \"seg\" : seg . view ( dshape ) . float () . to ( device ), \"affine\" : torch . diag ( torch . tensor ( list ( res ) + [ 1 ])) . to ( device ), \"threshold\" : 0.1 , } self . scanner_args . resolution_recon = res_ scanner = Scanner ( ** asdict ( self . scanner_args )) d_scan = scanner . scan ( d ) recon = PSFReconstructor ( ** asdict ( self . recon_args )) output , _ = recon . recon_psf ( d_scan ) metadata . update ( { \"resolution_recon\" : d_scan [ \"resolution_recon\" ], \"resolution_slice\" : d_scan [ \"resolution_slice\" ], \"slice_thickness\" : d_scan [ \"slice_thickness\" ], \"gap\" : d_scan [ \"gap\" ], \"nstacks\" : len ( torch . unique ( d_scan [ \"positions\" ][:, 1 ])), } ) metadata . update ( recon . get_seeds ()) return output . squeeze (), metadata else : return output , {} __init__ ( prob , scanner_params , recon_params ) Initialize the augmentation parameters. Parameters: prob ( float ) \u2013 Probability of applying the augmentation. scanner_params ( ScannerParams ) \u2013 Dataclass of parameters for the scanner. recon_params ( ReconParams ) \u2013 Dataclass of parameters for the reconstructor. Source code in fetalsyngen/generator/augmentation/artifacts.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 def __init__ ( self , prob : float , scanner_params : ScannerParams , recon_params : ReconParams , ): \"\"\" Initialize the augmentation parameters. Args: prob (float): Probability of applying the augmentation. scanner_params (ScannerParams): Dataclass of parameters for the scanner. recon_params (ReconParams): Dataclass of parameters for the reconstructor. \"\"\" self . scanner_args = scanner_params self . recon_args = recon_params self . prob = prob __call__ ( output , seg , device , genparams = {}, ** kwargs ) Apply the motion simulation to the input image. Parameters: output ( Tensor ) \u2013 Input image to resample. seg ( Tensor ) \u2013 Input segmentation corresponding to the image. device ( str ) \u2013 Device to use for computation. genparams ( dict , default: {} ) \u2013 Generation parameters. Returns: tuple [ Tensor , dict ] \u2013 Image with simulated motion and metadata containing the motion simulation parameters. Source code in fetalsyngen/generator/augmentation/artifacts.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 def __call__ ( self , output , seg , device , genparams : dict = {}, ** kwargs ) -> tuple [ torch . Tensor , dict ]: \"\"\" Apply the motion simulation to the input image. Args: output (torch.Tensor): Input image to resample. seg (torch.Tensor): Input segmentation corresponding to the image. device (str): Device to use for computation. genparams (dict): Generation parameters. Returns: Image with simulated motion and metadata containing the motion simulation parameters. \"\"\" # def _artifact_simulate_motion(self, im, seg, generator_params, res): if np . random . rand () < self . prob : device = output . device dshape = ( 1 , 1 , * output . shape [ - 3 :]) res = kwargs [ \"resolution\" ] res_ = np . float64 ( res [ 0 ]) metadata = {} d = { \"resolution\" : res_ , \"volume\" : output . view ( dshape ) . float () . to ( device ), \"mask\" : ( seg > 0 ) . view ( dshape ) . float () . to ( device ), \"seg\" : seg . view ( dshape ) . float () . to ( device ), \"affine\" : torch . diag ( torch . tensor ( list ( res ) + [ 1 ])) . to ( device ), \"threshold\" : 0.1 , } self . scanner_args . resolution_recon = res_ scanner = Scanner ( ** asdict ( self . scanner_args )) d_scan = scanner . scan ( d ) recon = PSFReconstructor ( ** asdict ( self . recon_args )) output , _ = recon . recon_psf ( d_scan ) metadata . update ( { \"resolution_recon\" : d_scan [ \"resolution_recon\" ], \"resolution_slice\" : d_scan [ \"resolution_slice\" ], \"slice_thickness\" : d_scan [ \"slice_thickness\" ], \"gap\" : d_scan [ \"gap\" ], \"nstacks\" : len ( torch . unique ( d_scan [ \"positions\" ][:, 1 ])), } ) metadata . update ( recon . get_seeds ()) return output . squeeze (), metadata else : return output , {} References Sanchez, Thomas, et al. \"Assessing data quality on fetal brain MRI reconstruction: a multi-site and multi-rater study.\" International Workshop on Preterm, Perinatal and Paediatric Image Analysis. Cham: Springer Nature Switzerland, 2024.","title":"Super-resolution artifacts"},{"location":"sr_artif_api/#super-resolution-artifacts","text":"Super-resolution reconstruction algorithms are used commonly in fetal MRI imaging to improve the resolution of the images, due to specifics of fetal (brain) MRI acquisitions (see figure below [1]). Fig. 1. (A) Illustration of the data acquisition and reconstruction in fetal brain MRI. Stacks of 2D images are acquired at multiple orientations and combined into a single 3D volume using super-resolution reconstruction techniques. Quality control checks are implemented on the stacks of 2D images (Step 1) and on the SRR volume (Step 2). (B) SRR volumes with different quality scores. (C) Example of the different SR artifacts We implemented a SR artifact simulation framework to generate synthetic fetal brain MRI images with different types of artifacts. To enable it, simply pass corresponding classes described below to the generator class . You can see examples of its application in the following notebook . It consists of following classes that each implement a specific type(s) of artifacts:","title":"Super-resolution artifacts"},{"location":"sr_artif_api/#cortex-blur","text":"Default configuration: blur_cortex : _target_ : fetalsyngen.generator.augmentation.artifacts.BlurCortex prob : 0.4 cortex_label : 2 nblur_min : 50 nblur_max : 200 sigma_gamma_loc : 3 sigma_gamma_scale : 1 std_blur_shape : 2 std_blur_scale : 1","title":"Cortex blur"},{"location":"sr_artif_api/#fetalsyngen.generator.augmentation.artifacts.BlurCortex","text":"Bases: RandTransform Blurs the cortex in the image (like in cases with imprecise reconstructions). Given a cortex_label , blurs the cortex with a Gaussian blur (shape and scale defined by std_blur_shape and std_blur_scale ). Then, generates 3D Gaussian blobs (between nblur_min and nblur_max ) with a given width (parametrized by a gamma distribution with parameters sigma_gamma_loc and sigma_gamma_scale ) defining where the blurring will be applied. Source code in fetalsyngen/generator/augmentation/artifacts.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 class BlurCortex ( RandTransform ): \"\"\"Blurs the cortex in the image (like in cases with imprecise reconstructions). Given a `cortex_label`, blurs the cortex with a Gaussian blur (shape and scale defined by `std_blur_shape` and `std_blur_scale`). Then, generates 3D Gaussian blobs (between `nblur_min` and `nblur_max`) with a given width (parametrized by a gamma distribution with parameters `sigma_gamma_loc` and `sigma_gamma_scale`) defining where the blurring will be applied. \"\"\" def __init__ ( self , prob : float , cortex_label : int , nblur_min : int , nblur_max : int , sigma_gamma_loc : int = 3 , sigma_gamma_scale : int = 1 , std_blur_shape : int = 2 , std_blur_scale : int = 1 , ): \"\"\" Initialize the augmentation parameters. Args: prob (float): Probability of applying the augmentation. cortex_label (int): Label of the cortex in the segmentation. nblur_min (int): Minimum number of blurs to apply. nblur_max (int): Maximum number of blurs to apply. sigma_gamma_loc (int): Location parameter of the gamma distribution for the blurring width. sigma_gamma_scale (int): Scale parameter of the gamma distribution for the blurring width. std_blur_shape (int): Shape parameter of the gamma distribution defining the Gaussian blur standard deviation. std_blur_scale (int): Scale parameter of the gamma distribution defining the Gaussian blur blur standard deviation. \"\"\" self . prob = prob self . cortex_label = cortex_label self . nblur_min = nblur_min self . nblur_max = nblur_max self . sigma_gamma_loc = sigma_gamma_loc self . sigma_gamma_scale = sigma_gamma_scale self . std_blur_shape = std_blur_shape self . std_blur_scale = std_blur_scale def blur_proba ( self , shape , cortex , device ): \"\"\" Generate the probability map for the blurring based on the cortex segmentation. This functions puts more probability of a blurring occuring in the frontal region of the brain, as observed empirically. \"\"\" x , y , z = shape # Blurring is more likely to happen in the frontal lobe cortex_prob = mog_3d_tensor ( shape , [( 0 , y , z // 2 ), ( x , y , z // 2 )], [ x // 5 , y // 5 ], device , ) idx_cortex = torch . where ( cortex > 0 ) cortex_prob = cortex_prob [ idx_cortex ] cortex_prob = cortex_prob / cortex_prob . sum () return cortex_prob def __call__ ( self , output , seg , device , genparams : dict = {}, ** kwargs ) -> tuple [ torch . Tensor , dict ]: \"\"\"Apply the blurring to the input image. Args: output (torch.Tensor): Input image to resample. seg (torch.Tensor): Input segmentation corresponding to the image. device (str): Device to use for computation. genparams (dict): Generation parameters. Default: {}. Should contain the key \"spacing\" if the spacing is fixed. Returns: Resampled image and Metadata containing the blurring parameters. \"\"\" if np . random . rand () < self . prob or len ( genparams . keys ()) > 0 : nblur = ( np . random . randint ( self . nblur_min , self . nblur_max ) if \"nblur\" not in genparams . keys () else genparams [ \"nblur\" ] ) std_blurs = np . random . gamma ( self . std_blur_shape , self . std_blur_scale , 3 ) cortex = seg == self . cortex_label cortex_prob = self . blur_proba ( output . shape , cortex , device ) # Reshape cortex prob onto to the cortex idx = torch . multinomial ( cortex_prob , nblur ) idx_cortex = torch . where ( cortex > 0 ) centers = [ [ idx_cortex [ i ][ id . item ()] . item () for i in range ( 3 )] for id in idx ] # Spatial merging parameters. sigmas = np . random . gamma ( self . sigma_gamma_loc , self . sigma_gamma_scale , ( nblur , 3 ) ) gaussian = mog_3d_tensor ( output . shape , centers = centers , sigmas = sigmas , device = output . device , ) # Generate the blurred image output_blur = gaussian_blur_3d ( output . float (), stds = std_blurs , device = output . device ) output = output * ( 1 - gaussian ) + output_blur * gaussian return output , { \"nblur\" : nblur , } else : return output , { \"nblur\" : None , }","title":"BlurCortex"},{"location":"sr_artif_api/#fetalsyngen.generator.augmentation.artifacts.BlurCortex.__init__","text":"Initialize the augmentation parameters. Parameters: prob ( float ) \u2013 Probability of applying the augmentation. cortex_label ( int ) \u2013 Label of the cortex in the segmentation. nblur_min ( int ) \u2013 Minimum number of blurs to apply. nblur_max ( int ) \u2013 Maximum number of blurs to apply. sigma_gamma_loc ( int , default: 3 ) \u2013 Location parameter of the gamma distribution for the blurring width. sigma_gamma_scale ( int , default: 1 ) \u2013 Scale parameter of the gamma distribution for the blurring width. std_blur_shape ( int , default: 2 ) \u2013 Shape parameter of the gamma distribution defining the Gaussian blur standard deviation. std_blur_scale ( int , default: 1 ) \u2013 Scale parameter of the gamma distribution defining the Gaussian blur blur standard deviation. Source code in fetalsyngen/generator/augmentation/artifacts.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def __init__ ( self , prob : float , cortex_label : int , nblur_min : int , nblur_max : int , sigma_gamma_loc : int = 3 , sigma_gamma_scale : int = 1 , std_blur_shape : int = 2 , std_blur_scale : int = 1 , ): \"\"\" Initialize the augmentation parameters. Args: prob (float): Probability of applying the augmentation. cortex_label (int): Label of the cortex in the segmentation. nblur_min (int): Minimum number of blurs to apply. nblur_max (int): Maximum number of blurs to apply. sigma_gamma_loc (int): Location parameter of the gamma distribution for the blurring width. sigma_gamma_scale (int): Scale parameter of the gamma distribution for the blurring width. std_blur_shape (int): Shape parameter of the gamma distribution defining the Gaussian blur standard deviation. std_blur_scale (int): Scale parameter of the gamma distribution defining the Gaussian blur blur standard deviation. \"\"\" self . prob = prob self . cortex_label = cortex_label self . nblur_min = nblur_min self . nblur_max = nblur_max self . sigma_gamma_loc = sigma_gamma_loc self . sigma_gamma_scale = sigma_gamma_scale self . std_blur_shape = std_blur_shape self . std_blur_scale = std_blur_scale","title":"__init__"},{"location":"sr_artif_api/#fetalsyngen.generator.augmentation.artifacts.BlurCortex.blur_proba","text":"Generate the probability map for the blurring based on the cortex segmentation. This functions puts more probability of a blurring occuring in the frontal region of the brain, as observed empirically. Source code in fetalsyngen/generator/augmentation/artifacts.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def blur_proba ( self , shape , cortex , device ): \"\"\" Generate the probability map for the blurring based on the cortex segmentation. This functions puts more probability of a blurring occuring in the frontal region of the brain, as observed empirically. \"\"\" x , y , z = shape # Blurring is more likely to happen in the frontal lobe cortex_prob = mog_3d_tensor ( shape , [( 0 , y , z // 2 ), ( x , y , z // 2 )], [ x // 5 , y // 5 ], device , ) idx_cortex = torch . where ( cortex > 0 ) cortex_prob = cortex_prob [ idx_cortex ] cortex_prob = cortex_prob / cortex_prob . sum () return cortex_prob","title":"blur_proba"},{"location":"sr_artif_api/#fetalsyngen.generator.augmentation.artifacts.BlurCortex.__call__","text":"Apply the blurring to the input image. Parameters: output ( Tensor ) \u2013 Input image to resample. seg ( Tensor ) \u2013 Input segmentation corresponding to the image. device ( str ) \u2013 Device to use for computation. genparams ( dict , default: {} ) \u2013 Generation parameters. Default: {}. Should contain the key \"spacing\" if the spacing is fixed. Returns: tuple [ Tensor , dict ] \u2013 Resampled image and Metadata containing the blurring parameters. Source code in fetalsyngen/generator/augmentation/artifacts.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def __call__ ( self , output , seg , device , genparams : dict = {}, ** kwargs ) -> tuple [ torch . Tensor , dict ]: \"\"\"Apply the blurring to the input image. Args: output (torch.Tensor): Input image to resample. seg (torch.Tensor): Input segmentation corresponding to the image. device (str): Device to use for computation. genparams (dict): Generation parameters. Default: {}. Should contain the key \"spacing\" if the spacing is fixed. Returns: Resampled image and Metadata containing the blurring parameters. \"\"\" if np . random . rand () < self . prob or len ( genparams . keys ()) > 0 : nblur = ( np . random . randint ( self . nblur_min , self . nblur_max ) if \"nblur\" not in genparams . keys () else genparams [ \"nblur\" ] ) std_blurs = np . random . gamma ( self . std_blur_shape , self . std_blur_scale , 3 ) cortex = seg == self . cortex_label cortex_prob = self . blur_proba ( output . shape , cortex , device ) # Reshape cortex prob onto to the cortex idx = torch . multinomial ( cortex_prob , nblur ) idx_cortex = torch . where ( cortex > 0 ) centers = [ [ idx_cortex [ i ][ id . item ()] . item () for i in range ( 3 )] for id in idx ] # Spatial merging parameters. sigmas = np . random . gamma ( self . sigma_gamma_loc , self . sigma_gamma_scale , ( nblur , 3 ) ) gaussian = mog_3d_tensor ( output . shape , centers = centers , sigmas = sigmas , device = output . device , ) # Generate the blurred image output_blur = gaussian_blur_3d ( output . float (), stds = std_blurs , device = output . device ) output = output * ( 1 - gaussian ) + output_blur * gaussian return output , { \"nblur\" : nblur , } else : return output , { \"nblur\" : None , }","title":"__call__"},{"location":"sr_artif_api/#skull-stripping-artifacts","text":"Default configuration: _target_ : fetalsyngen.generator.augmentation.artifacts.SimulatedBoundaries prob_no_mask : 0.5 prob_if_mask_halo : 0.5 prob_if_mask_fuzzy : 0.5","title":"Skull stripping artifacts"},{"location":"sr_artif_api/#fetalsyngen.generator.augmentation.artifacts.SimulatedBoundaries","text":"Bases: RandTransform Simulates various types of boundaries in the image, either doing no masking (with probability prob_no_mask ), adding a halo around the mask (with probability prob_if_mask_halo ), or adding fuzzy boundaries to the mask (with probability prob_if_mask_fuzzy ). Source code in fetalsyngen/generator/augmentation/artifacts.py 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 class SimulatedBoundaries ( RandTransform ): \"\"\" Simulates various types of boundaries in the image, either doing no masking (with probability `prob_no_mask`), adding a halo around the mask (with probability `prob_if_mask_halo`), or adding fuzzy boundaries to the mask (with probability `prob_if_mask_fuzzy`). \"\"\" def __init__ ( self , prob_no_mask : float , prob_if_mask_halo : float , prob_if_mask_fuzzy : float , ): \"\"\" Initialize the augmentation parameters. Args: prob_no_mask (float): Probability of not applying any mask. prob_if_mask_halo (float): Probability of applying a halo around the mask (in case masking is enabled). prob_if_mask_fuzzy (float): Probability of applying fuzzy boundaries to the mask (in case masking is enabled). \"\"\" self . prob_no_mask = prob_no_mask self . prob_halo = prob_if_mask_halo self . prob_fuzzy = prob_if_mask_fuzzy self . reset_seeds () def reset_seeds ( self ): \"\"\" Reset the seeds for the augmentation. \"\"\" self . no_mask_on = None self . halo_on = None self . halo_radius = None self . fuzzy_on = None self . n_generate_fuzzy = None self . n_centers = None self . base_sigma = None def sample_seeds ( self ): \"\"\" Sample the seeds for the augmentation. \"\"\" self . reset_seeds () self . no_mask_on = np . random . rand () < self . prob_no_mask if not self . no_mask_on : self . halo_on = np . random . rand () < self . prob_halo if self . halo_on : self . halo_radius = np . random . randint ( 5 , 15 ) self . fuzzy_on = np . random . rand () < self . prob_fuzzy if self . fuzzy_on : self . n_generate_fuzzy = np . random . randint ( 2 , 5 ) self . n_centers = np . random . poisson ( 100 ) self . base_sigma = np . random . poisson ( 8 ) def build_halo ( self , mask , radius ) -> torch . Tensor : \"\"\" Build a halo around the mask with a given radius. Args: mask (torch.Tensor): Input mask. radius (int): Radius of the halo. Returns: Mask with the halo. \"\"\" device = mask . device kernel = torch . tensor ( ball ( radius )) . float () . to ( device ) . unsqueeze ( 0 ) . unsqueeze ( 0 ) mask = mask . float () . view ( 1 , 1 , * mask . shape [ - 3 :]) mask = torch . nn . functional . conv3d ( mask , kernel , padding = \"same\" ) return ( mask > 0 ) . int () . view ( * mask . shape [ - 3 :]) def generate_fuzzy_boundaries ( self , mask , kernel_size = 7 , threshold_filter = 3 ) -> torch . Tensor : \"\"\" Generate fuzzy boundaries around the mask. Args: mask (torch.Tensor): Input mask. kernel_size (int): Size of the kernel for the dilation. threshold_filter (int): Threshold for the count of neighboring voxels. Returns: Mask with fuzzy boundaries. \"\"\" shape = mask . shape diff = ( dilate ( mask , kernel_size ) - mask ) . view ( shape [ - 3 :]) non_zero = diff . nonzero ( as_tuple = True ) idx = torch . randperm ( len ( non_zero [ 0 ]))[: int ( len ( non_zero [ 0 ]) * 0.9 )] idx = ( non_zero [ 0 ][ idx ], non_zero [ 1 ][ idx ], non_zero [ 2 ][ idx ]) diff [ idx ] = 0 dsamp = ( apply_kernel ( diff ) . squeeze () > threshold_filter ) . bool () closing = erode ( dilate ( torch . clamp ( mask + dsamp , 0 , 1 ), 5 ), 5 ) return closing . view ( shape ) def __call__ ( self , output , seg , device , genparams : dict = {}, ** kwargs ) -> tuple [ torch . Tensor , dict ]: \"\"\" Apply the simulated boundaries to the input image. Args: output (torch.Tensor): Input image to resample. seg (torch.Tensor): Input segmentation corresponding to the image. device (str): Device to use for computation. genparams (dict): Generation parameters. Returns: Image with structured noise and metadata containing the structured noise parameters. \"\"\" device = seg . device mask = ( seg > 0 ) . int () mask = mask . clone () self . sample_seeds () metadata = { \"no_mask_on\" : self . no_mask_on , \"halo_on\" : self . halo_on , \"fuzzy_on\" : self . fuzzy_on , } if self . no_mask_on : return output , metadata if self . halo_on : mask = self . build_halo ( mask , self . halo_radius ) if self . fuzzy_on : # Generate fuzzy boundaries for the mask mask_modif = mask . clone () for _ in range ( self . n_generate_fuzzy ): mask_modif = self . generate_fuzzy_boundaries ( mask_modif ) # Sample centers in the voxels that have been added # with a MoG surf = torch . where (( mask_modif - mask ) . squeeze () > 0 ) idx = torch . randperm ( surf [ 0 ] . shape [ 0 ])[: self . n_centers ] centers = [( surf [ 0 ][ i ], surf [ 1 ][ i ], surf [ 2 ][ i ]) for i in idx ] sigmas = [ self . base_sigma + 10 * np . random . beta ( 2 , 5 ) for _ in range ( len ( centers )) ] mog = mog_3d_tensor ( mask_modif . shape [ - 3 :], centers = centers , sigmas = sigmas , device = device , ) . view ( 1 , 1 , * mask_modif . shape [ - 3 :]) # Generate the probability map for the surface surf_proba = torch . zeros_like ( mog [ 0 , 0 ]) . float () surf_proba [ surf ] = mog [ 0 , 0 ][ surf ] # Generate kernel_size-1 x n_generate_fuzzy -1 dilations # Roughly matches the width of the generated halo n_dilate = 6 * ( self . n_generate_fuzzy - 1 ) # Then, generate more realistic boundaries by making the # boundary of the bask more or less large according to the # probability map. dilate_stack = [ mask ] * 2 for i in range ( n_dilate - 2 ): dilate_stack . append ( self . build_halo ( dilate_stack [ - 1 ], 1 )) # Generate a stack of dilations intersected with the mask dilate_stack = torch . stack ( dilate_stack , 0 ) * mask_modif . view ( 1 , * mask_modif . shape [ - 3 :] ) surf_proba = torch . clamp ( ( surf_proba * len ( dilate_stack ) - 1 ) . round () . int (), 0 , None ) # Generate the final mask with the fuzzily generated boundaries # and also randomized halos. one_hot = torch . nn . functional . one_hot ( surf_proba . to ( torch . int64 ), num_classes = len ( dilate_stack ) ) . int () dilate_stack = dilate_stack . permute ( 1 , 2 , 3 , 0 ) . int () mask = ( one_hot * dilate_stack ) . sum ( - 1 ) return output * mask , metadata","title":"SimulatedBoundaries"},{"location":"sr_artif_api/#fetalsyngen.generator.augmentation.artifacts.SimulatedBoundaries.__init__","text":"Initialize the augmentation parameters. Parameters: prob_no_mask ( float ) \u2013 Probability of not applying any mask. prob_if_mask_halo ( float ) \u2013 Probability of applying a halo around the mask (in case masking is enabled). prob_if_mask_fuzzy ( float ) \u2013 Probability of applying fuzzy boundaries to the mask (in case masking is enabled). Source code in fetalsyngen/generator/augmentation/artifacts.py 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 def __init__ ( self , prob_no_mask : float , prob_if_mask_halo : float , prob_if_mask_fuzzy : float , ): \"\"\" Initialize the augmentation parameters. Args: prob_no_mask (float): Probability of not applying any mask. prob_if_mask_halo (float): Probability of applying a halo around the mask (in case masking is enabled). prob_if_mask_fuzzy (float): Probability of applying fuzzy boundaries to the mask (in case masking is enabled). \"\"\" self . prob_no_mask = prob_no_mask self . prob_halo = prob_if_mask_halo self . prob_fuzzy = prob_if_mask_fuzzy self . reset_seeds ()","title":"__init__"},{"location":"sr_artif_api/#fetalsyngen.generator.augmentation.artifacts.SimulatedBoundaries.reset_seeds","text":"Reset the seeds for the augmentation. Source code in fetalsyngen/generator/augmentation/artifacts.py 399 400 401 402 403 404 405 406 407 408 409 def reset_seeds ( self ): \"\"\" Reset the seeds for the augmentation. \"\"\" self . no_mask_on = None self . halo_on = None self . halo_radius = None self . fuzzy_on = None self . n_generate_fuzzy = None self . n_centers = None self . base_sigma = None","title":"reset_seeds"},{"location":"sr_artif_api/#fetalsyngen.generator.augmentation.artifacts.SimulatedBoundaries.sample_seeds","text":"Sample the seeds for the augmentation. Source code in fetalsyngen/generator/augmentation/artifacts.py 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 def sample_seeds ( self ): \"\"\" Sample the seeds for the augmentation. \"\"\" self . reset_seeds () self . no_mask_on = np . random . rand () < self . prob_no_mask if not self . no_mask_on : self . halo_on = np . random . rand () < self . prob_halo if self . halo_on : self . halo_radius = np . random . randint ( 5 , 15 ) self . fuzzy_on = np . random . rand () < self . prob_fuzzy if self . fuzzy_on : self . n_generate_fuzzy = np . random . randint ( 2 , 5 ) self . n_centers = np . random . poisson ( 100 ) self . base_sigma = np . random . poisson ( 8 )","title":"sample_seeds"},{"location":"sr_artif_api/#fetalsyngen.generator.augmentation.artifacts.SimulatedBoundaries.build_halo","text":"Build a halo around the mask with a given radius. Parameters: mask ( Tensor ) \u2013 Input mask. radius ( int ) \u2013 Radius of the halo. Returns: Tensor \u2013 Mask with the halo. Source code in fetalsyngen/generator/augmentation/artifacts.py 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 def build_halo ( self , mask , radius ) -> torch . Tensor : \"\"\" Build a halo around the mask with a given radius. Args: mask (torch.Tensor): Input mask. radius (int): Radius of the halo. Returns: Mask with the halo. \"\"\" device = mask . device kernel = torch . tensor ( ball ( radius )) . float () . to ( device ) . unsqueeze ( 0 ) . unsqueeze ( 0 ) mask = mask . float () . view ( 1 , 1 , * mask . shape [ - 3 :]) mask = torch . nn . functional . conv3d ( mask , kernel , padding = \"same\" ) return ( mask > 0 ) . int () . view ( * mask . shape [ - 3 :])","title":"build_halo"},{"location":"sr_artif_api/#fetalsyngen.generator.augmentation.artifacts.SimulatedBoundaries.generate_fuzzy_boundaries","text":"Generate fuzzy boundaries around the mask. Parameters: mask ( Tensor ) \u2013 Input mask. kernel_size ( int , default: 7 ) \u2013 Size of the kernel for the dilation. threshold_filter ( int , default: 3 ) \u2013 Threshold for the count of neighboring voxels. Returns: Tensor \u2013 Mask with fuzzy boundaries. Source code in fetalsyngen/generator/augmentation/artifacts.py 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 def generate_fuzzy_boundaries ( self , mask , kernel_size = 7 , threshold_filter = 3 ) -> torch . Tensor : \"\"\" Generate fuzzy boundaries around the mask. Args: mask (torch.Tensor): Input mask. kernel_size (int): Size of the kernel for the dilation. threshold_filter (int): Threshold for the count of neighboring voxels. Returns: Mask with fuzzy boundaries. \"\"\" shape = mask . shape diff = ( dilate ( mask , kernel_size ) - mask ) . view ( shape [ - 3 :]) non_zero = diff . nonzero ( as_tuple = True ) idx = torch . randperm ( len ( non_zero [ 0 ]))[: int ( len ( non_zero [ 0 ]) * 0.9 )] idx = ( non_zero [ 0 ][ idx ], non_zero [ 1 ][ idx ], non_zero [ 2 ][ idx ]) diff [ idx ] = 0 dsamp = ( apply_kernel ( diff ) . squeeze () > threshold_filter ) . bool () closing = erode ( dilate ( torch . clamp ( mask + dsamp , 0 , 1 ), 5 ), 5 ) return closing . view ( shape )","title":"generate_fuzzy_boundaries"},{"location":"sr_artif_api/#fetalsyngen.generator.augmentation.artifacts.SimulatedBoundaries.__call__","text":"Apply the simulated boundaries to the input image. Parameters: output ( Tensor ) \u2013 Input image to resample. seg ( Tensor ) \u2013 Input segmentation corresponding to the image. device ( str ) \u2013 Device to use for computation. genparams ( dict , default: {} ) \u2013 Generation parameters. Returns: tuple [ Tensor , dict ] \u2013 Image with structured noise and metadata containing the structured noise parameters. Source code in fetalsyngen/generator/augmentation/artifacts.py 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 def __call__ ( self , output , seg , device , genparams : dict = {}, ** kwargs ) -> tuple [ torch . Tensor , dict ]: \"\"\" Apply the simulated boundaries to the input image. Args: output (torch.Tensor): Input image to resample. seg (torch.Tensor): Input segmentation corresponding to the image. device (str): Device to use for computation. genparams (dict): Generation parameters. Returns: Image with structured noise and metadata containing the structured noise parameters. \"\"\" device = seg . device mask = ( seg > 0 ) . int () mask = mask . clone () self . sample_seeds () metadata = { \"no_mask_on\" : self . no_mask_on , \"halo_on\" : self . halo_on , \"fuzzy_on\" : self . fuzzy_on , } if self . no_mask_on : return output , metadata if self . halo_on : mask = self . build_halo ( mask , self . halo_radius ) if self . fuzzy_on : # Generate fuzzy boundaries for the mask mask_modif = mask . clone () for _ in range ( self . n_generate_fuzzy ): mask_modif = self . generate_fuzzy_boundaries ( mask_modif ) # Sample centers in the voxels that have been added # with a MoG surf = torch . where (( mask_modif - mask ) . squeeze () > 0 ) idx = torch . randperm ( surf [ 0 ] . shape [ 0 ])[: self . n_centers ] centers = [( surf [ 0 ][ i ], surf [ 1 ][ i ], surf [ 2 ][ i ]) for i in idx ] sigmas = [ self . base_sigma + 10 * np . random . beta ( 2 , 5 ) for _ in range ( len ( centers )) ] mog = mog_3d_tensor ( mask_modif . shape [ - 3 :], centers = centers , sigmas = sigmas , device = device , ) . view ( 1 , 1 , * mask_modif . shape [ - 3 :]) # Generate the probability map for the surface surf_proba = torch . zeros_like ( mog [ 0 , 0 ]) . float () surf_proba [ surf ] = mog [ 0 , 0 ][ surf ] # Generate kernel_size-1 x n_generate_fuzzy -1 dilations # Roughly matches the width of the generated halo n_dilate = 6 * ( self . n_generate_fuzzy - 1 ) # Then, generate more realistic boundaries by making the # boundary of the bask more or less large according to the # probability map. dilate_stack = [ mask ] * 2 for i in range ( n_dilate - 2 ): dilate_stack . append ( self . build_halo ( dilate_stack [ - 1 ], 1 )) # Generate a stack of dilations intersected with the mask dilate_stack = torch . stack ( dilate_stack , 0 ) * mask_modif . view ( 1 , * mask_modif . shape [ - 3 :] ) surf_proba = torch . clamp ( ( surf_proba * len ( dilate_stack ) - 1 ) . round () . int (), 0 , None ) # Generate the final mask with the fuzzily generated boundaries # and also randomized halos. one_hot = torch . nn . functional . one_hot ( surf_proba . to ( torch . int64 ), num_classes = len ( dilate_stack ) ) . int () dilate_stack = dilate_stack . permute ( 1 , 2 , 3 , 0 ) . int () mask = ( one_hot * dilate_stack ) . sum ( - 1 ) return output * mask , metadata","title":"__call__"},{"location":"sr_artif_api/#structural-noise","text":"Default configuration: struct_noise : _target_ : fetalsyngen.generator.augmentation.artifacts.StructNoise prob : 0.4 wm_label : 3 std_min : 0.2 std_max : 0.4 nloc_min : 5 nloc_max : 15","title":"Structural noise"},{"location":"sr_artif_api/#fetalsyngen.generator.augmentation.artifacts.StructNoise","text":"Bases: RandTransform Adds a structured noise to the white matter in the image, similar to what can be seen with NeSVoR reconstructions without prior denoising. Given a wm_label , generates a multi-scale noise (between nstages_min and nstages_max stages) with a standard deviation between std_min and std_max . The noise is then added in a spatially varying manner at nloc locations ( between n_loc_min and n_loc_max locations) in the white matter. The merging is done as a weighted sum of the original image and the noisy image, with the weights defined by a MoG with centers at the nloc locations and sigmas defined by sigma_mu and sigma_std . Source code in fetalsyngen/generator/augmentation/artifacts.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 class StructNoise ( RandTransform ): \"\"\"Adds a structured noise to the white matter in the image, similar to what can be seen with NeSVoR reconstructions without prior denoising. Given a `wm_label`, generates a multi-scale noise (between `nstages_min` and `nstages_max` stages) with a standard deviation between `std_min` and `std_max`. The noise is then added in a spatially varying manner at `nloc` locations ( between `n_loc_min` and `n_loc_max` locations) in the white matter. The merging is done as a weighted sum of the original image and the noisy image, with the weights defined by a MoG with centers at the `nloc` locations and sigmas defined by `sigma_mu` and `sigma_std`. \"\"\" ### TO REFACTOR: THIS IS PERLIN NOISE def __init__ ( self , prob : float , wm_label : int , std_min : float , std_max : float , nloc_min : int , nloc_max : int , nstages_min : int = 1 , nstages_max : int = 5 , sigma_mu : int = 25 , sigma_std : int = 5 , ): \"\"\" Initialize the augmentation parameters. Args: prob (float): Probability of applying the augmentation. wm_label (int): Label of the white matter in the segmentation. std_min (float): Minimum standard deviation of the noise. std_max (float): Maximum standard deviation of the noise. nloc_min (int): Minimum number of locations to add noise. nloc_max (int): Maximum number of locations to add noise. nstages_min (int): Minimum number of stages for the noise. nstages_max (int): Maximum number of stages for the noise. sigma_mu (int): Mean of the sigmas for the MoG. sigma_std (int): Standard deviation of the sigmas for the MoG. \"\"\" self . prob = prob self . wm_label = wm_label self . nstages_min = nstages_min self . nstages_max = nstages_max self . std_min = std_min self . std_max = std_max self . nloc_min = nloc_min self . nloc_max = nloc_max self . sigma_mu = sigma_mu self . sigma_std = sigma_std def __call__ ( self , output , seg , device , genparams : dict = {}, ** kwargs ) -> tuple [ torch . Tensor , dict ]: \"\"\" Apply the structured noise to the input image. Args: output (torch.Tensor): Input image to resample. seg (torch.Tensor): Input segmentation corresponding to the image. device (str): Device to use for computation. genparams (dict): Generation parameters. Returns: Image with structured noise and metadata containing the structured noise parameters. \"\"\" if np . random . rand () < self . prob or \"nloc\" in genparams . keys (): ## Parameters nstages = ( np . random . randint ( self . nstages_min , self . nstages_max ) if \"nstages\" not in genparams else genparams [ \"nstages\" ] ) noise_std = self . std_min + ( self . std_max - self . std_min ) * np . random . rand () nloc = ( np . random . randint ( self . nloc_min , self . nloc_max , ) if \"nloc\" not in genparams else genparams [ \"nloc\" ] ) ## wm = seg == self . wm_label idx_wm = torch . nonzero ( wm , as_tuple = True ) idx = torch . randint ( 0 , len ( idx_wm [ 0 ]), ( nloc ,)) mask = ( seg > 0 ) . int () # Add multiscale noise. Start with a small tensor and add the noise to it. lr_gaussian_noise = torch . zeros ( [ i // 2 ** nstages for i in output . shape ] ) . to ( device ) for k in range ( nstages ): shape = [ i // 2 ** ( nstages - k ) for i in output . shape ] next_shape = [ i // 2 ** ( nstages - 1 - k ) for i in output . shape ] lr_gaussian_noise += torch . randn ( shape ) . to ( device ) lr_gaussian_noise = torch . nn . functional . interpolate ( lr_gaussian_noise . unsqueeze ( 0 ) . unsqueeze ( 0 ), size = next_shape , mode = \"trilinear\" , align_corners = False , ) . squeeze () lr_gaussian_noise = lr_gaussian_noise / torch . max ( abs ( lr_gaussian_noise )) output_noisy = torch . clamp ( output + noise_std * lr_gaussian_noise , 0 , output . max () * 2 ) sigmas = ( ( torch . clamp ( self . sigma_mu + self . sigma_std * torch . randn ( len ( idx )), 1 , 40 , ) ) . cpu () . numpy () ) centers = [ ( idx_wm [ 0 ][ id ] . item (), idx_wm [ 1 ][ id ] . item (), idx_wm [ 2 ][ id ] . item (), ) for id in idx ] gaussian = mog_3d_tensor ( output . shape , centers = centers , sigmas = sigmas , device = device ) output = output * ( 1 - mask ) + mask * ( gaussian * output_noisy + ( 1 - gaussian ) * output ) args = { \"nstages\" : nstages , \"noise_std\" : noise_std , \"nloc\" : nloc , } return output , args else : return output , {}","title":"StructNoise"},{"location":"sr_artif_api/#fetalsyngen.generator.augmentation.artifacts.StructNoise.__init__","text":"Initialize the augmentation parameters. Parameters: prob ( float ) \u2013 Probability of applying the augmentation. wm_label ( int ) \u2013 Label of the white matter in the segmentation. std_min ( float ) \u2013 Minimum standard deviation of the noise. std_max ( float ) \u2013 Maximum standard deviation of the noise. nloc_min ( int ) \u2013 Minimum number of locations to add noise. nloc_max ( int ) \u2013 Maximum number of locations to add noise. nstages_min ( int , default: 1 ) \u2013 Minimum number of stages for the noise. nstages_max ( int , default: 5 ) \u2013 Maximum number of stages for the noise. sigma_mu ( int , default: 25 ) \u2013 Mean of the sigmas for the MoG. sigma_std ( int , default: 5 ) \u2013 Standard deviation of the sigmas for the MoG. Source code in fetalsyngen/generator/augmentation/artifacts.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def __init__ ( self , prob : float , wm_label : int , std_min : float , std_max : float , nloc_min : int , nloc_max : int , nstages_min : int = 1 , nstages_max : int = 5 , sigma_mu : int = 25 , sigma_std : int = 5 , ): \"\"\" Initialize the augmentation parameters. Args: prob (float): Probability of applying the augmentation. wm_label (int): Label of the white matter in the segmentation. std_min (float): Minimum standard deviation of the noise. std_max (float): Maximum standard deviation of the noise. nloc_min (int): Minimum number of locations to add noise. nloc_max (int): Maximum number of locations to add noise. nstages_min (int): Minimum number of stages for the noise. nstages_max (int): Maximum number of stages for the noise. sigma_mu (int): Mean of the sigmas for the MoG. sigma_std (int): Standard deviation of the sigmas for the MoG. \"\"\" self . prob = prob self . wm_label = wm_label self . nstages_min = nstages_min self . nstages_max = nstages_max self . std_min = std_min self . std_max = std_max self . nloc_min = nloc_min self . nloc_max = nloc_max self . sigma_mu = sigma_mu self . sigma_std = sigma_std","title":"__init__"},{"location":"sr_artif_api/#fetalsyngen.generator.augmentation.artifacts.StructNoise.__call__","text":"Apply the structured noise to the input image. Parameters: output ( Tensor ) \u2013 Input image to resample. seg ( Tensor ) \u2013 Input segmentation corresponding to the image. device ( str ) \u2013 Device to use for computation. genparams ( dict , default: {} ) \u2013 Generation parameters. Returns: tuple [ Tensor , dict ] \u2013 Image with structured noise and metadata containing the structured noise parameters. Source code in fetalsyngen/generator/augmentation/artifacts.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def __call__ ( self , output , seg , device , genparams : dict = {}, ** kwargs ) -> tuple [ torch . Tensor , dict ]: \"\"\" Apply the structured noise to the input image. Args: output (torch.Tensor): Input image to resample. seg (torch.Tensor): Input segmentation corresponding to the image. device (str): Device to use for computation. genparams (dict): Generation parameters. Returns: Image with structured noise and metadata containing the structured noise parameters. \"\"\" if np . random . rand () < self . prob or \"nloc\" in genparams . keys (): ## Parameters nstages = ( np . random . randint ( self . nstages_min , self . nstages_max ) if \"nstages\" not in genparams else genparams [ \"nstages\" ] ) noise_std = self . std_min + ( self . std_max - self . std_min ) * np . random . rand () nloc = ( np . random . randint ( self . nloc_min , self . nloc_max , ) if \"nloc\" not in genparams else genparams [ \"nloc\" ] ) ## wm = seg == self . wm_label idx_wm = torch . nonzero ( wm , as_tuple = True ) idx = torch . randint ( 0 , len ( idx_wm [ 0 ]), ( nloc ,)) mask = ( seg > 0 ) . int () # Add multiscale noise. Start with a small tensor and add the noise to it. lr_gaussian_noise = torch . zeros ( [ i // 2 ** nstages for i in output . shape ] ) . to ( device ) for k in range ( nstages ): shape = [ i // 2 ** ( nstages - k ) for i in output . shape ] next_shape = [ i // 2 ** ( nstages - 1 - k ) for i in output . shape ] lr_gaussian_noise += torch . randn ( shape ) . to ( device ) lr_gaussian_noise = torch . nn . functional . interpolate ( lr_gaussian_noise . unsqueeze ( 0 ) . unsqueeze ( 0 ), size = next_shape , mode = \"trilinear\" , align_corners = False , ) . squeeze () lr_gaussian_noise = lr_gaussian_noise / torch . max ( abs ( lr_gaussian_noise )) output_noisy = torch . clamp ( output + noise_std * lr_gaussian_noise , 0 , output . max () * 2 ) sigmas = ( ( torch . clamp ( self . sigma_mu + self . sigma_std * torch . randn ( len ( idx )), 1 , 40 , ) ) . cpu () . numpy () ) centers = [ ( idx_wm [ 0 ][ id ] . item (), idx_wm [ 1 ][ id ] . item (), idx_wm [ 2 ][ id ] . item (), ) for id in idx ] gaussian = mog_3d_tensor ( output . shape , centers = centers , sigmas = sigmas , device = device ) output = output * ( 1 - mask ) + mask * ( gaussian * output_noisy + ( 1 - gaussian ) * output ) args = { \"nstages\" : nstages , \"noise_std\" : noise_std , \"nloc\" : nloc , } return output , args else : return output , {}","title":"__call__"},{"location":"sr_artif_api/#artifacts-related-to-the-wrong-fetal-motion-estimation-during-sr-reconstruction","text":"Default configuration: simulate_motion : _target_ : fetalsyngen.generator.augmentation.artifacts.SimulateMotion prob : 0.4 scanner_params : _target_ : fetalsyngen.generator.artifacts.utils.ScannerParams resolution_slice_fac_min : 0.5 resolution_slice_fac_max : 2 resolution_slice_max : 1.5 slice_thickness_min : 1.5 slice_thickness_max : 3.5 gap_min : 1.5 gap_max : 5.5 min_num_stack : 2 max_num_stack : 6 max_num_slices : 250 noise_sigma_min : 0 noise_sigma_max : 0.1 TR_min : 1 TR_max : 2 prob_gamma : 0.1 gamma_std : 0.05 prob_void : 0.2 slice_size : null restrict_transform : False txy : 3.0 recon_params : _target_ : fetalsyngen.generator.artifacts.utils.ReconParams prob_misreg_slice : 0.08 slices_misreg_ratio : 0.1 prob_misreg_stack : 0.08 txy : 3.0 prob_merge : 0.8 merge_ngaussians_min : 2 merge_ngaussians_max : 4 prob_smooth : 0.2 prob_rm_slices : 0.3 rm_slices_min : 0.1 rm_slices_max : 0.4","title":"Artifacts related to the wrong fetal motion estimation during SR reconstruction"},{"location":"sr_artif_api/#fetalsyngen.generator.augmentation.artifacts.SimulateMotion","text":"Bases: RandTransform Simulates motion in the image by simulating low-resolution slices (based on the scanner_params and then doing a simple point-spread function based on the low-resolution slices (using recon_params ). Source code in fetalsyngen/generator/augmentation/artifacts.py 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 class SimulateMotion ( RandTransform ): \"\"\" Simulates motion in the image by simulating low-resolution slices (based on the `scanner_params` and then doing a simple point-spread function based on the low-resolution slices (using `recon_params`). \"\"\" def __init__ ( self , prob : float , scanner_params : ScannerParams , recon_params : ReconParams , ): \"\"\" Initialize the augmentation parameters. Args: prob (float): Probability of applying the augmentation. scanner_params (ScannerParams): Dataclass of parameters for the scanner. recon_params (ReconParams): Dataclass of parameters for the reconstructor. \"\"\" self . scanner_args = scanner_params self . recon_args = recon_params self . prob = prob def __call__ ( self , output , seg , device , genparams : dict = {}, ** kwargs ) -> tuple [ torch . Tensor , dict ]: \"\"\" Apply the motion simulation to the input image. Args: output (torch.Tensor): Input image to resample. seg (torch.Tensor): Input segmentation corresponding to the image. device (str): Device to use for computation. genparams (dict): Generation parameters. Returns: Image with simulated motion and metadata containing the motion simulation parameters. \"\"\" # def _artifact_simulate_motion(self, im, seg, generator_params, res): if np . random . rand () < self . prob : device = output . device dshape = ( 1 , 1 , * output . shape [ - 3 :]) res = kwargs [ \"resolution\" ] res_ = np . float64 ( res [ 0 ]) metadata = {} d = { \"resolution\" : res_ , \"volume\" : output . view ( dshape ) . float () . to ( device ), \"mask\" : ( seg > 0 ) . view ( dshape ) . float () . to ( device ), \"seg\" : seg . view ( dshape ) . float () . to ( device ), \"affine\" : torch . diag ( torch . tensor ( list ( res ) + [ 1 ])) . to ( device ), \"threshold\" : 0.1 , } self . scanner_args . resolution_recon = res_ scanner = Scanner ( ** asdict ( self . scanner_args )) d_scan = scanner . scan ( d ) recon = PSFReconstructor ( ** asdict ( self . recon_args )) output , _ = recon . recon_psf ( d_scan ) metadata . update ( { \"resolution_recon\" : d_scan [ \"resolution_recon\" ], \"resolution_slice\" : d_scan [ \"resolution_slice\" ], \"slice_thickness\" : d_scan [ \"slice_thickness\" ], \"gap\" : d_scan [ \"gap\" ], \"nstacks\" : len ( torch . unique ( d_scan [ \"positions\" ][:, 1 ])), } ) metadata . update ( recon . get_seeds ()) return output . squeeze (), metadata else : return output , {}","title":"SimulateMotion"},{"location":"sr_artif_api/#fetalsyngen.generator.augmentation.artifacts.SimulateMotion.__init__","text":"Initialize the augmentation parameters. Parameters: prob ( float ) \u2013 Probability of applying the augmentation. scanner_params ( ScannerParams ) \u2013 Dataclass of parameters for the scanner. recon_params ( ReconParams ) \u2013 Dataclass of parameters for the reconstructor. Source code in fetalsyngen/generator/augmentation/artifacts.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 def __init__ ( self , prob : float , scanner_params : ScannerParams , recon_params : ReconParams , ): \"\"\" Initialize the augmentation parameters. Args: prob (float): Probability of applying the augmentation. scanner_params (ScannerParams): Dataclass of parameters for the scanner. recon_params (ReconParams): Dataclass of parameters for the reconstructor. \"\"\" self . scanner_args = scanner_params self . recon_args = recon_params self . prob = prob","title":"__init__"},{"location":"sr_artif_api/#fetalsyngen.generator.augmentation.artifacts.SimulateMotion.__call__","text":"Apply the motion simulation to the input image. Parameters: output ( Tensor ) \u2013 Input image to resample. seg ( Tensor ) \u2013 Input segmentation corresponding to the image. device ( str ) \u2013 Device to use for computation. genparams ( dict , default: {} ) \u2013 Generation parameters. Returns: tuple [ Tensor , dict ] \u2013 Image with simulated motion and metadata containing the motion simulation parameters. Source code in fetalsyngen/generator/augmentation/artifacts.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 def __call__ ( self , output , seg , device , genparams : dict = {}, ** kwargs ) -> tuple [ torch . Tensor , dict ]: \"\"\" Apply the motion simulation to the input image. Args: output (torch.Tensor): Input image to resample. seg (torch.Tensor): Input segmentation corresponding to the image. device (str): Device to use for computation. genparams (dict): Generation parameters. Returns: Image with simulated motion and metadata containing the motion simulation parameters. \"\"\" # def _artifact_simulate_motion(self, im, seg, generator_params, res): if np . random . rand () < self . prob : device = output . device dshape = ( 1 , 1 , * output . shape [ - 3 :]) res = kwargs [ \"resolution\" ] res_ = np . float64 ( res [ 0 ]) metadata = {} d = { \"resolution\" : res_ , \"volume\" : output . view ( dshape ) . float () . to ( device ), \"mask\" : ( seg > 0 ) . view ( dshape ) . float () . to ( device ), \"seg\" : seg . view ( dshape ) . float () . to ( device ), \"affine\" : torch . diag ( torch . tensor ( list ( res ) + [ 1 ])) . to ( device ), \"threshold\" : 0.1 , } self . scanner_args . resolution_recon = res_ scanner = Scanner ( ** asdict ( self . scanner_args )) d_scan = scanner . scan ( d ) recon = PSFReconstructor ( ** asdict ( self . recon_args )) output , _ = recon . recon_psf ( d_scan ) metadata . update ( { \"resolution_recon\" : d_scan [ \"resolution_recon\" ], \"resolution_slice\" : d_scan [ \"resolution_slice\" ], \"slice_thickness\" : d_scan [ \"slice_thickness\" ], \"gap\" : d_scan [ \"gap\" ], \"nstacks\" : len ( torch . unique ( d_scan [ \"positions\" ][:, 1 ])), } ) metadata . update ( recon . get_seeds ()) return output . squeeze (), metadata else : return output , {}","title":"__call__"},{"location":"sr_artif_api/#references","text":"Sanchez, Thomas, et al. \"Assessing data quality on fetal brain MRI reconstruction: a multi-site and multi-rater study.\" International Workshop on Preterm, Perinatal and Paediatric Image Analysis. Cham: Springer Nature Switzerland, 2024.","title":"References"},{"location":"usage/","text":"How to use the generator? Follow these steps to integrate and use the generator in your project: 1. Install the Package Refer to the Installation page for detailed instructions on how to install the package. 2. Prepare the Dataset Ensure your dataset is formatted according to the BIDS format . Your dataset must include the following files: T2w image : Files should have the naming pattern *_T2w.nii.gz . Segmentation mask : Files should have the naming pattern *_dseg.nii.gz . Seeds : Seeds must be generated from a BIDS-formatted directory using the fetalsyngen/scripts/generate_seeds.py script. For additional details on seed generation, refer to the Seed Generation page. 3. Copy Dataset Configurations We provide a variety of ready-to-use configurations for different tasks. These configuration files are stored in the fetalsyngen/configs/dataset folder and are further detailed in the Configs page. Each configuration is a .yaml file that contains the parameters for the generation pipeline. You can modify these configurations to suit the specific requirements of your project. 4. Run the Generator We offer several torch.Dataset classes for loading synthetic and real datasets: fetalsyngen.data.datasets.FetalTestDataset : Loads real images and segmentations. Used for testing and validation on real data. fetalsyngen.data.datasets.FetalSynthDataset : Can be used to either to create synthetic images and segmentation on the fly or apply the same transformations used in generation of synthetic data to real images and segmentations. For more details on these datasets, see the Datasets page. Note \ud83d\udcdd Configs : Use the local copy of the config files from your repository to instantiate the generator/dataset classes. \ud83d\udcdd Paths : When using the dataset classes, ensure that the paths in your local configuration files are updated to correctly reference your dataset and seed files.","title":"How to use the generator?"},{"location":"usage/#how-to-use-the-generator","text":"Follow these steps to integrate and use the generator in your project:","title":"How to use the generator?"},{"location":"usage/#1-install-the-package","text":"Refer to the Installation page for detailed instructions on how to install the package.","title":"1. Install the Package"},{"location":"usage/#2-prepare-the-dataset","text":"Ensure your dataset is formatted according to the BIDS format . Your dataset must include the following files: T2w image : Files should have the naming pattern *_T2w.nii.gz . Segmentation mask : Files should have the naming pattern *_dseg.nii.gz . Seeds : Seeds must be generated from a BIDS-formatted directory using the fetalsyngen/scripts/generate_seeds.py script. For additional details on seed generation, refer to the Seed Generation page.","title":"2. Prepare the Dataset"},{"location":"usage/#3-copy-dataset-configurations","text":"We provide a variety of ready-to-use configurations for different tasks. These configuration files are stored in the fetalsyngen/configs/dataset folder and are further detailed in the Configs page. Each configuration is a .yaml file that contains the parameters for the generation pipeline. You can modify these configurations to suit the specific requirements of your project.","title":"3. Copy Dataset Configurations"},{"location":"usage/#4-run-the-generator","text":"We offer several torch.Dataset classes for loading synthetic and real datasets: fetalsyngen.data.datasets.FetalTestDataset : Loads real images and segmentations. Used for testing and validation on real data. fetalsyngen.data.datasets.FetalSynthDataset : Can be used to either to create synthetic images and segmentation on the fly or apply the same transformations used in generation of synthetic data to real images and segmentations. For more details on these datasets, see the Datasets page. Note \ud83d\udcdd Configs : Use the local copy of the config files from your repository to instantiate the generator/dataset classes. \ud83d\udcdd Paths : When using the dataset classes, ensure that the paths in your local configuration files are updated to correctly reference your dataset and seed files.","title":"4. Run the Generator"}]}